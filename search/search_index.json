{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Generative AI models have shown tremendous usefulness in increasing accessibility and automation of a wide range of tasks. Yet, their application to the biomedical domain is still limited, in part due to the lack of a common framework for deploying, testing, and evaluating the diverse models and auxiliary technologies that are needed. <code>biochatter</code> is a Python package implementing a generic backend library for the connection of biomedical applications to conversational AI. We describe the framework in this preprint; for a more hands-on experience, check out our two web app implementations:</p> <ul> <li> <p> BioChatter Light</p> <p>Agile framework in pure Python built with Streamlit, for fast prototyping and iteration.</p> <p> Go To BioChatter Light</p> </li> <li> <p> BioChatter Next</p> <p>Advanced client-server architecture based on Flask and Next.js.</p> <p> Go To BioChatter Next</p> </li> </ul> <p>BioChatter is part of the BioCypher ecosystem,  connecting natively to BioCypher knowledge graphs. The BioChatter paper is being written here and the current version can be read here.</p> <p></p> <p>Hot Topics</p> <p>BioChatter natively extends BioCypher knowledge graphs. Check there for more information.</p> <p>We have also recently published a perspective on connecting knowledge and machine learning to enable causal reasoning in biomedicine, with a particular focus on the currently emerging \"foundation models.\" You can read it here.</p>"},{"location":"#installation","title":"Installation","text":"<p>To use the package, install it from PyPI, for instance using pip (<code>pip install biochatter</code>) or Poetry (<code>poetry add biochatter</code>).</p>"},{"location":"#extras","title":"Extras","text":"<p>The package has some optional dependencies that can be installed using the following extras (e.g. <code>pip install biochatter[xinference]</code>):</p> <ul> <li> <p><code>xinference</code>: support for querying open-source LLMs through Xorbits Inference</p> </li> <li> <p><code>podcast</code>: support for podcast text-to-speech (for the free Google TTS; the paid OpenAI TTS can be used without this extra)</p> </li> <li> <p><code>streamlit</code>: support for streamlit UI functions (used in ChatGSE)</p> </li> </ul> <p> </p>"},{"location":"benchmark-developer/","title":"Benchmark - Developer Notes","text":"<p>To understand the benchmarking procedure, you should be familiar with Pytest. The benchmark test matrix is executed via Pytest fixtures that iterate through the combinations of test parameters such as model name and size.  This basic setup happens in the <code>conftest.py</code> file in the <code>benchmark</code> directory.  The benchmark Pytest setup is distinct from the Pytest setup we use for our continuous integration (in the <code>test</code> folder).</p> <p>You can imagine the benchmark as a set of unit tests, with the only difference being that the test subject is not our codebase, but the behaviour and performance of the various LLMs, prompts, etc. These tests are defined in two parts: the data and the method. Data are collected in a set of YAML files in the <code>benchmark/data</code> directory, while the benchmark methods are implemented in the Pytest functions in the individual Python modules (file names starting with <code>test_</code>). We also have some Python modules for auxiliary functions, such as <code>load_dataset.py</code>.</p> <p>In the following, we will describe a walkthrough of how to implement your own tests according to our benchmark philosophy.</p>"},{"location":"benchmark-developer/#test-setup","title":"Test setup","text":"<p>Tests are collected in the typical Pytest manner at the start of the procedure. In <code>conftest.py</code>, we define the model combinations we want to use in the benchmark.  We distinguish between closed-source and open-source models, since open-source models offer more flexibility, e.g., by setting their size and quantisation. In contrast, for OpenAI models, all we need is the name.</p>"},{"location":"benchmark-developer/#quickstart","title":"Quickstart","text":"<p>For getting started with developing your own benchmark, OpenAI models offer the simplest way, only requiring an API key and an internet connection. If you don't want to run open-source models right away, which is tied to setting up an external service for deploying the models, we can remove the <code>XINFERENCE_MODEL_NAMES</code> from the list of models to be benchmarked in <code>conftest.py</code> (by deleting or commenting them out):</p> <pre><code>BENCHMARKED_MODELS = OPENAI_MODEL_NAMES\n</code></pre> <p>In addition, we can reduce the number of OpenAI models to call to one for development; <code>gpt-3.5-turbo-0125</code> is a well-performing and economical initial choice (in <code>conftest.py</code>).</p> <pre><code>OPENAI_MODEL_NAMES = [\n    \"gpt-3.5-turbo-0125\",\n]\n</code></pre> <p>The last thing to look out for when running the benchmark is to reduce the number of iterations for each test to one. We run iterations to account for stochasticity in LLM responses when we run the benchmark for real, but in development, this iteration brings no benefit and just increases computational cost. Set <code>N_ITERATIONS</code> to 1 in <code>conftest.py</code>.</p> <pre><code># how often should each benchmark be run?\nN_ITERATIONS = 1\n</code></pre> <p>This setup should allow you to run and debug a newly developed benchmark dataset or method effectively. For more explanation on how to do this, please read on.</p>"},{"location":"benchmark-developer/#debugging-the-benchmark","title":"Debugging the benchmark","text":"<p>You can get some insight into how the benchmark works by debugging the existing test cases and stepping through the code line-by-line. For this, it is necessary that you are familiar with the debugging procedure in your programming environment of choice, for instance, VSCode. You can set breakpoints in the initial setup (e.g., in <code>conftest.py</code> and <code>load_dataset.py</code>) as well as the test functions (e.g., <code>test_rag_interpretation.py</code>). Stepping through the code will give you insights into how the benchmark is designed and also how the LLMs respond in detail to each specific task. This is particularly helpful for ensuring that your newly developed benchmark test cases behave as expected and test accurately the functionality you aim to test.</p>"},{"location":"benchmark-developer/#creating-new-test-cases-for-existing-tests","title":"Creating new test cases for existing tests","text":"<p>Our test cases are collected in YAML files that follow a simple formalism for defining each test. A test case consists of</p> <ul> <li> <p>a descriptive name</p> </li> <li> <p>a set of input data to simulate a real-world task (e.g., the question of a user and some auxiliary information)</p> </li> <li> <p>a set of expected results to assess the performance of the model's response, akin to assert statements in regular unit tests</p> </li> </ul> <p>Here is a simple example test case:</p> <pre><code>rag_interpretation:\n# test simple relevance judgement\n  - case: explicit_relevance_yes\n    input:\n      prompt: Which molecular pathways are associated with cancer?\n      system_messages:\n        [\n          \"You will receive a text fragment to help answer the user's question. Your task is to judge this text fragment for  relevance to the user's question, and return either 'yes' or 'no'; only respond with one word, do not offer explanation  or justification! Here is the fragment: \",\n          \"The EGFR pathway is deregulated in a number of cancers.\",\n        ]\n    expected:\n      answer: \"yes\"\n</code></pre> <p>In this test, we benchmark the model's ability to judge the relevance of text fragments to be used in a Retrieval-Augmented Generation (RAG) scenario in an explicit fashion (i.e., we directly ask the model to judge the relevance of the given fragments). Thus, we choose the descriptive name (<code>case</code>) <code>explicit_relevance_yes</code>. The <code>input</code>s are a simulated user question (<code>prompt</code>) and the <code>system_messages</code> that simulate the prompt engineering and RAG fragments (that in the real application would be retrieved by some suitable mechanism). Finally, we define the <code>expected</code> output, which in this case is only the <code>answer</code> we expect from the LLM.</p> <p>The way that these test cases are evaluated is defined in the Pytest functions, which are tuned to the specific fields we define in the YAML. We can freely define test definitions and testing implementation by adjusting the YAML alongside the test Python code. The coordination between cases and Pytest functions is done by name of the test category, in this example <code>rag_interpretation</code> in the YAML and <code>test_rag_interpretation.py</code> in the <code>benchmark</code> directory.</p> <p>New tests can be arbitrarily complex as long as the test function is synchronised with the content of the YAML test dataset's fields (see below).</p>"},{"location":"benchmark-developer/#combinatorial-expansion-of-tests","title":"Combinatorial expansion of tests","text":"<p>Sometimes, it can be efficient to run the same test with slightly different variations of input data. For instance, LLMs are very sensitive to the prompt setup, and thus, we can run the same test with varying approaches to the prompt engineering. To simplify this, we can define input data as dictionaries (with keys being a descriptive name of the sub-test, and values being the content), which will lead to expansion of these test cases into full cases according to the definition. For instance, we can define a RAG interpretation test with prompts at three different levels of detail (<code>simple</code>, <code>more_explicit</code>, and <code>repeat_instruction</code>):</p> <pre><code>rag_interpretation:\n  # test simple irrelevance judgement\n  - case: explicit_relevance_no\n    input:\n      prompt: Which molecular pathways are associated with cancer?\n      system_messages:\n        simple:\n          [\n            \"You will receive a text fragment to help answer the user's question. Your task is to judge this text fragment for relevance to the user's question, and return either 'yes' or 'no'! Here is the fragment: \",\n            \"The earth is a globe.\",\n          ]\n        more_explicit:\n          [\n            \"You will receive a text fragment to help answer the user's question. Your task is to judge this text fragment for relevance to the user's question, and return either 'yes' or 'no'; only respond with one word, do not offer explanation or justification! Here is the fragment: \",\n            \"The earth is a globe.\",\n          ]\n        repeat_instruction:\n          [\n            \"You will receive a text fragment to help answer the user's question. You should only respond with 'yes' or 'no' without additional words. Your task is to judge this text fragment for relevance to the user's question, and return either 'yes' or 'no'; only respond with one word, do not offer explanation or justification! Here is the fragment: \",\n            \"The earth is a globe.\",\n          ]\n    expected:\n      answer: \"no\"\n</code></pre> <p>Upon instantiation of the test matrix, this definition will be expanded into three full tests, each with their respective prompt setup.</p>"},{"location":"benchmark-developer/#creating-new-test-procedures","title":"Creating new test procedures","text":"<p>If a new kind of test requires a bespoke procedure, such as evaluating a newly introduced functionality or calculating a score in a distinct way, we can introduce new methods to the test modules or even entire new modules. Following the layout of the existing tests, the newly created test functions should refer to fixtures for their data inputs.  Such a test function typically has as parameters:</p> <ul> <li> <p>the <code>model_name</code> fixture, to be able to record a model-specific benchmark metric</p> </li> <li> <p>a <code>test_data</code> object that is generated from the benchmark dataset according to the name of the test module (e.g., <code>test_data_rag_interpretation</code>)</p> </li> <li> <p>a <code>conversation</code> instance (the connection to the LLM to be tested)</p> </li> <li> <p>the <code>multiple_testing</code> fixture that implements running the test multiple times and averaging the results</p> </li> <li> <p>any number of additional inputs that may be required for the tests</p> </li> </ul> <p>For instance, the knowledge graph query generation tests acquire additional tests inputs from the YAML definition (the schema of the BioCypher knowledge graph underlying the test) and additional functionality from BioChatter (an instance of the prompt engine class that generates the knowledge graph query using the aforementioned schema).</p>"},{"location":"benchmark-developer/#running-open-source-models","title":"Running open-source models","text":"<p>To execute the benchmark on any of the open-source models in the test matrix, you need to deploy an Xorbits Inference server at an arbitrary IP, either via Docker (available on Linux machines with dedicated Nvidia GPU) or natively (e.g., on Apple machines). Please refer to the Xinference documentation for details.</p> <p>When you have deployed the Xinference server, you can point the benchmark to the server by setting the <code>BENCHMARK_URL</code> parameter in <code>conftest.py</code>:</p> <pre><code># Xinference IP and port\nBENCHMARK_URL = \"http://localhost:9997\"\n</code></pre>"},{"location":"benchmark-results/","title":"Benchmark - All Results","text":""},{"location":"benchmark-results/#biocypher-query-generation","title":"BioCypher query generation","text":"<p>In this set of tasks, we test LLM abilities to generate queries for a BioCypher Knowledge Graph using BioChatter. The <code>schema_config.yaml</code> of the BioCypher Knowledge Graph and a natural language query are passed to BioChatter.</p> <p>Individual steps of the query generation process are tested separately, as well as the end-to-end performance of the process.</p> Entity selectionRelationship selectionProperty selectionProperty confabulationNaive query generationBioChatter query generationEnd to end query generation Full model name Passed test cases Total test cases Accuracy Iterations gpt-3.5-turbo-0125 8 8 1 5 openhermes-2.5:7:ggufv2:Q6_K 8 8 1 5 openhermes-2.5:7:ggufv2:Q3_K_M 9 9 1 5 openhermes-2.5:7:ggufv2:Q8_0 8 9 0.888889 5 openhermes-2.5:7:ggufv2:Q5_K_M 8 9 0.888889 5 openhermes-2.5:7:ggufv2:Q4_K_M 8 9 0.888889 5 gpt-4-0613 8 9 0.888889 5 gpt-3.5-turbo-0613 8 9 0.888889 5 gpt-4-0125-preview 7 9 0.777778 5 chatglm3:6:ggmlv3:q4_0 6 8 0.75 5 openhermes-2.5:7:ggufv2:Q2_K 5 9 0.555556 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 4 8 0.5 5 code-llama-instruct:7:ggufv2:Q3_K_M 4 8 0.5 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 3.8 8 0.475 5 code-llama-instruct:13:ggufv2:Q3_K_M 3.6 8 0.45 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 4 9 0.444444 5 llama-2-chat:7:ggufv2:Q8_0 4 9 0.444444 5 llama-2-chat:7:ggufv2:Q5_K_M 4 9 0.444444 5 llama-2-chat:7:ggufv2:Q4_K_M 4 9 0.444444 5 llama-2-chat:70:ggufv2:Q5_K_M 4 9 0.444444 5 llama-2-chat:70:ggufv2:Q4_K_M 4 9 0.444444 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 3.8 9 0.422222 5 llama-2-chat:7:ggufv2:Q6_K 3 8 0.375 5 llama-2-chat:70:ggufv2:Q3_K_M 3 9 0.333333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 3 9 0.333333 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 3 9 0.333333 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 3 9 0.333333 5 llama-2-chat:7:ggufv2:Q3_K_M 3 9 0.333333 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 3 9 0.333333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 3 9 0.333333 5 code-llama-instruct:7:ggufv2:Q4_K_M 3 9 0.333333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 2.8 9 0.311111 5 code-llama-instruct:7:ggufv2:Q2_K 2 8 0.25 5 code-llama-instruct:34:ggufv2:Q8_0 2 8 0.25 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 2 9 0.222222 5 code-llama-instruct:34:ggufv2:Q6_K 1 8 0.125 5 code-llama-instruct:34:ggufv2:Q5_K_M 1 8 0.125 5 code-llama-instruct:7:ggufv2:Q5_K_M 1 9 0.111111 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 0 9 0 5 code-llama-instruct:13:ggufv2:Q6_K 0 8 0 5 code-llama-instruct:13:ggufv2:Q5_K_M 0 8 0 5 code-llama-instruct:13:ggufv2:Q4_K_M 0 8 0 5 llama-2-chat:7:ggufv2:Q2_K 0 9 0 5 llama-2-chat:70:ggufv2:Q2_K 0 9 0 5 code-llama-instruct:13:ggufv2:Q2_K 0 8 0 5 llama-2-chat:13:ggufv2:Q6_K 0 8 0 5 llama-2-chat:13:ggufv2:Q5_K_M 0 9 0 5 llama-2-chat:13:ggufv2:Q4_K_M 0 9 0 5 llama-2-chat:13:ggufv2:Q3_K_M 0 9 0 5 llama-2-chat:13:ggufv2:Q2_K 0 9 0 5 code-llama-instruct:13:ggufv2:Q8_0 0 8 0 5 code-llama-instruct:34:ggufv2:Q2_K 0 8 0 5 code-llama-instruct:34:ggufv2:Q3_K_M 0 8 0 5 code-llama-instruct:34:ggufv2:Q4_K_M 0 8 0 5 code-llama-instruct:7:ggufv2:Q8_0 0 9 0 5 code-llama-instruct:7:ggufv2:Q6_K 0 8 0 5 llama-2-chat:13:ggufv2:Q8_0 0 9 0 5 Full model name Passed test cases Total test cases Accuracy Iterations openhermes-2.5:7:ggufv2:Q8_0 12 12 1 5 gpt-3.5-turbo-0125 12 12 1 5 openhermes-2.5:7:ggufv2:Q6_K 12 12 1 5 openhermes-2.5:7:ggufv2:Q5_K_M 12 12 1 5 openhermes-2.5:7:ggufv2:Q4_K_M 12 12 1 5 openhermes-2.5:7:ggufv2:Q3_K_M 12 12 1 5 gpt-4-0125-preview 9 12 0.75 5 gpt-4-0613 7.8 12 0.65 5 openhermes-2.5:7:ggufv2:Q2_K 6 12 0.5 5 code-llama-instruct:34:ggufv2:Q2_K 6 12 0.5 5 gpt-3.5-turbo-0613 6 12 0.5 5 chatglm3:6:ggmlv3:q4_0 4.8 12 0.4 5 code-llama-instruct:7:ggufv2:Q3_K_M 3 12 0.25 5 code-llama-instruct:7:ggufv2:Q2_K 3 12 0.25 5 llama-2-chat:70:ggufv2:Q5_K_M 3 12 0.25 5 llama-2-chat:70:ggufv2:Q4_K_M 3 12 0.25 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 3 12 0.25 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 3 12 0.25 5 code-llama-instruct:34:ggufv2:Q3_K_M 3 12 0.25 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 3 12 0.25 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 3 12 0.25 5 code-llama-instruct:34:ggufv2:Q8_0 0 12 0 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 0 12 0 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 0 12 0 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 0 12 0 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 0 12 0 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 0 12 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 0 12 0 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 0 12 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 0 12 0 5 llama-2-chat:7:ggufv2:Q6_K 0 12 0 5 code-llama-instruct:13:ggufv2:Q8_0 0 12 0 5 code-llama-instruct:13:ggufv2:Q6_K 0 12 0 5 code-llama-instruct:13:ggufv2:Q5_K_M 0 12 0 5 code-llama-instruct:13:ggufv2:Q4_K_M 0 12 0 5 code-llama-instruct:13:ggufv2:Q3_K_M 0 12 0 5 llama-2-chat:7:ggufv2:Q8_0 0 12 0 5 llama-2-chat:7:ggufv2:Q3_K_M 0 12 0 5 llama-2-chat:7:ggufv2:Q5_K_M 0 12 0 5 llama-2-chat:13:ggufv2:Q2_K 0 12 0 5 code-llama-instruct:7:ggufv2:Q4_K_M 0 12 0 5 code-llama-instruct:7:ggufv2:Q5_K_M 0 12 0 5 code-llama-instruct:7:ggufv2:Q6_K 0 12 0 5 code-llama-instruct:7:ggufv2:Q8_0 0 12 0 5 code-llama-instruct:34:ggufv2:Q6_K 0 12 0 5 code-llama-instruct:34:ggufv2:Q5_K_M 0 12 0 5 code-llama-instruct:34:ggufv2:Q4_K_M 0 12 0 5 llama-2-chat:13:ggufv2:Q3_K_M 0 12 0 5 llama-2-chat:7:ggufv2:Q4_K_M 0 12 0 5 llama-2-chat:13:ggufv2:Q4_K_M 0 12 0 5 llama-2-chat:13:ggufv2:Q5_K_M 0 12 0 5 llama-2-chat:13:ggufv2:Q6_K 0 12 0 5 code-llama-instruct:13:ggufv2:Q2_K 0 12 0 5 llama-2-chat:70:ggufv2:Q2_K 0 12 0 5 llama-2-chat:70:ggufv2:Q3_K_M 0 12 0 5 llama-2-chat:7:ggufv2:Q2_K 0 12 0 5 llama-2-chat:13:ggufv2:Q8_0 0 12 0 5 Full model name Passed test cases Total test cases Accuracy Iterations gpt-3.5-turbo-0613 23.2 64 0.3625 5 gpt-4-0613 23 64 0.359375 5 gpt-3.5-turbo-0125 22.8 64 0.35625 5 chatglm3:6:ggmlv3:q4_0 18.4 64 0.2875 5 llama-2-chat:70:ggufv2:Q3_K_M 11 64 0.171875 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 10.4 64 0.1625 5 openhermes-2.5:7:ggufv2:Q5_K_M 8 64 0.125 5 openhermes-2.5:7:ggufv2:Q3_K_M 8 64 0.125 5 openhermes-2.5:7:ggufv2:Q8_0 8 64 0.125 5 llama-2-chat:7:ggufv2:Q3_K_M 6.4 64 0.1 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 4.2 64 0.065625 5 code-llama-instruct:7:ggufv2:Q2_K 4 64 0.0625 5 openhermes-2.5:7:ggufv2:Q6_K 3 64 0.046875 5 openhermes-2.5:7:ggufv2:Q4_K_M 3 64 0.046875 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 3 64 0.046875 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 3 64 0.046875 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 2.4 64 0.0375 5 llama-2-chat:7:ggufv2:Q5_K_M 2.4 64 0.0375 5 llama-2-chat:7:ggufv2:Q6_K 0 64 0 5 llama-2-chat:7:ggufv2:Q8_0 0 64 0 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 0 64 0 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 0 64 0 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 0 64 0 5 code-llama-instruct:34:ggufv2:Q4_K_M 0 64 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 0 64 0 5 code-llama-instruct:13:ggufv2:Q5_K_M 0 64 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 0 64 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 0 64 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 0 64 0 5 openhermes-2.5:7:ggufv2:Q2_K 0 64 0 5 code-llama-instruct:13:ggufv2:Q4_K_M 0 64 0 5 code-llama-instruct:13:ggufv2:Q3_K_M 0 64 0 5 llama-2-chat:7:ggufv2:Q4_K_M 0 64 0 5 llama-2-chat:7:ggufv2:Q2_K 0 64 0 5 code-llama-instruct:34:ggufv2:Q5_K_M 0 64 0 5 gpt-4-0125-preview 0 64 0 5 code-llama-instruct:34:ggufv2:Q6_K 0 64 0 5 code-llama-instruct:34:ggufv2:Q8_0 0 64 0 5 code-llama-instruct:7:ggufv2:Q3_K_M 0 64 0 5 code-llama-instruct:7:ggufv2:Q4_K_M 0 64 0 5 code-llama-instruct:7:ggufv2:Q5_K_M 0 64 0 5 code-llama-instruct:7:ggufv2:Q6_K 0 64 0 5 code-llama-instruct:7:ggufv2:Q8_0 0 64 0 5 code-llama-instruct:34:ggufv2:Q3_K_M 0 64 0 5 code-llama-instruct:34:ggufv2:Q2_K 0 64 0 5 code-llama-instruct:13:ggufv2:Q8_0 0 64 0 5 llama-2-chat:70:ggufv2:Q5_K_M 0 64 0 5 llama-2-chat:13:ggufv2:Q2_K 0 64 0 5 llama-2-chat:13:ggufv2:Q3_K_M 0 64 0 5 llama-2-chat:13:ggufv2:Q4_K_M 0 64 0 5 llama-2-chat:13:ggufv2:Q5_K_M 0 64 0 5 llama-2-chat:13:ggufv2:Q6_K 0 64 0 5 code-llama-instruct:13:ggufv2:Q2_K 0 64 0 5 llama-2-chat:70:ggufv2:Q2_K 0 64 0 5 code-llama-instruct:13:ggufv2:Q6_K 0 64 0 5 llama-2-chat:70:ggufv2:Q4_K_M 0 64 0 5 llama-2-chat:13:ggufv2:Q8_0 0 64 0 5 Full model name Passed test cases Total test cases Accuracy Iterations code-llama-instruct:34:ggufv2:Q4_K_M 7.8 8 0.975 5 code-llama-instruct:34:ggufv2:Q5_K_M 7.6 8 0.95 5 code-llama-instruct:34:ggufv2:Q8_0 7.4 8 0.925 5 code-llama-instruct:34:ggufv2:Q6_K 7.2 8 0.9 5 gpt-4-0613 8 9 0.888889 5 code-llama-instruct:13:ggufv2:Q2_K 7 8 0.875 5 code-llama-instruct:34:ggufv2:Q3_K_M 7 8 0.875 5 gpt-3.5-turbo-0125 7.8 9 0.866667 5 code-llama-instruct:13:ggufv2:Q3_K_M 6.8 8 0.85 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 6.8 8 0.85 5 openhermes-2.5:7:ggufv2:Q2_K 7.6 9 0.844444 5 code-llama-instruct:13:ggufv2:Q6_K 6.6 8 0.825 5 code-llama-instruct:7:ggufv2:Q2_K 6.4 8 0.8 5 code-llama-instruct:7:ggufv2:Q3_K_M 7.2 9 0.8 5 llama-2-chat:70:ggufv2:Q5_K_M 7 9 0.777778 5 llama-2-chat:13:ggufv2:Q4_K_M 7 9 0.777778 5 openhermes-2.5:7:ggufv2:Q5_K_M 7 9 0.777778 5 llama-2-chat:70:ggufv2:Q3_K_M 7 9 0.777778 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 7 9 0.777778 5 code-llama-instruct:7:ggufv2:Q6_K 6.2 8 0.775 5 code-llama-instruct:13:ggufv2:Q5_K_M 6.2 8 0.775 5 code-llama-instruct:13:ggufv2:Q4_K_M 6.2 8 0.775 5 llama-2-chat:13:ggufv2:Q6_K 6.2 8 0.775 5 llama-2-chat:70:ggufv2:Q4_K_M 6.8 9 0.755556 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 6.8 9 0.755556 5 openhermes-2.5:7:ggufv2:Q8_0 6.8 9 0.755556 5 gpt-3.5-turbo-0613 6.8 9 0.755556 5 openhermes-2.5:7:ggufv2:Q4_K_M 6.8 9 0.755556 5 code-llama-instruct:34:ggufv2:Q2_K 6 8 0.75 5 code-llama-instruct:13:ggufv2:Q8_0 6 8 0.75 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 6.6 9 0.733333 5 llama-2-chat:13:ggufv2:Q3_K_M 6.6 9 0.733333 5 gpt-4-0125-preview 6.6 9 0.733333 5 openhermes-2.5:7:ggufv2:Q6_K 6.6 9 0.733333 5 openhermes-2.5:7:ggufv2:Q3_K_M 7.2 10 0.72 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 6.4 9 0.711111 5 llama-2-chat:13:ggufv2:Q8_0 6.4 9 0.711111 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 6.2 9 0.688889 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 6.2 9 0.688889 5 llama-2-chat:7:ggufv2:Q2_K 6.2 9 0.688889 5 code-llama-instruct:7:ggufv2:Q5_K_M 6.2 9 0.688889 5 code-llama-instruct:7:ggufv2:Q8_0 6 9 0.666667 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 6 9 0.666667 5 llama-2-chat:70:ggufv2:Q2_K 6 9 0.666667 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 6 9 0.666667 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 5.2 8 0.65 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 5.8 9 0.644444 5 llama-2-chat:13:ggufv2:Q5_K_M 5.8 9 0.644444 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 5.4 9 0.6 5 code-llama-instruct:7:ggufv2:Q4_K_M 5.4 9 0.6 5 llama-2-chat:7:ggufv2:Q4_K_M 4.4 9 0.488889 5 llama-2-chat:7:ggufv2:Q3_K_M 4.2 9 0.466667 5 llama-2-chat:7:ggufv2:Q8_0 3.2 9 0.355556 5 llama-2-chat:7:ggufv2:Q6_K 3 9 0.333333 5 llama-2-chat:13:ggufv2:Q2_K 2.6 9 0.288889 5 llama-2-chat:7:ggufv2:Q5_K_M 2.6 9 0.288889 5 chatglm3:6:ggmlv3:q4_0 2.2 8 0.275 5 Full model name Passed test cases Total test cases Accuracy Iterations gpt-4-0613 20.4 30 0.68 5 code-llama-instruct:7:ggufv2:Q4_K_M 19.6 30 0.653333 5 code-llama-instruct:34:ggufv2:Q3_K_M 18 30 0.6 5 openhermes-2.5:7:ggufv2:Q5_K_M 17.6 30 0.586667 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 17.2 30 0.573333 5 code-llama-instruct:13:ggufv2:Q2_K 17 30 0.566667 5 code-llama-instruct:13:ggufv2:Q5_K_M 17 30 0.566667 5 code-llama-instruct:13:ggufv2:Q8_0 17 30 0.566667 5 code-llama-instruct:34:ggufv2:Q2_K 17 30 0.566667 5 code-llama-instruct:13:ggufv2:Q6_K 16.2 30 0.54 5 openhermes-2.5:7:ggufv2:Q6_K 16 30 0.533333 5 code-llama-instruct:13:ggufv2:Q3_K_M 16 30 0.533333 5 code-llama-instruct:13:ggufv2:Q4_K_M 16 30 0.533333 5 code-llama-instruct:7:ggufv2:Q2_K 16 30 0.533333 5 gpt-3.5-turbo-0613 15 30 0.5 5 gpt-3.5-turbo-0125 14.6 30 0.486667 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 14.4 30 0.48 5 llama-2-chat:13:ggufv2:Q3_K_M 14.4 30 0.48 5 chatglm3:6:ggmlv3:q4_0 14.4 30 0.48 5 llama-2-chat:13:ggufv2:Q8_0 14.4 30 0.48 5 llama-2-chat:70:ggufv2:Q2_K 14.2 30 0.473333 5 code-llama-instruct:34:ggufv2:Q6_K 14.2 30 0.473333 5 code-llama-instruct:34:ggufv2:Q8_0 14 30 0.466667 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 14 30 0.466667 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 14 30 0.466667 5 openhermes-2.5:7:ggufv2:Q8_0 14 30 0.466667 5 openhermes-2.5:7:ggufv2:Q3_K_M 14 30 0.466667 5 openhermes-2.5:7:ggufv2:Q4_K_M 14 30 0.466667 5 code-llama-instruct:34:ggufv2:Q5_K_M 14 30 0.466667 5 code-llama-instruct:34:ggufv2:Q4_K_M 14 30 0.466667 5 gpt-4-0125-preview 13.2 30 0.44 5 llama-2-chat:13:ggufv2:Q5_K_M 13 30 0.433333 5 openhermes-2.5:7:ggufv2:Q2_K 13 30 0.433333 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 13 30 0.433333 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 13 30 0.433333 5 code-llama-instruct:7:ggufv2:Q3_K_M 12.8 30 0.426667 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 12.8 30 0.426667 5 llama-2-chat:70:ggufv2:Q4_K_M 12.6 30 0.42 5 llama-2-chat:70:ggufv2:Q3_K_M 12.4 30 0.413333 5 code-llama-instruct:7:ggufv2:Q8_0 12 30 0.4 5 code-llama-instruct:7:ggufv2:Q5_K_M 12 30 0.4 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 11.6 30 0.386667 5 llama-2-chat:13:ggufv2:Q6_K 11.6 30 0.386667 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 11.4 30 0.38 5 llama-2-chat:13:ggufv2:Q4_K_M 11 30 0.366667 5 llama-2-chat:13:ggufv2:Q2_K 11 30 0.366667 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 11 30 0.366667 5 llama-2-chat:70:ggufv2:Q5_K_M 10.8 30 0.36 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 10 30 0.333333 5 code-llama-instruct:7:ggufv2:Q6_K 10 30 0.333333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 10 30 0.333333 5 llama-2-chat:7:ggufv2:Q5_K_M 8.8 30 0.293333 5 llama-2-chat:7:ggufv2:Q6_K 8 30 0.266667 5 llama-2-chat:7:ggufv2:Q8_0 8 30 0.266667 5 llama-2-chat:7:ggufv2:Q4_K_M 7.2 30 0.24 5 llama-2-chat:7:ggufv2:Q3_K_M 7 30 0.233333 5 llama-2-chat:7:ggufv2:Q2_K 3 30 0.1 5 Full model name Passed test cases Total test cases Accuracy Iterations gpt-3.5-turbo-0125 29 30 0.966667 5 code-llama-instruct:7:ggufv2:Q4_K_M 29 30 0.966667 5 gpt-4-0613 29 30 0.966667 5 code-llama-instruct:7:ggufv2:Q8_0 28.8 30 0.96 5 code-llama-instruct:7:ggufv2:Q5_K_M 28.8 30 0.96 5 code-llama-instruct:7:ggufv2:Q6_K 28.8 30 0.96 5 gpt-3.5-turbo-0613 28.4 30 0.946667 5 openhermes-2.5:7:ggufv2:Q3_K_M 28.2 30 0.94 5 openhermes-2.5:7:ggufv2:Q2_K 28.2 30 0.94 5 llama-2-chat:70:ggufv2:Q4_K_M 27.6 30 0.92 5 code-llama-instruct:7:ggufv2:Q2_K 27.6 30 0.92 5 openhermes-2.5:7:ggufv2:Q5_K_M 27.4 30 0.913333 5 llama-2-chat:70:ggufv2:Q3_K_M 27.2 30 0.906667 5 llama-2-chat:70:ggufv2:Q5_K_M 27.2 30 0.906667 5 code-llama-instruct:34:ggufv2:Q4_K_M 27.2 30 0.906667 5 llama-2-chat:70:ggufv2:Q2_K 27 30 0.9 5 code-llama-instruct:34:ggufv2:Q5_K_M 27 30 0.9 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 26.8 30 0.893333 5 openhermes-2.5:7:ggufv2:Q8_0 26.4 30 0.88 5 code-llama-instruct:7:ggufv2:Q3_K_M 26.2 30 0.873333 5 openhermes-2.5:7:ggufv2:Q4_K_M 26.2 30 0.873333 5 code-llama-instruct:34:ggufv2:Q8_0 25.8 30 0.86 5 openhermes-2.5:7:ggufv2:Q6_K 25.8 30 0.86 5 code-llama-instruct:34:ggufv2:Q6_K 25.6 30 0.853333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 25.4 30 0.846667 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 25.4 30 0.846667 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 25.2 30 0.84 5 code-llama-instruct:13:ggufv2:Q3_K_M 25 30 0.833333 5 gpt-4-0125-preview 25 30 0.833333 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 25 30 0.833333 5 code-llama-instruct:13:ggufv2:Q4_K_M 25 30 0.833333 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 24.8 30 0.826667 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 24.8 30 0.826667 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 24.8 30 0.826667 5 code-llama-instruct:13:ggufv2:Q2_K 24.6 30 0.82 5 llama-2-chat:13:ggufv2:Q6_K 24.4 30 0.813333 5 code-llama-instruct:13:ggufv2:Q6_K 23.8 30 0.793333 5 llama-2-chat:13:ggufv2:Q8_0 23.6 30 0.786667 5 code-llama-instruct:34:ggufv2:Q3_K_M 23.6 30 0.786667 5 code-llama-instruct:13:ggufv2:Q5_K_M 23.4 30 0.78 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 23.2 30 0.773333 5 code-llama-instruct:13:ggufv2:Q8_0 23 30 0.766667 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 22.8 30 0.76 5 llama-2-chat:13:ggufv2:Q4_K_M 22.8 30 0.76 5 llama-2-chat:13:ggufv2:Q5_K_M 22.4 30 0.746667 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 21.8 30 0.726667 5 llama-2-chat:7:ggufv2:Q3_K_M 20.8 30 0.693333 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 20.8 30 0.693333 5 code-llama-instruct:34:ggufv2:Q2_K 20.6 30 0.686667 5 llama-2-chat:7:ggufv2:Q2_K 20.6 30 0.686667 5 llama-2-chat:13:ggufv2:Q3_K_M 20.4 30 0.68 5 llama-2-chat:7:ggufv2:Q6_K 19.8 30 0.66 5 llama-2-chat:7:ggufv2:Q4_K_M 19.4 30 0.646667 5 llama-2-chat:7:ggufv2:Q8_0 19.2 30 0.64 5 llama-2-chat:7:ggufv2:Q5_K_M 19 30 0.633333 5 chatglm3:6:ggmlv3:q4_0 16.6 30 0.553333 5 llama-2-chat:13:ggufv2:Q2_K 13 30 0.433333 5 Full model name Passed test cases Total test cases Accuracy Iterations gpt-3.5-turbo-0125 27.8 30 0.926667 5 gpt-4-0613 26.4 30 0.88 5 gpt-3.5-turbo-0613 25 30 0.833333 5 chatglm3:6:ggmlv3:q4_0 0 30 0 5 llama-2-chat:70:ggufv2:Q3_K_M 0 30 0 5 llama-2-chat:70:ggufv2:Q5_K_M 0 30 0 5 llama-2-chat:7:ggufv2:Q2_K 0 30 0 5 llama-2-chat:7:ggufv2:Q3_K_M 0 30 0 5 llama-2-chat:7:ggufv2:Q4_K_M 0 30 0 5 llama-2-chat:7:ggufv2:Q5_K_M 0 30 0 5 llama-2-chat:7:ggufv2:Q6_K 0 30 0 5 llama-2-chat:7:ggufv2:Q8_0 0 30 0 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 0 30 0 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 0 30 0 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 0 30 0 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 0 30 0 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 0 30 0 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 0 30 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 0 30 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 0 30 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 0 30 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 0 30 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 0 30 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 0 30 0 5 openhermes-2.5:7:ggufv2:Q2_K 0 30 0 5 openhermes-2.5:7:ggufv2:Q3_K_M 0 30 0 5 openhermes-2.5:7:ggufv2:Q4_K_M 0 30 0 5 openhermes-2.5:7:ggufv2:Q5_K_M 0 30 0 5 openhermes-2.5:7:ggufv2:Q6_K 0 30 0 5 llama-2-chat:70:ggufv2:Q4_K_M 0 30 0 5 llama-2-chat:13:ggufv2:Q8_0 0 30 0 5 llama-2-chat:70:ggufv2:Q2_K 0 30 0 5 code-llama-instruct:13:ggufv2:Q2_K 0 30 0 5 code-llama-instruct:13:ggufv2:Q3_K_M 0 30 0 5 code-llama-instruct:13:ggufv2:Q4_K_M 0 30 0 5 code-llama-instruct:13:ggufv2:Q5_K_M 0 30 0 5 code-llama-instruct:13:ggufv2:Q6_K 0 30 0 5 code-llama-instruct:13:ggufv2:Q8_0 0 30 0 5 code-llama-instruct:34:ggufv2:Q2_K 0 30 0 5 code-llama-instruct:34:ggufv2:Q3_K_M 0 30 0 5 code-llama-instruct:34:ggufv2:Q4_K_M 0 30 0 5 code-llama-instruct:34:ggufv2:Q5_K_M 0 30 0 5 code-llama-instruct:34:ggufv2:Q6_K 0 30 0 5 code-llama-instruct:34:ggufv2:Q8_0 0 30 0 5 code-llama-instruct:7:ggufv2:Q2_K 0 30 0 5 code-llama-instruct:7:ggufv2:Q3_K_M 0 30 0 5 code-llama-instruct:7:ggufv2:Q4_K_M 0 30 0 5 code-llama-instruct:7:ggufv2:Q5_K_M 0 30 0 5 code-llama-instruct:7:ggufv2:Q6_K 0 30 0 5 code-llama-instruct:7:ggufv2:Q8_0 0 30 0 5 gpt-4-0125-preview 0 30 0 5 llama-2-chat:13:ggufv2:Q2_K 0 30 0 5 llama-2-chat:13:ggufv2:Q3_K_M 0 30 0 5 llama-2-chat:13:ggufv2:Q4_K_M 0 30 0 5 llama-2-chat:13:ggufv2:Q5_K_M 0 30 0 5 llama-2-chat:13:ggufv2:Q6_K 0 30 0 5 openhermes-2.5:7:ggufv2:Q8_0 0 30 0 5"},{"location":"benchmark-results/#retrieval-augmented-generation-rag","title":"Retrieval-Augmented Generation (RAG)","text":"<p>In this set of tasks, we test LLM abilities to generate answers to a given question using a RAG agent, or to judge the relevance of a RAG fragment to a given question. Instructions can be explicit (\"is this fragment relevant to the question?\") or implicit (just asking the question without instructions and evaluating whether the model responds with 'not enough information given').</p> Explicit relevance of single fragmentsImplicit relevance of single fragments Full model name Passed test cases Total test cases Accuracy Iterations llama-2-chat:13:ggufv2:Q8_0 6 6 1 5 llama-2-chat:13:ggufv2:Q4_K_M 6 6 1 5 llama-2-chat:13:ggufv2:Q6_K 6 6 1 5 llama-2-chat:70:ggufv2:Q2_K 6 6 1 5 llama-2-chat:70:ggufv2:Q3_K_M 6 6 1 5 llama-2-chat:70:ggufv2:Q4_K_M 6 6 1 5 llama-2-chat:70:ggufv2:Q5_K_M 6 6 1 5 llama-2-chat:7:ggufv2:Q3_K_M 6 6 1 5 llama-2-chat:7:ggufv2:Q4_K_M 6 6 1 5 llama-2-chat:7:ggufv2:Q5_K_M 6 6 1 5 llama-2-chat:7:ggufv2:Q6_K 6 6 1 5 llama-2-chat:7:ggufv2:Q8_0 6 6 1 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 6 6 1 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 6 6 1 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 6 6 1 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 6 6 1 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 6 6 1 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 6 6 1 5 openhermes-2.5:7:ggufv2:Q2_K 6 6 1 5 openhermes-2.5:7:ggufv2:Q3_K_M 6 6 1 5 openhermes-2.5:7:ggufv2:Q4_K_M 6 6 1 5 openhermes-2.5:7:ggufv2:Q5_K_M 6 6 1 5 openhermes-2.5:7:ggufv2:Q6_K 6 6 1 5 llama-2-chat:13:ggufv2:Q5_K_M 6 6 1 5 openhermes-2.5:7:ggufv2:Q8_0 6 6 1 5 llama-2-chat:13:ggufv2:Q3_K_M 6 6 1 5 llama-2-chat:13:ggufv2:Q2_K 6 6 1 5 gpt-4-0613 6 6 1 5 gpt-4-0125-preview 6 6 1 5 gpt-3.5-turbo-0613 6 6 1 5 gpt-3.5-turbo-0125 6 6 1 5 code-llama-instruct:7:ggufv2:Q8_0 6 6 1 5 code-llama-instruct:7:ggufv2:Q4_K_M 6 6 1 5 code-llama-instruct:13:ggufv2:Q6_K 5 6 0.833333 5 llama-2-chat:7:ggufv2:Q2_K 5 6 0.833333 5 code-llama-instruct:7:ggufv2:Q6_K 5 6 0.833333 5 code-llama-instruct:7:ggufv2:Q5_K_M 5 6 0.833333 5 code-llama-instruct:7:ggufv2:Q3_K_M 5 6 0.833333 5 code-llama-instruct:13:ggufv2:Q8_0 5 6 0.833333 5 chatglm3:6:ggmlv3:q4_0 4.4 6 0.733333 5 code-llama-instruct:13:ggufv2:Q5_K_M 4 6 0.666667 5 code-llama-instruct:34:ggufv2:Q4_K_M 3 6 0.5 5 code-llama-instruct:34:ggufv2:Q3_K_M 3 6 0.5 5 code-llama-instruct:34:ggufv2:Q2_K 3 6 0.5 5 code-llama-instruct:34:ggufv2:Q6_K 2 6 0.333333 5 code-llama-instruct:34:ggufv2:Q8_0 2 6 0.333333 5 code-llama-instruct:13:ggufv2:Q4_K_M 2 6 0.333333 5 code-llama-instruct:7:ggufv2:Q2_K 2 6 0.333333 5 code-llama-instruct:34:ggufv2:Q5_K_M 2 6 0.333333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 2 6 0.333333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 1 6 0.166667 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 0.8 6 0.133333 5 code-llama-instruct:13:ggufv2:Q2_K 0.2 6 0.0333333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 0 6 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 0 6 0 5 code-llama-instruct:13:ggufv2:Q3_K_M 0 6 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 0 6 0 5 Full model name Passed test cases Total test cases Accuracy Iterations chatglm3:6:ggmlv3:q4_0 2 2 1 5 gpt-3.5-turbo-0613 2 2 1 5 openhermes-2.5:7:ggufv2:Q6_K 2 2 1 5 openhermes-2.5:7:ggufv2:Q5_K_M 2 2 1 5 openhermes-2.5:7:ggufv2:Q4_K_M 2 2 1 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 2 2 1 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 2 2 1 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 2 2 1 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 2 2 1 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 2 2 1 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 2 2 1 5 llama-2-chat:7:ggufv2:Q3_K_M 2 2 1 5 llama-2-chat:7:ggufv2:Q2_K 2 2 1 5 llama-2-chat:70:ggufv2:Q4_K_M 2 2 1 5 gpt-4-0613 2 2 1 5 openhermes-2.5:7:ggufv2:Q8_0 2 2 1 5 code-llama-instruct:34:ggufv2:Q2_K 2 2 1 5 code-llama-instruct:7:ggufv2:Q4_K_M 2 2 1 5 code-llama-instruct:34:ggufv2:Q5_K_M 2 2 1 5 gpt-3.5-turbo-0125 1.8 2 0.9 5 code-llama-instruct:7:ggufv2:Q6_K 1.8 2 0.9 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 1.8 2 0.9 5 code-llama-instruct:34:ggufv2:Q6_K 1.8 2 0.9 5 code-llama-instruct:34:ggufv2:Q8_0 1.8 2 0.9 5 llama-2-chat:70:ggufv2:Q5_K_M 1.8 2 0.9 5 code-llama-instruct:7:ggufv2:Q2_K 1.4 2 0.7 5 code-llama-instruct:7:ggufv2:Q3_K_M 1.4 2 0.7 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 1.4 2 0.7 5 llama-2-chat:7:ggufv2:Q5_K_M 1.2 2 0.6 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 1.2 2 0.6 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 1.2 2 0.6 5 code-llama-instruct:13:ggufv2:Q6_K 1 2 0.5 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 1 2 0.5 5 code-llama-instruct:13:ggufv2:Q8_0 1 2 0.5 5 openhermes-2.5:7:ggufv2:Q2_K 1 2 0.5 5 openhermes-2.5:7:ggufv2:Q3_K_M 1 2 0.5 5 code-llama-instruct:13:ggufv2:Q5_K_M 1 2 0.5 5 code-llama-instruct:13:ggufv2:Q4_K_M 1 2 0.5 5 code-llama-instruct:34:ggufv2:Q3_K_M 1 2 0.5 5 code-llama-instruct:7:ggufv2:Q5_K_M 1 2 0.5 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 1 2 0.5 5 llama-2-chat:13:ggufv2:Q5_K_M 1 2 0.5 5 code-llama-instruct:7:ggufv2:Q8_0 1 2 0.5 5 gpt-4-0125-preview 1 2 0.5 5 llama-2-chat:13:ggufv2:Q2_K 1 2 0.5 5 llama-2-chat:13:ggufv2:Q3_K_M 1 2 0.5 5 llama-2-chat:13:ggufv2:Q4_K_M 1 2 0.5 5 llama-2-chat:13:ggufv2:Q6_K 1 2 0.5 5 llama-2-chat:70:ggufv2:Q2_K 1 2 0.5 5 llama-2-chat:70:ggufv2:Q3_K_M 1 2 0.5 5 llama-2-chat:7:ggufv2:Q4_K_M 1 2 0.5 5 llama-2-chat:7:ggufv2:Q6_K 1 2 0.5 5 llama-2-chat:7:ggufv2:Q8_0 1 2 0.5 5 llama-2-chat:13:ggufv2:Q8_0 1 2 0.5 5 code-llama-instruct:34:ggufv2:Q4_K_M 0.8 2 0.4 5 code-llama-instruct:13:ggufv2:Q2_K 0.8 2 0.4 5 code-llama-instruct:13:ggufv2:Q3_K_M 0 2 0 5"},{"location":"benchmark-results/#semantic-search","title":"Semantic search","text":"<p>Coming soon.</p>"},{"location":"benchmark/","title":"Benchmark Results - Overview","text":"<p>Here we collect the results of the living BioChatter benchmark. For an explanation, see the benchmarking documentation and the developer docs for further reading.</p>"},{"location":"benchmark/#scores-per-model","title":"Scores per model","text":"<p>Table sorted by mean score in descending order. Click the column names to reorder.</p> Model name Size Median Accuracy SD gpt-3.5-turbo-0125 175 0.91 0.22 gpt-4-0613 Unknown 0.88 0.19 openhermes-2.5 7 0.8 0.35 gpt-3.5-turbo-0613 175 0.79 0.22 gpt-4-0125-preview Unknown 0.65 0.32 chatglm3 6 0.49 0.27 mistral-instruct-v0.2 7 0.46 0.36 llama-2-chat 70 0.45 0.35 code-llama-instruct 7 0.4 0.35 llama-2-chat 13 0.4 0.34 code-llama-instruct 13 0.38 0.33 code-llama-instruct 34 0.38 0.35 llama-2-chat 7 0.35 0.32 mixtral-instruct-v0.1 46,7 0.35 0.3 <p> </p>"},{"location":"benchmark/#scores-per-quantisation","title":"Scores per quantisation","text":"<p>Table sorted by mean score in descending order. Click the column names to reorder.</p> Model name Size Version Quantisation Median Accuracy SD gpt-3.5-turbo-0125 175 nan nan 0.91 0.22 gpt-4-0613 Unknown nan nan 0.88 0.19 openhermes-2.5 7 ggufv2 Q5_K_M 0.83 0.35 openhermes-2.5 7 ggufv2 Q8_0 0.82 0.35 openhermes-2.5 7 ggufv2 Q4_K_M 0.81 0.36 openhermes-2.5 7 ggufv2 Q6_K 0.8 0.37 gpt-3.5-turbo-0613 175 nan nan 0.79 0.22 openhermes-2.5 7 ggufv2 Q3_K_M 0.68 0.35 gpt-4-0125-preview Unknown nan nan 0.65 0.32 code-llama-instruct 7 ggufv2 Q4_K_M 0.55 0.39 openhermes-2.5 7 ggufv2 Q2_K 0.52 0.33 code-llama-instruct 34 ggufv2 Q2_K 0.5 0.33 mistral-instruct-v0.2 7 ggufv2 Q6_K 0.5 0.37 code-llama-instruct 7 ggufv2 Q3_K_M 0.49 0.31 chatglm3 6 ggmlv3 q4_0 0.49 0.27 llama-2-chat 70 ggufv2 Q4_K_M 0.49 0.36 llama-2-chat 70 ggufv2 Q5_K_M 0.48 0.35 mistral-instruct-v0.2 7 ggufv2 Q5_K_M 0.48 0.37 mistral-instruct-v0.2 7 ggufv2 Q3_K_M 0.47 0.36 mistral-instruct-v0.2 7 ggufv2 Q8_0 0.45 0.36 mistral-instruct-v0.2 7 ggufv2 Q2_K 0.45 0.32 code-llama-instruct 34 ggufv2 Q3_K_M 0.45 0.31 code-llama-instruct 13 ggufv2 Q6_K 0.44 0.35 code-llama-instruct 13 ggufv2 Q8_0 0.44 0.33 llama-2-chat 70 ggufv2 Q3_K_M 0.43 0.33 llama-2-chat 70 ggufv2 Q2_K 0.43 0.37 llama-2-chat 13 ggufv2 Q8_0 0.43 0.36 code-llama-instruct 13 ggufv2 Q5_K_M 0.43 0.32 llama-2-chat 13 ggufv2 Q3_K_M 0.43 0.35 mistral-instruct-v0.2 7 ggufv2 Q4_K_M 0.42 0.38 llama-2-chat 7 ggufv2 Q4_K_M 0.41 0.31 llama-2-chat 13 ggufv2 Q5_K_M 0.4 0.34 code-llama-instruct 7 ggufv2 Q8_0 0.4 0.37 code-llama-instruct 7 ggufv2 Q5_K_M 0.39 0.34 llama-2-chat 13 ggufv2 Q6_K 0.39 0.36 code-llama-instruct 7 ggufv2 Q2_K 0.38 0.29 code-llama-instruct 34 ggufv2 Q4_K_M 0.38 0.35 llama-2-chat 7 ggufv2 Q3_K_M 0.38 0.35 code-llama-instruct 7 ggufv2 Q6_K 0.38 0.39 code-llama-instruct 34 ggufv2 Q5_K_M 0.38 0.38 code-llama-instruct 34 ggufv2 Q8_0 0.37 0.35 llama-2-chat 13 ggufv2 Q4_K_M 0.37 0.36 mixtral-instruct-v0.1 46,7 ggufv2 Q4_K_M 0.37 0.32 code-llama-instruct 34 ggufv2 Q6_K 0.37 0.36 mixtral-instruct-v0.1 46,7 ggufv2 Q5_K_M 0.36 0.34 mixtral-instruct-v0.1 46,7 ggufv2 Q6_K 0.36 0.31 llama-2-chat 7 ggufv2 Q8_0 0.36 0.3 mixtral-instruct-v0.1 46,7 ggufv2 Q3_K_M 0.34 0.29 llama-2-chat 7 ggufv2 Q6_K 0.34 0.3 mixtral-instruct-v0.1 46,7 ggufv2 Q8_0 0.33 0.27 code-llama-instruct 13 ggufv2 Q4_K_M 0.33 0.31 llama-2-chat 7 ggufv2 Q5_K_M 0.33 0.3 mixtral-instruct-v0.1 46,7 ggufv2 Q2_K 0.33 0.29 llama-2-chat 13 ggufv2 Q2_K 0.29 0.3 llama-2-chat 7 ggufv2 Q2_K 0.23 0.38 code-llama-instruct 13 ggufv2 Q2_K 0.17 0.34 code-llama-instruct 13 ggufv2 Q3_K_M 0.15 0.34 <p></p>"},{"location":"benchmark/#scores-of-all-tasks","title":"Scores of all tasks","text":"<p>Wide table; you may need to scroll horizontally to see all columns. Table sorted by mean score in descending order. Click the column names to reorder.</p> Full model name naive_query_generation_using_schema entity_selection end_to_end_query_generation property_exists explicit_relevance_of_single_fragments property_selection implicit_relevance_of_multiple_fragments query_generation relationship_selection Mean Accuracy Median Accuracy SD gpt-3.5-turbo-0125 0.486667 1 0.926667 0.866667 1 0.35625 0.9 0.966667 1 0.833657 0.913333 0.216549 gpt-4-0613 0.68 0.888889 0.88 0.888889 1 0.359375 1 0.966667 0.65 0.812647 0.884444 0.190861 openhermes-2.5:7:ggufv2:Q5_K_M 0.586667 0.888889 0 0.777778 1 0.125 1 0.913333 1 0.699074 0.833333 0.347432 openhermes-2.5:7:ggufv2:Q8_0 0.466667 0.888889 0 0.755556 1 0.125 1 0.88 1 0.679568 0.817778 0.350691 openhermes-2.5:7:ggufv2:Q4_K_M 0.466667 0.888889 0 0.755556 1 0.046875 1 0.873333 1 0.670147 0.814444 0.363419 openhermes-2.5:7:ggufv2:Q6_K 0.533333 1 0 0.733333 1 0.046875 1 0.86 1 0.685949 0.796667 0.366119 gpt-3.5-turbo-0613 0.5 0.888889 0.833333 0.755556 1 0.3625 1 0.946667 0.5 0.754105 0.794444 0.215969 openhermes-2.5:7:ggufv2:Q3_K_M 0.466667 1 0 0.72 1 0.125 0.5 0.94 1 0.639074 0.679537 0.348016 gpt-4-0125-preview 0.44 0.777778 0 0.733333 1 0 0.5 0.833333 0.75 0.559383 0.646358 0.321552 code-llama-instruct:7:ggufv2:Q4_K_M 0.653333 0.333333 0 0.6 1 0 1 0.966667 0 0.505926 0.552963 0.39125 openhermes-2.5:7:ggufv2:Q2_K 0.433333 0.555556 0 0.844444 1 0 0.5 0.94 0.5 0.53037 0.515185 0.325835 code-llama-instruct:34:ggufv2:Q2_K 0.566667 0 0 0.75 0.5 0 1 0.686667 0.5 0.444815 0.5 0.328199 mistral-instruct-v0.2:7:ggufv2:Q6_K 0.433333 0.5 0 0.65 1 0.046875 1 0.833333 0 0.495949 0.497975 0.366502 code-llama-instruct:7:ggufv2:Q3_K_M 0.426667 0.5 0 0.8 0.833333 0 0.7 0.873333 0.25 0.487037 0.493519 0.307716 chatglm3:6:ggmlv3:q4_0 0.48 0.75 0 0.275 0.733333 0.2875 1 0.553333 0.4 0.497685 0.488843 0.269443 llama-2-chat:70:ggufv2:Q4_K_M 0.42 0.444444 0 0.755556 1 0 1 0.92 0.25 0.532222 0.488333 0.3614 llama-2-chat:70:ggufv2:Q5_K_M 0.36 0.444444 0 0.777778 1 0 0.9 0.906667 0.25 0.515432 0.479938 0.351671 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 0.466667 0.444444 0 0.688889 1 0 1 0.826667 0 0.491852 0.479259 0.373649 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 0.466667 0.333333 0 0.666667 1 0.046875 1 0.773333 0 0.476319 0.471493 0.364921 mistral-instruct-v0.2:7:ggufv2:Q8_0 0.433333 0.333333 0 0.644444 1 0.0375 0.9 0.846667 0 0.466142 0.449738 0.358761 mistral-instruct-v0.2:7:ggufv2:Q2_K 0.573333 0.222222 0 0.6 1 0 0.5 0.693333 0 0.398765 0.449383 0.322492 code-llama-instruct:34:ggufv2:Q3_K_M 0.6 0 0 0.875 0.5 0 0.5 0.786667 0.25 0.390185 0.445093 0.306514 code-llama-instruct:13:ggufv2:Q6_K 0.54 0 0 0.825 0.833333 0 0.5 0.793333 0 0.387963 0.443981 0.345581 code-llama-instruct:13:ggufv2:Q8_0 0.566667 0 0 0.75 0.833333 0 0.5 0.766667 0 0.37963 0.439815 0.334971 llama-2-chat:70:ggufv2:Q3_K_M 0.413333 0.333333 0 0.777778 1 0.171875 0.5 0.906667 0 0.455887 0.43461 0.334424 llama-2-chat:70:ggufv2:Q2_K 0.473333 0 0 0.666667 1 0 0.5 0.9 0 0.393333 0.433333 0.365724 llama-2-chat:13:ggufv2:Q8_0 0.48 0 0 0.711111 1 0 0.5 0.786667 0 0.38642 0.43321 0.355392 code-llama-instruct:13:ggufv2:Q5_K_M 0.566667 0 0 0.775 0.666667 0 0.5 0.78 0 0.36537 0.432685 0.320506 llama-2-chat:13:ggufv2:Q3_K_M 0.48 0 0 0.733333 1 0 0.5 0.68 0 0.377037 0.428519 0.346926 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 0.366667 0.333333 0 0.688889 1 0 1 0.826667 0 0.468395 0.417531 0.378326 llama-2-chat:7:ggufv2:Q4_K_M 0.24 0.444444 0 0.488889 1 0 0.5 0.646667 0 0.368889 0.406667 0.306416 llama-2-chat:13:ggufv2:Q5_K_M 0.433333 0 0 0.644444 1 0 0.5 0.746667 0 0.369383 0.401358 0.344025 code-llama-instruct:7:ggufv2:Q8_0 0.4 0 0 0.666667 1 0 0.5 0.96 0 0.391852 0.395926 0.37338 code-llama-instruct:7:ggufv2:Q5_K_M 0.4 0.111111 0 0.688889 0.833333 0 0.5 0.96 0 0.388148 0.394074 0.340156 llama-2-chat:13:ggufv2:Q6_K 0.386667 0 0 0.775 1 0 0.5 0.813333 0 0.386111 0.386389 0.363306 code-llama-instruct:7:ggufv2:Q2_K 0.533333 0.25 0 0.8 0.333333 0.0625 0.7 0.92 0.25 0.427685 0.380509 0.292686 code-llama-instruct:34:ggufv2:Q4_K_M 0.466667 0 0 0.975 0.5 0 0.4 0.906667 0 0.360926 0.380463 0.350483 llama-2-chat:7:ggufv2:Q3_K_M 0.233333 0.333333 0 0.466667 1 0.1 1 0.693333 0 0.425185 0.379259 0.353401 code-llama-instruct:7:ggufv2:Q6_K 0.333333 0 0 0.775 0.833333 0 0.9 0.96 0 0.422407 0.37787 0.391629 code-llama-instruct:34:ggufv2:Q5_K_M 0.466667 0.125 0 0.95 0.333333 0 1 0.9 0 0.419444 0.376389 0.384096 code-llama-instruct:34:ggufv2:Q8_0 0.466667 0.25 0 0.925 0.333333 0 0.9 0.86 0 0.415 0.374167 0.353285 llama-2-chat:13:ggufv2:Q4_K_M 0.366667 0 0 0.777778 1 0 0.5 0.76 0 0.378272 0.372469 0.35766 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 0.426667 0.333333 0 0.755556 0.166667 0.1625 1 0.76 0 0.400525 0.366929 0.324507 code-llama-instruct:34:ggufv2:Q6_K 0.473333 0.125 0 0.9 0.333333 0 0.9 0.853333 0 0.398333 0.365833 0.356636 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 0.333333 0.422222 0 0.711111 0 0 1 0.84 0.25 0.395185 0.364259 0.340366 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 0.333333 0.475 0 0.85 0 0 0.7 0.826667 0.25 0.381667 0.3575 0.313787 llama-2-chat:7:ggufv2:Q8_0 0.266667 0.444444 0 0.355556 1 0 0.5 0.64 0 0.356296 0.355926 0.302016 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 0.38 0.333333 0 0.777778 0 0.065625 0.5 0.893333 0.25 0.355563 0.344448 0.289411 llama-2-chat:7:ggufv2:Q6_K 0.266667 0.375 0 0.333333 1 0 0.5 0.66 0 0.348333 0.340833 0.302733 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 0.386667 0.311111 0 0.666667 0.133333 0 0.6 0.846667 0.25 0.354938 0.333025 0.267296 code-llama-instruct:13:ggufv2:Q4_K_M 0.533333 0 0 0.775 0.333333 0 0.5 0.833333 0 0.330556 0.331944 0.30939 llama-2-chat:7:ggufv2:Q5_K_M 0.293333 0.444444 0 0.288889 1 0.0375 0.6 0.633333 0 0.366389 0.329861 0.303743 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 0.48 0 0 0.733333 0.333333 0 0.6 0.726667 0 0.319259 0.326296 0.291554 llama-2-chat:13:ggufv2:Q2_K 0.366667 0 0 0.288889 1 0 0.5 0.433333 0 0.287654 0.288272 0.301824 llama-2-chat:7:ggufv2:Q2_K 0.1 0 0 0.688889 0.833333 0 1 0.686667 0 0.367654 0.233827 0.380825 code-llama-instruct:13:ggufv2:Q2_K 0.566667 0 0 0.875 0.0333333 0 0.4 0.82 0 0.299444 0.166389 0.336056 code-llama-instruct:13:ggufv2:Q3_K_M 0.533333 0.45 0 0.85 0 0 0 0.833333 0 0.296296 0.148148 0.336707"},{"location":"benchmarking/","title":"Benchmarking","text":"<p>For trustworthy application of LLMs to real-world and biomedical problems, it is imperative to understand their performance and limitations. We need to constantly evaluate the multitude of combinations of individual models and versions, their parameters (e.g., temperature), prompt sets, databases and vector databases, and diverse application scenarios. To this end, we are maintaining a living benchmarking framework that allows us to continuously compare the performance of different models and configurations on a variety of tasks. The procedure of the living benchmark follows the idea of test-driven development, in that test cases are added with practical reference to a desired feature or behaviour. For cases where a model does not yield a perfect response out of the box, which will be most cases, modifications to the components of the framework, such as prompts or functions, are made to improve the model's performance. By tracking model performance on these tests over time, we can gain insights into the reliability of the framework and identify the gaps to be addressed.</p>"},{"location":"benchmarking/#running-the-benchmark","title":"Running the benchmark","text":"<p>The benchmark uses the pytest framework to orchestrate the evaluation of a number of models on a number of tasks. The benchmark is run on a regular basis, and the results are published in the benchmark section. The benchmarking suite can be found in the <code>benchmark</code> directory of the BioChatter repository. It can be executed using standard pytest syntax, e.g., <code>poetry run pytest benchmark</code>. As default behavior it checks, which test cases have already been executed and only executes the tests that have not been executed yet. To run all benchmarks again, use <code>poetry run pytest benchmark --run-all</code>. If you want to develop and debug the benchmark instead of just running it, please refer to the developer docs.</p> <p>To allow flexible extension of the benchmark, we have implemeted a modular test framework that uses pytest fixtures to allow easy addition of new models and tasks. All setup is done in the <code>conftest.py</code> file in the <code>benchmark</code> directory. The benchmarks are organised by module, task, and subtask. Module names (e.g., <code>biocypher_query_generation</code> or <code>rag_interpretation</code>) group tasks by use case and are used to create benchmark dataset objects for the tests. Tasks (e.g., <code>entity_selection</code> or <code>query_generation</code>) are the actual tests that are executed in each module. The name of the test (minus the <code>test_</code> prefix) is used to generate the result file name. Subtasks (e.g., <code>single_word</code> or <code>multi_word</code>) are used to parametrise the tests and track the results of different subtasks in the result files. In addition, we generate md5 hashes of the test data for each subtask and use them to skip tests that have already been executed. Exemplary tests are defined in a YAML file in the <code>benchmark/data</code> directory, while the actual test dataset is encrypted and only available to the benchmarking pipeline. The result files are simple CSVs whose file names are generated from the name of the test function; they can be found in <code>benchmark/results</code> and contain scores for all executed combination of parameters.</p> <p>To achieve modularity, we use pytest fixtures and parametrization. For instance, to add a new model, we can modify the <code>OPENAI_MODEL_NAMES</code> and <code>XINFERENCE_MODELS</code> dictionary in <code>conftest.py</code>. The latter carries model names, sizes, format, and quantisations. The environment that runs the benchmark needs to make available all prerequisites for the different modules. For instance, the tasks requiring connection to an LLM need to provide the necessary credentials and API keys, or a connection to a self-hosted model. Likewise, the benchmarks of retrieval-augmented generation (RAG) processes require a connection to the RAG agent, e.g., a vector database.</p>"},{"location":"benchmarking/#calibration","title":"Calibration","text":"<p>To ensure valid assessment of LLM performance, we need to ensure calibration and technical validity of the benchmarking framework. More recent LLMs in particular may be problematic when using publicly available benchmark datasets, since they could have been used for training the model. This is particularly relevant in closed-source (e.g., OpenAI) models. Thus, we need to consider strategies for avoiding contamination, such as hand-crafting datasets, carefully testing for contamination, or using perturbation strategies to generate new datasets from existing ones. Advanced scenarios could utilise LLMs as \"examiners,\" allowing more flexible test design and free-form answers. There is much research into these phenomena, all of which should be considered in the maintenance of this testing framework.</p>"},{"location":"benchmarking/#aspects-of-benchmarking","title":"Aspects of benchmarking","text":"<p>In the following, we will detail the different aspects of benchmarking that we are currently testing. This is a living document that will be updated as we add new tests and test modules.</p>"},{"location":"benchmarking/#models","title":"Models","text":"<p>Naturally the biggest impact on BioChatter performance comes with the model used. However, model versions can have a significant impact, which can be obfuscated by the fact that model names are often not unique. For instance, OpenAI's GPT models often have versions with significantly diverging capabilities and performance. Open-source models even come in a range of sizes and quantisations, which can have a significant impact on performance.</p>"},{"location":"benchmarking/#prompts","title":"Prompts","text":"<p>As has been recently studied extensively, prompt engineering can make or break the performance of a model on a given task. As such, it is important to test the default prompts we commonly use, as well as a range of variations to determine factors of prompt performance and robustness. As an added complexity, LLMs are often used to generate prompts, which theoretically allows for procedural generation of an infinite number of prompts, as long as time and resources allow.</p>"},{"location":"benchmarking/#model-parameters","title":"Model parameters","text":"<p>The parameters of the model can have a significant impact on the performance of the model. We often set model temperature to 0 to provide consistent results, but some applications may benefit from a higher temperature. In testing, we mostly rely on a temperature of 0 due to the complexity of testing highly variable results in most cases.</p>"},{"location":"benchmarking/#databases","title":"Databases","text":"<p>An important facet of BioChatter and BioCypher is their combination in querying databases. This helps to ameliorate the limitations of LLMs by providing structured and validated knowledge to counteract hallucinations. To ensure the seamless interaction of BioChatter and BioCypher, we need to test the performance of BioChatter on a variety of databases.</p>"},{"location":"benchmarking/#vector-databases","title":"Vector databases","text":"<p>Similarly to regular databases, vector databases are an important tool to provide validated knowledge to LLMs. Vector databases bring their own set of parameters and application scenarios, which likewise need to be tested. For instance, the length and overlap of fragments, the embedding algorithms, as well as the semantic search algorithms applied can have an impact on LLM conversation performance.</p>"},{"location":"benchmarking/#tasks","title":"Tasks","text":"<p>There is a wide range of tasks that are potentially useful to BioChatter users. To cover most scenarios of research and development use, as well as clinical applications, we test a variety of tasks and LLM personas.</p>"},{"location":"chat/","title":"Basic Usage: Chat","text":"<p>BioChatter provides access to chat functionality via the <code>Conversation</code> class, which is implemented in several child classes (in the <code>llm_connect.py</code> module) to account for differences in APIs of the LLMs.</p>"},{"location":"chat/#setting-up-the-conversation","title":"Setting up the conversation","text":"<p>To start a conversation, we can initialise the Conversation class (here exemplified by GPT):</p> <pre><code>from biochatter.llm_connect import GptConversation\n\nconversation = GptConversation(\n    model_name=\"gpt-3.5-turbo\",\n    prompts={},\n)\n</code></pre> <p>It is possible to supply a dictionary of prompts to the conversation from the outset, which is formatted in a way to correspond to the different roles of the conversation, i.e., primary and correcting models. Prompts with the <code>primary_model_prompts</code> key will be appended to the System Messages of the primary model, and <code>correcting_agent_prompts</code> will be appended to the System Messages of the correction model at setup. If we pass a dictionary without these keys (or an empty one), there will be no system messages appended to the models. They can however be introduced later by using the following method:</p> <pre><code>conversation.append_system_message(\"System Message\")\n</code></pre> <p>Similarly, the user queries (<code>HumanMessage</code>) are passed to the conversation using <code>convo.append_user_message(\"User Message\")</code>. For purposes of keeping track of the conversation history, we can also append the model's responses as <code>AIMessage</code> using <code>convo.append_ai_message</code>. </p>"},{"location":"chat/#querying-the-model","title":"Querying the model","text":"<p>After setting up the conversation in this way, for instance by establishing a flattery component (e.g. 'You are an assistant to a researcher ...'), the model can be queried using the <code>query</code> function.</p> <pre><code>msg, token_usage, correction = conversation.query('Question here')\n</code></pre> <p>Note that a query will automatically append a user message to the message history, so there is no need to call <code>append_user_message()</code> again. The query function returns the actual answer of the model (<code>msg</code>), the token usage statistics reported by the API (<code>token_usage</code>), and an optional <code>correction</code> that contains the opinion of the corrective agent.</p>"},{"location":"chat/#using-openai-models","title":"Using OpenAI models","text":"<p>Using an OpenAI model via the API is generally the easiest way to get started, but requires the provision of an API key to the OpenAI API. To do this, you can designate the <code>OPENAI_API_KEY</code> variable in your environment directly (<code>export OPENAI_API_KEY=sk-...</code>) by adding it to your shell configuration (e.g., the <code>zshrc</code>).</p>"},{"location":"kgs/","title":"Connecting Knowledge Graphs","text":"<p>To increase accessibility of databases, we can leverage the BioCypher integration of BioChatter.  In BioCypher, we use a YAML configuration (<code>schema_config.yaml</code>) to specify the contents of the knowledge graph and their ontological associations.  We also generate a more extensive, but essentially similar YAML file during the BioCypher creation of a knowledge graph (<code>schema_info.yaml</code>), which contains more information pertinent to LLM interaction with the database.  The current prototypical implementation of query generation through an LLM is implemented in the <code>prompts.py</code> module on the example of a Neo4j knowledge graph connection.</p>"},{"location":"kgs/#connecting","title":"Connecting","text":"<p>Currently, BioChatter does not handle database connectivity, but simply returns a query for a given language.  The application using BioChatter should establish connectivity and send the query to the database, as is implemented in ChatGSE, for instance.  For a demonstration using a simple Docker compose setup, see the Pole Crime Dataset demo repository.</p>"},{"location":"kgs/#querying","title":"Querying","text":"<p>The generation of a query based on BioCypher configuration files is a multi-step process.  This is partly to account for the limited token input space of some models, and partly to better be able to test and compare the individual steps. We will implement a wrapper function that goes through the steps automatically soon, but for now the steps need to be run individually.</p>"},{"location":"kgs/#setup","title":"Setup","text":"<p>We use the <code>BioCypherPromptEngine</code> class to handle the LLM conversation.</p> <pre><code>from biochatter.prompts import BioCypherPromptEngine\nprompt_engine = BioCypherPromptEngine(\n    schema_config_or_info_path=\"test/schema_info.yaml\"\n)\n</code></pre> <p>This will load the <code>schema_config.yaml</code> or <code>schema_info.yaml</code> (preferred) file and set up the conversation.</p>"},{"location":"kgs/#query-generation","title":"Query generation","text":"<p>Using the <code>generate_query</code> wrapper, we can generate a query from a question and a database language.</p> <pre><code>query = prompt_engine.generate_query(\n    question=\"Which genes are associated with mucoviscidosis?\",\n    database_language=\"Cypher\",\n)\n</code></pre> <p>This will return a query that can be used in the database query language (e.g., Cypher).  This end to end process executes the steps detailed below, namely, entity selection, relationship selection, and property selection, as well as the generation of the final query using the selected components. You can run each of these steps individually, if you want.</p>"},{"location":"kgs/#entity-selection","title":"Entity selection","text":"<p>Starting from the <code>schema_config.yaml</code> or <code>schema_info.yaml</code> (preferred) file, we first have the model decide which entities in the database are relevant to the user's question.</p> <pre><code>success = prompt_engine._select_entities(\n    question=\"Which genes are associated with mucoviscidosis?\"\n)\n</code></pre> <p>This will select a number of entities from the database schema to be used subsequently, and return True or False to indicate success.</p>"},{"location":"kgs/#relationship-selection","title":"Relationship selection","text":"<p>Next, we will use the entities determined in the first step to select relationships between them.  The entities selected in the first step will be stored in the <code>selected_entities</code> attribute of the <code>BioCypherPromptEngine</code> instance, and the question is stored in the <code>question</code> attribute.  Both are automatically used to select relationships.</p> <pre><code>success = prompt_engine._select_relationships()\n</code></pre>"},{"location":"kgs/#property-selection","title":"Property selection","text":"<p>To not unnecessarily waste token input space, we are only interested in selecting properties of entities that are of interest given the question asked. We do so in the third step, which uses the entities and relationships determined in the first steps.  Again, <code>question</code>, <code>selected_entities</code>, and <code>selected_relationships</code> are automatically used to select properties.</p> <pre><code>success = prompt_engine._select_properties()\n</code></pre> <p>This will select a number of properties to be used in the query, and also return True or False to indicate success.</p>"},{"location":"kgs/#query-generation_1","title":"Query generation","text":"<p>Finally, we can use the entities and relationships, as well as the selected properties, to ask the LLM to generate a query in the desired language.</p> <pre><code>query = prompt_engine._generate_query(\n    question=\"Which genes are associated with mucoviscidosis?\",\n    entities=[\"Gene\", \"Disease\"],\n    relationships=[\"GeneToDiseaseAssociation\"],\n    properties={\"Disease\": [\"name\", \"ICD10\", \"DSM5\"]},\n    database_language=\"Cypher\",\n)\n</code></pre> <p>This will (hopefully) return a query that can be used in the database query language (e.g., Cypher).</p>"},{"location":"kgs/#query-interaction","title":"Query interaction","text":"<p>As an optional follow-up, you can interact with the returned query using the  <code>BioCypherQueryHandler</code> class (<code>query_interaction.py</code>). It takes the query,  the original question and the KG information so that the interaction is still aware of the KG. </p> <pre><code>from biochatter.query_interaction import BioCypherQueryHandler\nquery_handler = BioCypherQueryHandler(\n    query=query,\n    query_lang=\"Cypher\",\n    kg_selected={\n        entities: [\"Gene\", \"Disease\"],\n        relationships: [\"GeneToDiseaseAssociation\"],\n        properties: {\"Disease\": [\"name\", \"ICD10\", \"DSM5\"]}\n    },\n    question=\"Which genes are associated with mucoviscidosis?\"\n)\n</code></pre>"},{"location":"kgs/#explanation","title":"Explanation","text":"<p>You can retrieve an explanation of the returned query with:</p> <pre><code>explanation = query_handler.explain_query()\n</code></pre>"},{"location":"kgs/#updating","title":"Updating","text":"<p>Alternatively, you can ask the LLM for an update of the query with:</p> <pre><code>request = \"Only return 10 results and sort them alphabetically\"\nexplanation = query_handler.update_query(request)\n</code></pre> <p>NB: for updates, it might sometimes be relevant that all the KG  enitites/relationships/properties are known to the LLM instead  of only those that were selected to be relevant for the original question. For this, you can optionally pass them as input to the query handler  with <code>kg</code> (similar to <code>kg_selected</code>).</p> <p>(Tip: the prompt_engine object contains both the selected and non-selected as attributes)</p>"},{"location":"llm_connect-reference/","title":"LLM connectivity module","text":"<p>Here we handle connections to various LLM services, proprietary and open source.</p>"},{"location":"llm_connect-reference/#biochatter.llm_connect.AzureGptConversation","title":"<code>AzureGptConversation</code>","text":"<p>             Bases: <code>GptConversation</code></p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>class AzureGptConversation(GptConversation):\n    def __init__(\n        self,\n        deployment_name: str,\n        model_name: str,\n        prompts: dict,\n        correct: bool = True,\n        split_correction: bool = False,\n        version: Optional[str] = None,\n        base_url: Optional[str] = None,\n    ):\n        \"\"\"\n        Connect to Azure's GPT API and set up a conversation with the user.\n        Extends GptConversation.\n\n        Args:\n            deployment_name (str): The name of the Azure deployment to use.\n\n            model_name (str): The name of the model to use. This is distinct\n                from the deployment name.\n\n            prompts (dict): A dictionary of prompts to use for the conversation.\n\n            split_correction (bool): Whether to correct the model output by\n                splitting the output into sentences and correcting each\n                sentence individually.\n\n            version (str): The version of the Azure API to use.\n\n            base_url (str): The base URL of the Azure API to use.\n        \"\"\"\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            correct=correct,\n            split_correction=split_correction,\n        )\n\n        self.version = version\n        self.base_url = base_url\n        self.deployment_name = deployment_name\n\n    def set_api_key(self, api_key: str, user: Optional[str] = None):\n        \"\"\"\n        Set the API key for the Azure API. If the key is valid, initialise the\n        conversational agent. No user stats on Azure.\n\n        Args:\n            api_key (str): The API key for the Azure API.\n\n        Returns:\n            bool: True if the API key is valid, False otherwise.\n        \"\"\"\n\n        try:\n            self.chat = AzureChatOpenAI(\n                deployment_name=self.deployment_name,\n                model_name=self.model_name,\n                openai_api_version=self.version,\n                azure_endpoint=self.base_url,\n                openai_api_key=api_key,\n                temperature=0,\n            )\n            # TODO this is the same model as the primary one; refactor to be\n            # able to use any model for correction\n            self.ca_chat = AzureChatOpenAI(\n                deployment_name=self.deployment_name,\n                model_name=self.model_name,\n                openai_api_version=self.version,\n                azure_endpoint=self.base_url,\n                openai_api_key=api_key,\n                temperature=0,\n            )\n\n            test = self.chat.generate([[HumanMessage(content=\"Hello\")]])\n\n            return True\n\n        except openai._exceptions.AuthenticationError as e:\n            return False\n\n    def _update_usage_stats(self, model: str, token_usage: dict):\n        \"\"\"\n        We do not track usage stats for Azure.\n        \"\"\"\n        return\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.AzureGptConversation.__init__","title":"<code>__init__(deployment_name, model_name, prompts, correct=True, split_correction=False, version=None, base_url=None)</code>","text":"<p>Connect to Azure's GPT API and set up a conversation with the user. Extends GptConversation.</p> <p>Parameters:</p> Name Type Description Default <code>deployment_name</code> <code>str</code> <p>The name of the Azure deployment to use.</p> required <code>model_name</code> <code>str</code> <p>The name of the model to use. This is distinct from the deployment name.</p> required <code>prompts</code> <code>dict</code> <p>A dictionary of prompts to use for the conversation.</p> required <code>split_correction</code> <code>bool</code> <p>Whether to correct the model output by splitting the output into sentences and correcting each sentence individually.</p> <code>False</code> <code>version</code> <code>str</code> <p>The version of the Azure API to use.</p> <code>None</code> <code>base_url</code> <code>str</code> <p>The base URL of the Azure API to use.</p> <code>None</code> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def __init__(\n    self,\n    deployment_name: str,\n    model_name: str,\n    prompts: dict,\n    correct: bool = True,\n    split_correction: bool = False,\n    version: Optional[str] = None,\n    base_url: Optional[str] = None,\n):\n    \"\"\"\n    Connect to Azure's GPT API and set up a conversation with the user.\n    Extends GptConversation.\n\n    Args:\n        deployment_name (str): The name of the Azure deployment to use.\n\n        model_name (str): The name of the model to use. This is distinct\n            from the deployment name.\n\n        prompts (dict): A dictionary of prompts to use for the conversation.\n\n        split_correction (bool): Whether to correct the model output by\n            splitting the output into sentences and correcting each\n            sentence individually.\n\n        version (str): The version of the Azure API to use.\n\n        base_url (str): The base URL of the Azure API to use.\n    \"\"\"\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        correct=correct,\n        split_correction=split_correction,\n    )\n\n    self.version = version\n    self.base_url = base_url\n    self.deployment_name = deployment_name\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.AzureGptConversation.set_api_key","title":"<code>set_api_key(api_key, user=None)</code>","text":"<p>Set the API key for the Azure API. If the key is valid, initialise the conversational agent. No user stats on Azure.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The API key for the Azure API.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the API key is valid, False otherwise.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def set_api_key(self, api_key: str, user: Optional[str] = None):\n    \"\"\"\n    Set the API key for the Azure API. If the key is valid, initialise the\n    conversational agent. No user stats on Azure.\n\n    Args:\n        api_key (str): The API key for the Azure API.\n\n    Returns:\n        bool: True if the API key is valid, False otherwise.\n    \"\"\"\n\n    try:\n        self.chat = AzureChatOpenAI(\n            deployment_name=self.deployment_name,\n            model_name=self.model_name,\n            openai_api_version=self.version,\n            azure_endpoint=self.base_url,\n            openai_api_key=api_key,\n            temperature=0,\n        )\n        # TODO this is the same model as the primary one; refactor to be\n        # able to use any model for correction\n        self.ca_chat = AzureChatOpenAI(\n            deployment_name=self.deployment_name,\n            model_name=self.model_name,\n            openai_api_version=self.version,\n            azure_endpoint=self.base_url,\n            openai_api_key=api_key,\n            temperature=0,\n        )\n\n        test = self.chat.generate([[HumanMessage(content=\"Hello\")]])\n\n        return True\n\n    except openai._exceptions.AuthenticationError as e:\n        return False\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.BloomConversation","title":"<code>BloomConversation</code>","text":"<p>             Bases: <code>Conversation</code></p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>class BloomConversation(Conversation):\n    def __init__(\n        self,\n        model_name: str,\n        prompts: dict,\n        split_correction: bool,\n    ):\n        \"\"\"\n        DEPRECATED: Superceded by XinferenceConversation.\n        \"\"\"\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            split_correction=split_correction,\n        )\n\n        self.messages = []\n\n    def set_api_key(self, api_key: str, user: Optional[str] = None):\n        self.chat = HuggingFaceHub(\n            repo_id=self.model_name,\n            model_kwargs={\"temperature\": 1.0},  # \"regular sampling\"\n            # as per https://huggingface.co/docs/api-inference/detailed_parameters\n            huggingfacehub_api_token=api_key,\n        )\n\n        try:\n            self.chat.generate([\"Hello, I am a biomedical researcher.\"])\n            return True\n        except ValueError as e:\n            return False\n\n    def _cast_messages(self, messages):\n        \"\"\"\n        Render the different roles of the chat-based conversation as plain text.\n        \"\"\"\n        cast = \"\"\n        for m in messages:\n            if isinstance(m, SystemMessage):\n                cast += f\"System: {m.content}\\n\"\n            elif isinstance(m, HumanMessage):\n                cast += f\"Human: {m.content}\\n\"\n            elif isinstance(m, AIMessage):\n                cast += f\"AI: {m.content}\\n\"\n            else:\n                raise ValueError(f\"Unknown message type: {type(m)}\")\n\n        return cast\n\n    def _primary_query(self):\n        response = self.chat.generate([self._cast_messages(self.messages)])\n\n        msg = response.generations[0][0].text\n        token_usage = {\n            \"prompt_tokens\": 0,\n            \"completion_tokens\": 0,\n            \"total_tokens\": 0,\n        }\n\n        self.append_ai_message(msg)\n\n        return msg, token_usage\n\n    def _correct_response(self, msg: str):\n        return \"ok\"\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.BloomConversation.__init__","title":"<code>__init__(model_name, prompts, split_correction)</code>","text":"<p>DEPRECATED: Superceded by XinferenceConversation.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    prompts: dict,\n    split_correction: bool,\n):\n    \"\"\"\n    DEPRECATED: Superceded by XinferenceConversation.\n    \"\"\"\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        split_correction=split_correction,\n    )\n\n    self.messages = []\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.Conversation","title":"<code>Conversation</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Use this class to set up a connection to an LLM API. Can be used to set the user name and API key, append specific messages for system, user, and AI roles (if available), set up the general context as well as manual and tool-based data inputs, and finally to query the API with prompts made by the user.</p> <p>The conversation class is expected to have a <code>messages</code> attribute to store the conversation, and a <code>history</code> attribute, which is a list of messages in a specific format for logging / printing.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>class Conversation(ABC):\n    \"\"\"\n\n    Use this class to set up a connection to an LLM API. Can be used to set the\n    user name and API key, append specific messages for system, user, and AI\n    roles (if available), set up the general context as well as manual and\n    tool-based data inputs, and finally to query the API with prompts made by\n    the user.\n\n    The conversation class is expected to have a `messages` attribute to store\n    the conversation, and a `history` attribute, which is a list of messages in\n    a specific format for logging / printing.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        prompts: dict,\n        correct: bool = True,\n        split_correction: bool = False,\n    ):\n        super().__init__()\n        self.model_name = model_name\n        self.prompts = prompts\n        self.correct = correct\n        self.split_correction = split_correction\n        self.rag_agents: List[RagAgent] = []\n        self.history = []\n        self.messages = []\n        self.ca_messages = []\n        self.current_statements = []\n\n    def set_user_name(self, user_name: str):\n        self.user_name = user_name\n\n    def set_rag_agent(self, agent: RagAgent):\n        \"\"\"\n        Update or insert rag_agent: if the rag_agent with the same mode already\n        exists, it will be updated. Otherwise, the new rag_agent will be inserted.\n        \"\"\"\n        i, _ = self._find_rag_agent(agent.mode)\n        if i &lt; 0:\n            # insert\n            self.rag_agents.append(agent)\n        else:\n            # update\n            self.rag_agents[i] = agent\n\n    def _find_rag_agent(self, mode: str) -&gt; Tuple[int, RagAgent]:\n        for i, val in enumerate(self.rag_agents):\n            if val.mode == mode:\n                return i, val\n        return -1, None\n\n    @abstractmethod\n    def set_api_key(self, api_key: str, user: Optional[str] = None):\n        pass\n\n    def get_prompts(self):\n        return self.prompts\n\n    def set_prompts(self, prompts: dict):\n        self.prompts = prompts\n\n    def append_ai_message(self, message: str):\n        self.messages.append(\n            AIMessage(\n                content=message,\n            ),\n        )\n\n    def append_system_message(self, message: str):\n        self.messages.append(\n            SystemMessage(\n                content=message,\n            ),\n        )\n\n    def append_ca_message(self, message: str):\n        self.ca_messages.append(\n            SystemMessage(\n                content=message,\n            ),\n        )\n\n    def append_user_message(self, message: str):\n        self.messages.append(\n            HumanMessage(\n                content=message,\n            ),\n        )\n\n    def setup(self, context: str):\n        \"\"\"\n        Set up the conversation with general prompts and a context.\n        \"\"\"\n        for msg in self.prompts[\"primary_model_prompts\"]:\n            if msg:\n                self.append_system_message(msg)\n\n        for msg in self.prompts[\"correcting_agent_prompts\"]:\n            if msg:\n                self.append_ca_message(msg)\n\n        self.context = context\n        msg = f\"The topic of the research is {context}.\"\n        self.append_system_message(msg)\n\n    def setup_data_input_manual(self, data_input: str):\n        self.data_input = data_input\n        msg = f\"The user has given information on the data input: {data_input}.\"\n        self.append_system_message(msg)\n\n    def setup_data_input_tool(self, df, input_file_name: str):\n        self.data_input_tool = df\n\n        for tool_name in self.prompts[\"tool_prompts\"]:\n            if tool_name in input_file_name:\n                msg = self.prompts[\"tool_prompts\"][tool_name].format(df=df)\n                self.append_system_message(msg)\n\n    def query(self, text: str):\n        self.append_user_message(text)\n\n        self._inject_context(text)\n\n        msg, token_usage = self._primary_query()\n\n        if not token_usage:\n            # indicates error\n            return (msg, token_usage, None)\n\n        if not self.correct:\n            return (msg, token_usage, None)\n\n        cor_msg = (\n            \"Correcting (using single sentences) ...\"\n            if self.split_correction\n            else \"Correcting ...\"\n        )\n\n        if st:\n            with st.spinner(cor_msg):\n                corrections = self._correct_query(text)\n        else:\n            corrections = self._correct_query(text)\n\n        if not corrections:\n            return (msg, token_usage, None)\n\n        correction = \"\\n\".join(corrections)\n        return (msg, token_usage, correction)\n\n    def _correct_query(self, msg: str):\n        corrections = []\n        if self.split_correction:\n            nltk.download(\"punkt\")\n            tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n            sentences = tokenizer.tokenize(msg)\n            for sentence in sentences:\n                correction = self._correct_response(sentence)\n\n                if not str(correction).lower() in [\"ok\", \"ok.\"]:\n                    corrections.append(correction)\n        else:\n            correction = self._correct_response(msg)\n\n            if not str(correction).lower() in [\"ok\", \"ok.\"]:\n                corrections.append(correction)\n\n        return corrections\n\n    @abstractmethod\n    def _primary_query(self, text: str):\n        pass\n\n    @abstractmethod\n    def _correct_response(self, msg: str):\n        pass\n\n    def _inject_context(self, text: str):\n        \"\"\"\n\n        Inject the context received from the RAG agent into the prompt. The RAG\n        agent will find the most similar n text fragments and add them to the\n        message history object for usage in the next prompt. Uses the document\n        summarisation prompt set to inject the context. The ultimate prompt\n        should include the placeholder for the statements, `{statements}` (used\n        for formatting the string).\n\n        Args:\n            text (str): The user query to be used for similarity search.\n        \"\"\"\n\n        sim_msg = f\"Performing similarity search to inject fragments ...\"\n\n        if st:\n            with st.spinner(sim_msg):\n                statements = []\n                for agent in self.rag_agents:\n                    try:\n                        docs = agent.generate_responses(text)\n                        statements = statements + [doc[0] for doc in docs]\n                    except ValueError as e:\n                        logger.warning(e)\n\n        else:\n            statements = []\n            for agent in self.rag_agents:\n                try:\n                    docs = agent.generate_responses(text)\n                    statements = statements + [doc[0] for doc in docs]\n                except ValueError as e:\n                    logger.warning(e)\n\n        if statements and len(statements) &gt; 0:\n            prompts = self.prompts[\"rag_agent_prompts\"]\n            self.current_statements = statements\n            for i, prompt in enumerate(prompts):\n                # if last prompt, format the statements into the prompt\n                if i == len(prompts) - 1:\n                    self.append_system_message(\n                        prompt.format(statements=statements)\n                    )\n                else:\n                    self.append_system_message(prompt)\n\n    def get_last_injected_context(self) -&gt; List[dict]:\n        \"\"\"\n        Get a formatted list of the last context injected into the\n        conversation. Contains one dictionary for each RAG mode.\n\n        Returns:\n            List[dict]: A list of dictionaries containing the mode and context\n            for each RAG agent.\n        \"\"\"\n        last_context = []\n        for agent in self.rag_agents:\n            last_context.append(\n                {\"mode\": agent.mode, \"context\": agent.last_response}\n            )\n        return last_context\n\n    def get_msg_json(self):\n        \"\"\"\n        Return a JSON representation (of a list of dicts) of the messages in\n        the conversation. The keys of the dicts are the roles, the values are\n        the messages.\n\n        Returns:\n            str: A JSON representation of the messages in the conversation.\n        \"\"\"\n        d = []\n        for msg in self.messages:\n            if isinstance(msg, SystemMessage):\n                role = \"system\"\n            elif isinstance(msg, HumanMessage):\n                role = \"user\"\n            elif isinstance(msg, AIMessage):\n                role = \"ai\"\n            else:\n                raise ValueError(f\"Unknown message type: {type(msg)}\")\n\n            d.append({role: msg.content})\n\n        return json.dumps(d)\n\n    def reset(self):\n        \"\"\"\n        Resets the conversation to the initial state.\n        \"\"\"\n\n        self.history = []\n        self.messages = []\n        self.ca_messages = []\n        self.current_statements = []\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.Conversation.get_last_injected_context","title":"<code>get_last_injected_context()</code>","text":"<p>Get a formatted list of the last context injected into the conversation. Contains one dictionary for each RAG mode.</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: A list of dictionaries containing the mode and context</p> <code>List[dict]</code> <p>for each RAG agent.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def get_last_injected_context(self) -&gt; List[dict]:\n    \"\"\"\n    Get a formatted list of the last context injected into the\n    conversation. Contains one dictionary for each RAG mode.\n\n    Returns:\n        List[dict]: A list of dictionaries containing the mode and context\n        for each RAG agent.\n    \"\"\"\n    last_context = []\n    for agent in self.rag_agents:\n        last_context.append(\n            {\"mode\": agent.mode, \"context\": agent.last_response}\n        )\n    return last_context\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.Conversation.get_msg_json","title":"<code>get_msg_json()</code>","text":"<p>Return a JSON representation (of a list of dicts) of the messages in the conversation. The keys of the dicts are the roles, the values are the messages.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>A JSON representation of the messages in the conversation.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def get_msg_json(self):\n    \"\"\"\n    Return a JSON representation (of a list of dicts) of the messages in\n    the conversation. The keys of the dicts are the roles, the values are\n    the messages.\n\n    Returns:\n        str: A JSON representation of the messages in the conversation.\n    \"\"\"\n    d = []\n    for msg in self.messages:\n        if isinstance(msg, SystemMessage):\n            role = \"system\"\n        elif isinstance(msg, HumanMessage):\n            role = \"user\"\n        elif isinstance(msg, AIMessage):\n            role = \"ai\"\n        else:\n            raise ValueError(f\"Unknown message type: {type(msg)}\")\n\n        d.append({role: msg.content})\n\n    return json.dumps(d)\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.Conversation.reset","title":"<code>reset()</code>","text":"<p>Resets the conversation to the initial state.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def reset(self):\n    \"\"\"\n    Resets the conversation to the initial state.\n    \"\"\"\n\n    self.history = []\n    self.messages = []\n    self.ca_messages = []\n    self.current_statements = []\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.Conversation.set_rag_agent","title":"<code>set_rag_agent(agent)</code>","text":"<p>Update or insert rag_agent: if the rag_agent with the same mode already exists, it will be updated. Otherwise, the new rag_agent will be inserted.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def set_rag_agent(self, agent: RagAgent):\n    \"\"\"\n    Update or insert rag_agent: if the rag_agent with the same mode already\n    exists, it will be updated. Otherwise, the new rag_agent will be inserted.\n    \"\"\"\n    i, _ = self._find_rag_agent(agent.mode)\n    if i &lt; 0:\n        # insert\n        self.rag_agents.append(agent)\n    else:\n        # update\n        self.rag_agents[i] = agent\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.Conversation.setup","title":"<code>setup(context)</code>","text":"<p>Set up the conversation with general prompts and a context.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def setup(self, context: str):\n    \"\"\"\n    Set up the conversation with general prompts and a context.\n    \"\"\"\n    for msg in self.prompts[\"primary_model_prompts\"]:\n        if msg:\n            self.append_system_message(msg)\n\n    for msg in self.prompts[\"correcting_agent_prompts\"]:\n        if msg:\n            self.append_ca_message(msg)\n\n    self.context = context\n    msg = f\"The topic of the research is {context}.\"\n    self.append_system_message(msg)\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.GptConversation","title":"<code>GptConversation</code>","text":"<p>             Bases: <code>Conversation</code></p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>class GptConversation(Conversation):\n    def __init__(\n        self,\n        model_name: str,\n        prompts: dict,\n        correct: bool = True,\n        split_correction: bool = False,\n    ):\n        \"\"\"\n        Connect to OpenAI's GPT API and set up a conversation with the user.\n        Also initialise a second conversational agent to provide corrections to\n        the model output, if necessary.\n\n        Args:\n            model_name (str): The name of the model to use.\n\n            prompts (dict): A dictionary of prompts to use for the conversation.\n\n            split_correction (bool): Whether to correct the model output by\n                splitting the output into sentences and correcting each\n                sentence individually.\n        \"\"\"\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            correct=correct,\n            split_correction=split_correction,\n        )\n\n        self.ca_model_name = \"gpt-3.5-turbo\"\n        # TODO make accessible by drop-down\n\n    def set_api_key(self, api_key: str, user: str):\n        \"\"\"\n        Set the API key for the OpenAI API. If the key is valid, initialise the\n        conversational agent. Set the user for usage statistics.\n\n        Args:\n            api_key (str): The API key for the OpenAI API.\n\n            user (str): The user for usage statistics.\n\n        Returns:\n            bool: True if the API key is valid, False otherwise.\n        \"\"\"\n        client = openai.OpenAI(\n            api_key=api_key,\n        )\n        self.user = user\n\n        try:\n            client.models.list()\n            self.chat = ChatOpenAI(\n                model_name=self.model_name,\n                temperature=0,\n                openai_api_key=api_key,\n            )\n            self.ca_chat = ChatOpenAI(\n                model_name=self.ca_model_name,\n                temperature=0,\n                openai_api_key=api_key,\n            )\n            if user == \"community\":\n                self.usage_stats = get_stats(user=user)\n\n            return True\n\n        except openai._exceptions.AuthenticationError as e:\n            return False\n\n    def _primary_query(self):\n        \"\"\"\n        Query the OpenAI API with the user's message and return the response\n        using the message history (flattery system messages, prior conversation)\n        as context. Correct the response if necessary.\n\n        Returns:\n            tuple: A tuple containing the response from the OpenAI API and the\n                token usage.\n        \"\"\"\n        try:\n            response = self.chat.generate([self.messages])\n        except (\n            openai._exceptions.APIError,\n            openai._exceptions.OpenAIError,\n            openai._exceptions.ConflictError,\n            openai._exceptions.NotFoundError,\n            openai._exceptions.APIStatusError,\n            openai._exceptions.RateLimitError,\n            openai._exceptions.APITimeoutError,\n            openai._exceptions.BadRequestError,\n            openai._exceptions.APIConnectionError,\n            openai._exceptions.AuthenticationError,\n            openai._exceptions.InternalServerError,\n            openai._exceptions.PermissionDeniedError,\n            openai._exceptions.UnprocessableEntityError,\n            openai._exceptions.APIResponseValidationError,\n        ) as e:\n            return str(e), None\n\n        msg = response.generations[0][0].text\n        token_usage = response.llm_output.get(\"token_usage\")\n\n        self._update_usage_stats(self.model_name, token_usage)\n\n        self.append_ai_message(msg)\n\n        return msg, token_usage\n\n    def _correct_response(self, msg: str):\n        \"\"\"\n        Correct the response from the OpenAI API by sending it to a secondary\n        language model. Optionally split the response into single sentences and\n        correct each sentence individually. Update usage stats.\n\n        Args:\n            msg (str): The response from the OpenAI API.\n\n        Returns:\n            str: The corrected response (or OK if no correction necessary).\n        \"\"\"\n        ca_messages = self.ca_messages.copy()\n        ca_messages.append(\n            HumanMessage(\n                content=msg,\n            ),\n        )\n        ca_messages.append(\n            SystemMessage(\n                content=\"If there is nothing to correct, please respond \"\n                \"with just 'OK', and nothing else!\",\n            ),\n        )\n\n        response = self.ca_chat.generate([ca_messages])\n\n        correction = response.generations[0][0].text\n        token_usage = response.llm_output.get(\"token_usage\")\n\n        self._update_usage_stats(self.ca_model_name, token_usage)\n\n        return correction\n\n    def _update_usage_stats(self, model: str, token_usage: dict):\n        \"\"\"\n        Update redis database with token usage statistics using the usage_stats\n        object with the increment method.\n\n        Args:\n            model (str): The model name.\n\n            token_usage (dict): The token usage statistics.\n        \"\"\"\n        if self.user == \"community\":\n            self.usage_stats.increment(\n                f\"usage:[date]:[user]\",\n                {f\"{k}:{model}\": v for k, v in token_usage.items()},\n            )\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.GptConversation.__init__","title":"<code>__init__(model_name, prompts, correct=True, split_correction=False)</code>","text":"<p>Connect to OpenAI's GPT API and set up a conversation with the user. Also initialise a second conversational agent to provide corrections to the model output, if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to use.</p> required <code>prompts</code> <code>dict</code> <p>A dictionary of prompts to use for the conversation.</p> required <code>split_correction</code> <code>bool</code> <p>Whether to correct the model output by splitting the output into sentences and correcting each sentence individually.</p> <code>False</code> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    prompts: dict,\n    correct: bool = True,\n    split_correction: bool = False,\n):\n    \"\"\"\n    Connect to OpenAI's GPT API and set up a conversation with the user.\n    Also initialise a second conversational agent to provide corrections to\n    the model output, if necessary.\n\n    Args:\n        model_name (str): The name of the model to use.\n\n        prompts (dict): A dictionary of prompts to use for the conversation.\n\n        split_correction (bool): Whether to correct the model output by\n            splitting the output into sentences and correcting each\n            sentence individually.\n    \"\"\"\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        correct=correct,\n        split_correction=split_correction,\n    )\n\n    self.ca_model_name = \"gpt-3.5-turbo\"\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.GptConversation.set_api_key","title":"<code>set_api_key(api_key, user)</code>","text":"<p>Set the API key for the OpenAI API. If the key is valid, initialise the conversational agent. Set the user for usage statistics.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The API key for the OpenAI API.</p> required <code>user</code> <code>str</code> <p>The user for usage statistics.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the API key is valid, False otherwise.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def set_api_key(self, api_key: str, user: str):\n    \"\"\"\n    Set the API key for the OpenAI API. If the key is valid, initialise the\n    conversational agent. Set the user for usage statistics.\n\n    Args:\n        api_key (str): The API key for the OpenAI API.\n\n        user (str): The user for usage statistics.\n\n    Returns:\n        bool: True if the API key is valid, False otherwise.\n    \"\"\"\n    client = openai.OpenAI(\n        api_key=api_key,\n    )\n    self.user = user\n\n    try:\n        client.models.list()\n        self.chat = ChatOpenAI(\n            model_name=self.model_name,\n            temperature=0,\n            openai_api_key=api_key,\n        )\n        self.ca_chat = ChatOpenAI(\n            model_name=self.ca_model_name,\n            temperature=0,\n            openai_api_key=api_key,\n        )\n        if user == \"community\":\n            self.usage_stats = get_stats(user=user)\n\n        return True\n\n    except openai._exceptions.AuthenticationError as e:\n        return False\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.WasmConversation","title":"<code>WasmConversation</code>","text":"<p>             Bases: <code>Conversation</code></p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>class WasmConversation(Conversation):\n    def __init__(\n        self,\n        model_name: str,\n        prompts: dict,\n        correct: bool = True,\n        split_correction: bool = False,\n    ):\n        \"\"\"\n\n        This class is used to return the complete query as a string to be used\n        in the frontend running the wasm model. It does not call the API itself,\n        but updates the message history similarly to the other conversation\n        classes. It overrides the `query` method from the `Conversation` class\n        to return a plain string that contains the entire message for the model\n        as the first element of the tuple. The second and third elements are\n        `None` as there is no token usage or correction for the wasm model.\n\n        \"\"\"\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            correct=correct,\n            split_correction=split_correction,\n        )\n\n    def query(self, text: str):\n        \"\"\"\n        Return the entire message history as a single string. This is the\n        message that is sent to the wasm model.\n\n        Args:\n            text (str): The user query.\n\n            collection_name (str): The name of the collection to use for\n                retrieval-augmented generation.\n\n        Returns:\n            tuple: A tuple containing the message history as a single string,\n                and `None` for the second and third elements of the tuple.\n        \"\"\"\n        self.append_user_message(text)\n\n        self._inject_context(text)\n\n        return (self._primary_query(), None, None)\n\n    def _primary_query(self):\n        \"\"\"\n        Concatenate all messages in the conversation into a single string and\n        return it. Currently discards information about roles (system, user).\n        \"\"\"\n        return \"\\n\".join([m.content for m in self.messages])\n\n    def _correct_response(self, msg: str):\n        \"\"\"\n        This method is not used for the wasm model.\n        \"\"\"\n        return \"ok\"\n\n    def set_api_key(self, api_key: str, user: str | None = None):\n        \"\"\"\n        This method is not used for the wasm model.\n        \"\"\"\n        return True\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.WasmConversation.__init__","title":"<code>__init__(model_name, prompts, correct=True, split_correction=False)</code>","text":"<p>This class is used to return the complete query as a string to be used in the frontend running the wasm model. It does not call the API itself, but updates the message history similarly to the other conversation classes. It overrides the <code>query</code> method from the <code>Conversation</code> class to return a plain string that contains the entire message for the model as the first element of the tuple. The second and third elements are <code>None</code> as there is no token usage or correction for the wasm model.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    prompts: dict,\n    correct: bool = True,\n    split_correction: bool = False,\n):\n    \"\"\"\n\n    This class is used to return the complete query as a string to be used\n    in the frontend running the wasm model. It does not call the API itself,\n    but updates the message history similarly to the other conversation\n    classes. It overrides the `query` method from the `Conversation` class\n    to return a plain string that contains the entire message for the model\n    as the first element of the tuple. The second and third elements are\n    `None` as there is no token usage or correction for the wasm model.\n\n    \"\"\"\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        correct=correct,\n        split_correction=split_correction,\n    )\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.WasmConversation.query","title":"<code>query(text)</code>","text":"<p>Return the entire message history as a single string. This is the message that is sent to the wasm model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The user query.</p> required <code>collection_name</code> <code>str</code> <p>The name of the collection to use for retrieval-augmented generation.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing the message history as a single string, and <code>None</code> for the second and third elements of the tuple.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def query(self, text: str):\n    \"\"\"\n    Return the entire message history as a single string. This is the\n    message that is sent to the wasm model.\n\n    Args:\n        text (str): The user query.\n\n        collection_name (str): The name of the collection to use for\n            retrieval-augmented generation.\n\n    Returns:\n        tuple: A tuple containing the message history as a single string,\n            and `None` for the second and third elements of the tuple.\n    \"\"\"\n    self.append_user_message(text)\n\n    self._inject_context(text)\n\n    return (self._primary_query(), None, None)\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.WasmConversation.set_api_key","title":"<code>set_api_key(api_key, user=None)</code>","text":"<p>This method is not used for the wasm model.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def set_api_key(self, api_key: str, user: str | None = None):\n    \"\"\"\n    This method is not used for the wasm model.\n    \"\"\"\n    return True\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.XinferenceConversation","title":"<code>XinferenceConversation</code>","text":"<p>             Bases: <code>Conversation</code></p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>class XinferenceConversation(Conversation):\n    def __init__(\n        self,\n        base_url: str,\n        prompts: dict,\n        model_name: str = \"auto\",\n        correct: bool = True,\n        split_correction: bool = False,\n    ):\n        \"\"\"\n\n        Connect to an open-source LLM via the Xinference client library and set\n        up a conversation with the user.  Also initialise a second\n        conversational agent to provide corrections to the model output, if\n        necessary.\n\n        Args:\n\n            base_url (str): The base URL of the Xinference instance (should not\n            include the /v1 part).\n\n            prompts (dict): A dictionary of prompts to use for the conversation.\n\n            model_name (str): The name of the model to use. Will be mapped to\n            the according uid from the list of available models. Can be set to\n            \"auto\" to use the first available model.\n\n            correct (bool): Whether to correct the model output.\n\n            split_correction (bool): Whether to correct the model output by\n            splitting the output into sentences and correcting each sentence\n            individually.\n\n        \"\"\"\n        from xinference.client import Client\n\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            correct=correct,\n            split_correction=split_correction,\n        )\n        self.client = Client(base_url=base_url)\n\n        self.models = {}\n        self.load_models()\n\n        self.ca_model_name = model_name\n\n        self.set_api_key()\n\n        # TODO make accessible by drop-down\n\n    def load_models(self):\n        for id, model in self.client.list_models().items():\n            model[\"id\"] = id\n            self.models[model[\"model_name\"]] = model\n\n    # def list_models_by_type(self, type: str):\n    #     names = []\n    #     if type == 'embed' or type == 'embedding':\n    #         for name, model in self.models.items():\n    #             if \"model_ability\" in model:\n    #                 if \"embed\" in model[\"model_ability\"]:\n    #                     names.append(name)\n    #             elif model[\"model_type\"] == \"embedding\":\n    #                 names.append(name)\n    #         return names\n    #     for name, model in self.models.items():\n    #         if \"model_ability\" in model:\n    #             if type in model[\"model_ability\"]:\n    #                 names.append(name)\n    #         elif model[\"model_type\"] == type:\n    #             names.append(name)\n    #     return names\n\n    def append_system_message(self, message: str):\n        \"\"\"\n        We override the system message addition because Xinference does not\n        accept multiple system messages. We concatenate them if there are\n        multiple.\n\n        Args:\n            message (str): The message to append.\n        \"\"\"\n        # if there is not already a system message in self.messages\n        if not any(isinstance(m, SystemMessage) for m in self.messages):\n            self.messages.append(\n                SystemMessage(\n                    content=message,\n                ),\n            )\n        else:\n            # if there is a system message, append to the last one\n            for i, msg in enumerate(self.messages):\n                if isinstance(msg, SystemMessage):\n                    self.messages[i].content += f\"\\n{message}\"\n                    break\n\n    def append_ca_message(self, message: str):\n        \"\"\"\n\n        We also override the system message addition for the correcting agent,\n        likewise because Xinference does not accept multiple system messages. We\n        concatenate them if there are multiple.\n\n        TODO this currently assumes that the correcting agent is the same model\n        as the primary one.\n\n        Args:\n            message (str): The message to append.\n        \"\"\"\n        # if there is not already a system message in self.messages\n        if not any(isinstance(m, SystemMessage) for m in self.ca_messages):\n            self.ca_messages.append(\n                SystemMessage(\n                    content=message,\n                ),\n            )\n        else:\n            # if there is a system message, append to the last one\n            for i, msg in enumerate(self.ca_messages):\n                if isinstance(msg, SystemMessage):\n                    self.ca_messages[i].content += f\"\\n{message}\"\n                    break\n\n    def _primary_query(self):\n        \"\"\"\n\n        Query the Xinference client API with the user's message and return the\n        response using the message history (flattery system messages, prior\n        conversation) as context. Correct the response if necessary.\n\n        LLaMA2 architecture does not accept separate system messages, so we\n        concatenate the system message with the user message to form the prompt.\n        'LLaMA enforces a strict rule that chats should alternate\n        user/assistant/user/assistant, and the system message, if present,\n        should be embedded into the first user message.' (from\n        https://discuss.huggingface.co/t/issue-with-llama-2-chat-template-and-out-of-date-documentation/61645/3)\n\n        Returns:\n\n            tuple: A tuple containing the response from the Xinference API\n            (formatted similarly to responses from the OpenAI API) and the token\n            usage.\n\n        \"\"\"\n        try:\n            history = self._create_history()\n            # TODO this is for LLaMA2 arch, may be different for newer models\n            prompt = history.pop()\n            response = self.model.chat(\n                prompt=prompt[\"content\"],\n                chat_history=history,\n                generate_config={\"max_tokens\": 2048, \"temperature\": 0},\n            )\n        except (\n            openai._exceptions.APIError,\n            openai._exceptions.OpenAIError,\n            openai._exceptions.ConflictError,\n            openai._exceptions.NotFoundError,\n            openai._exceptions.APIStatusError,\n            openai._exceptions.RateLimitError,\n            openai._exceptions.APITimeoutError,\n            openai._exceptions.BadRequestError,\n            openai._exceptions.APIConnectionError,\n            openai._exceptions.AuthenticationError,\n            openai._exceptions.InternalServerError,\n            openai._exceptions.PermissionDeniedError,\n            openai._exceptions.UnprocessableEntityError,\n            openai._exceptions.APIResponseValidationError,\n        ) as e:\n            return str(e), None\n\n        msg = response[\"choices\"][0][\"message\"][\"content\"]\n        token_usage = response[\"usage\"]\n\n        self._update_usage_stats(self.model_name, token_usage)\n\n        self.append_ai_message(msg)\n\n        return msg, token_usage\n\n    def _create_history(self):\n        history = []\n        # find location of last AI message (if any)\n        last_ai_message = None\n        for i, m in enumerate(self.messages):\n            if isinstance(m, AIMessage):\n                last_ai_message = i\n\n        # concatenate all messages before the last AI message into one message\n        if last_ai_message:\n            history.append(\n                {\n                    \"role\": \"user\",\n                    \"content\": \"\\n\".join(\n                        [m.content for m in self.messages[:last_ai_message]]\n                    ),\n                }\n            )\n            # then append the last AI message\n            history.append(\n                {\n                    \"role\": \"assistant\",\n                    \"content\": self.messages[last_ai_message].content,\n                }\n            )\n\n            # then concatenate all messages after that\n            # into one HumanMessage\n            history.append(\n                {\n                    \"role\": \"user\",\n                    \"content\": \"\\n\".join(\n                        [\n                            m.content\n                            for m in self.messages[last_ai_message + 1 :]\n                        ]\n                    ),\n                }\n            )\n\n        # if there is no AI message, concatenate all messages into one user\n        # message\n        else:\n            history.append(\n                {\n                    \"role\": \"user\",\n                    \"content\": \"\\n\".join([m.content for m in self.messages]),\n                }\n            )\n\n        return history\n\n    def _correct_response(self, msg: str):\n        \"\"\"\n\n        Correct the response from the Xinference API by sending it to a\n        secondary language model. Optionally split the response into single\n        sentences and correct each sentence individually. Update usage stats.\n\n        Args:\n            msg (str): The response from the model.\n\n        Returns:\n            str: The corrected response (or OK if no correction necessary).\n        \"\"\"\n        ca_messages = self.ca_messages.copy()\n        ca_messages.append(\n            HumanMessage(\n                content=msg,\n            ),\n        )\n        ca_messages.append(\n            SystemMessage(\n                content=\"If there is nothing to correct, please respond \"\n                \"with just 'OK', and nothing else!\",\n            ),\n        )\n        history = []\n        for m in self.messages:\n            if isinstance(m, SystemMessage):\n                history.append({\"role\": \"system\", \"content\": m.content})\n            elif isinstance(m, HumanMessage):\n                history.append({\"role\": \"user\", \"content\": m.content})\n            elif isinstance(m, AIMessage):\n                history.append({\"role\": \"assistant\", \"content\": m.content})\n        prompt = history.pop()\n        response = self.ca_model.chat(\n            prompt=prompt[\"content\"],\n            chat_history=history,\n            generate_config={\"max_tokens\": 2048, \"temperature\": 0},\n        )\n\n        correction = response[\"choices\"][0][\"message\"][\"content\"]\n        token_usage = response[\"usage\"]\n\n        self._update_usage_stats(self.ca_model_name, token_usage)\n\n        return correction\n\n    def _update_usage_stats(self, model: str, token_usage: dict):\n        \"\"\"\n        Update redis database with token usage statistics using the usage_stats\n        object with the increment method.\n\n        Args:\n            model (str): The model name.\n\n            token_usage (dict): The token usage statistics.\n        \"\"\"\n        # if self.user == \"community\":\n        # self.usage_stats.increment(\n        #     f\"usage:[date]:[user]\",\n        #     {f\"{k}:{model}\": v for k, v in token_usage.items()},\n        # )\n\n    def set_api_key(self):\n        \"\"\"\n        Try to get the Xinference model from the client API. If the model is\n        found, initialise the conversational agent. If the model is not found,\n        `get_model` will raise a RuntimeError.\n\n        Returns:\n            bool: True if the model is found, False otherwise.\n        \"\"\"\n\n        try:\n            if self.model_name is None or self.model_name == \"auto\":\n                self.model_name = self.list_models_by_type(\"chat\")[0]\n            self.model = self.client.get_model(\n                self.models[self.model_name][\"id\"]\n            )\n\n            if self.ca_model_name is None or self.ca_model_name == \"auto\":\n                self.ca_model_name = self.list_models_by_type(\"chat\")[0]\n            self.ca_model = self.client.get_model(\n                self.models[self.ca_model_name][\"id\"]\n            )\n            return True\n\n        except RuntimeError as e:\n            # TODO handle error, log?\n            return False\n\n    def list_models_by_type(self, type: str):\n        names = []\n        if type == \"embed\" or type == \"embedding\":\n            for name, model in self.models.items():\n                if \"model_ability\" in model:\n                    if \"embed\" in model[\"model_ability\"]:\n                        names.append(name)\n                elif model[\"model_type\"] == \"embedding\":\n                    names.append(name)\n            return names\n        for name, model in self.models.items():\n            if \"model_ability\" in model:\n                if type in model[\"model_ability\"]:\n                    names.append(name)\n            elif model[\"model_type\"] == type:\n                names.append(name)\n        return names\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.XinferenceConversation.__init__","title":"<code>__init__(base_url, prompts, model_name='auto', correct=True, split_correction=False)</code>","text":"<p>Connect to an open-source LLM via the Xinference client library and set up a conversation with the user.  Also initialise a second conversational agent to provide corrections to the model output, if necessary.</p> <p>Args:</p> <pre><code>base_url (str): The base URL of the Xinference instance (should not\ninclude the /v1 part).\n\nprompts (dict): A dictionary of prompts to use for the conversation.\n\nmodel_name (str): The name of the model to use. Will be mapped to\nthe according uid from the list of available models. Can be set to\n\"auto\" to use the first available model.\n\ncorrect (bool): Whether to correct the model output.\n\nsplit_correction (bool): Whether to correct the model output by\nsplitting the output into sentences and correcting each sentence\nindividually.\n</code></pre> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def __init__(\n    self,\n    base_url: str,\n    prompts: dict,\n    model_name: str = \"auto\",\n    correct: bool = True,\n    split_correction: bool = False,\n):\n    \"\"\"\n\n    Connect to an open-source LLM via the Xinference client library and set\n    up a conversation with the user.  Also initialise a second\n    conversational agent to provide corrections to the model output, if\n    necessary.\n\n    Args:\n\n        base_url (str): The base URL of the Xinference instance (should not\n        include the /v1 part).\n\n        prompts (dict): A dictionary of prompts to use for the conversation.\n\n        model_name (str): The name of the model to use. Will be mapped to\n        the according uid from the list of available models. Can be set to\n        \"auto\" to use the first available model.\n\n        correct (bool): Whether to correct the model output.\n\n        split_correction (bool): Whether to correct the model output by\n        splitting the output into sentences and correcting each sentence\n        individually.\n\n    \"\"\"\n    from xinference.client import Client\n\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        correct=correct,\n        split_correction=split_correction,\n    )\n    self.client = Client(base_url=base_url)\n\n    self.models = {}\n    self.load_models()\n\n    self.ca_model_name = model_name\n\n    self.set_api_key()\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.XinferenceConversation.append_ca_message","title":"<code>append_ca_message(message)</code>","text":"<p>We also override the system message addition for the correcting agent, likewise because Xinference does not accept multiple system messages. We concatenate them if there are multiple.</p> <p>TODO this currently assumes that the correcting agent is the same model as the primary one.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to append.</p> required Source code in <code>biochatter/llm_connect.py</code> <pre><code>def append_ca_message(self, message: str):\n    \"\"\"\n\n    We also override the system message addition for the correcting agent,\n    likewise because Xinference does not accept multiple system messages. We\n    concatenate them if there are multiple.\n\n    TODO this currently assumes that the correcting agent is the same model\n    as the primary one.\n\n    Args:\n        message (str): The message to append.\n    \"\"\"\n    # if there is not already a system message in self.messages\n    if not any(isinstance(m, SystemMessage) for m in self.ca_messages):\n        self.ca_messages.append(\n            SystemMessage(\n                content=message,\n            ),\n        )\n    else:\n        # if there is a system message, append to the last one\n        for i, msg in enumerate(self.ca_messages):\n            if isinstance(msg, SystemMessage):\n                self.ca_messages[i].content += f\"\\n{message}\"\n                break\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.XinferenceConversation.append_system_message","title":"<code>append_system_message(message)</code>","text":"<p>We override the system message addition because Xinference does not accept multiple system messages. We concatenate them if there are multiple.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to append.</p> required Source code in <code>biochatter/llm_connect.py</code> <pre><code>def append_system_message(self, message: str):\n    \"\"\"\n    We override the system message addition because Xinference does not\n    accept multiple system messages. We concatenate them if there are\n    multiple.\n\n    Args:\n        message (str): The message to append.\n    \"\"\"\n    # if there is not already a system message in self.messages\n    if not any(isinstance(m, SystemMessage) for m in self.messages):\n        self.messages.append(\n            SystemMessage(\n                content=message,\n            ),\n        )\n    else:\n        # if there is a system message, append to the last one\n        for i, msg in enumerate(self.messages):\n            if isinstance(msg, SystemMessage):\n                self.messages[i].content += f\"\\n{message}\"\n                break\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.XinferenceConversation.set_api_key","title":"<code>set_api_key()</code>","text":"<p>Try to get the Xinference model from the client API. If the model is found, initialise the conversational agent. If the model is not found, <code>get_model</code> will raise a RuntimeError.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the model is found, False otherwise.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def set_api_key(self):\n    \"\"\"\n    Try to get the Xinference model from the client API. If the model is\n    found, initialise the conversational agent. If the model is not found,\n    `get_model` will raise a RuntimeError.\n\n    Returns:\n        bool: True if the model is found, False otherwise.\n    \"\"\"\n\n    try:\n        if self.model_name is None or self.model_name == \"auto\":\n            self.model_name = self.list_models_by_type(\"chat\")[0]\n        self.model = self.client.get_model(\n            self.models[self.model_name][\"id\"]\n        )\n\n        if self.ca_model_name is None or self.ca_model_name == \"auto\":\n            self.ca_model_name = self.list_models_by_type(\"chat\")[0]\n        self.ca_model = self.client.get_model(\n            self.models[self.ca_model_name][\"id\"]\n        )\n        return True\n\n    except RuntimeError as e:\n        # TODO handle error, log?\n        return False\n</code></pre>"},{"location":"open-llm/","title":"Open-source and Local LLMs","text":"<p>Xorbits Inference is an open-source toolkit for running open-source models, particularly language models. To support BioChatter applications in local and protected contexts, we provide API access through the LangChain OpenAI Xinference module. Briefly, this module allows to connect to any open-source model supported by Xinference via the state-of-the-art and easy-to-use OpenAI API. This allows local and remote access to essentially all relevant open-source models, including these builtin models, at very little setup cost.</p>"},{"location":"open-llm/#usage","title":"Usage","text":"<p>Usage is essentially the same as when calling the official OpenAI API, but uses the <code>XinferenceConversation</code> class under the hood. Interaction with the class is possible in the exact same way as with the standard class.</p>"},{"location":"open-llm/#connecting-to-the-model-from-biochatter","title":"Connecting to the model from BioChatter","text":"<p>All that remains once Xinference has started your model is to tell BioChatter the API endpoint of your deployed model via the <code>base_url</code> parameter of the <code>XinferenceConversation</code> class. For instance:</p> <pre><code>from biochatter.llm_connect import XinferenceConversation\n\nconversation = XinferenceConversation(\n         base_url=\"http://llm.biocypher.org\",\n         prompts={},\n         correct=False,\n     )\nresponse, token_usage, correction = conversation.query(\"Hello world!\")\n</code></pre>"},{"location":"open-llm/#deploying-locally-via-docker","title":"Deploying locally via Docker","text":"<p>We have created a Docker workflow that allows the deployment of builtin Xinference models, here. It will soon be available via Dockerhub. There is another workflow that allows mounting (potentially) any compatible model from HuggingFace, here. Note that, due to graphics driver limitations, this currently only works for Linux machines with dedicated Nvidia graphics cards. If you have a different setup, please check below for deploying Xinference without the Docker workflow.</p>"},{"location":"open-llm/#deploying-locally-without-docker","title":"Deploying locally without Docker","text":""},{"location":"open-llm/#installation","title":"Installation","text":"<p>To run Xinference locally on your computer or a workstation available on your network, follow the official instructions for your type of hardware. Briefly, this includes installing the <code>xinference</code> and <code>ctransformers</code> Python libraries into an environment of your choice, as well as a hardware-specific installation of the <code>llama-ccp-python</code> library.</p>"},{"location":"open-llm/#deploying-your-model","title":"Deploying your model","text":"<p>After installation, you can run the model (locally using <code>xinference</code> or in a distributed fashion. After startup, you can visit the local server address in your browser (standard is <code>http://localhost:9997</code>) and select and start your desired model. There is a large selection of predefined models to choose from, as well as the possibility to add your own favourite models to the framework. You will see your running models in the <code>Running Models</code> tab, once they have started.</p> <p>Alternatively, you can deploy (and query) your model via the Xinference Python client:</p> <pre><code>from xinference.client import Client\n\nclient = Client(\"http://localhost:9997\")\nmodel_uid = client.launch_model(model_name=\"chatglm2\")  # download model from HuggingFace and deploy\nmodel = client.get_model(model_uid)\n\nchat_history = []\nprompt = \"What is the largest animal?\"\nmodel.chat(\n    prompt,\n    chat_history,\n    generate_config={\"max_tokens\": 1024}\n)\n</code></pre>"},{"location":"podcast-reference/","title":"Podcast module","text":"<p>Here we handle generation of podcasts from texts.</p>"},{"location":"podcast-reference/#biochatter.podcast.Podcaster","title":"<code>Podcaster</code>","text":"Source code in <code>biochatter/podcast.py</code> <pre><code>class Podcaster:\n    def __init__(\n        self,\n        document: Document,\n        model_name: str = \"gpt-3.5-turbo\",\n    ) -&gt; None:\n        \"\"\"\n        Orchestrates the podcasting of a document.\n        \"\"\"\n        self.document = document\n        self.model_name = model_name\n\n    def generate_podcast(self, characters_per_paragraph: int) -&gt; None:\n        \"\"\"\n        Podcasts the document.\n\n        TODO:\n        - chain of density prompting for variable summary length\n        \"\"\"\n        full_text = self.document[0].page_content\n\n        # split text by sentence\n        sentences = self._split_text(full_text)\n\n        # could embed sentences and cluster on cosine similarity to identify\n        # paragraphs here\n\n        # preprocess text\n        for i, sentence in enumerate(sentences):\n            # special cases i.e. and e.g. - if sentence ends with one of these,\n            # append next sentence\n            special_cases = [\"i.e.\", \"e.g.\"]\n            if sentence.endswith(tuple(special_cases)):\n                sentences[i] = sentence + \" \" + sentences[i + 1]\n                del sentences[i + 1]\n\n        # concatenate first 5 sentences for title and author extraction\n        first_5 = \"\\n\".join(sentences[:5])\n        self.podcast_info = self._title_and_authors(first_5)\n\n        # LLM to determine section breaks?\n\n        # go through sections and summarise each\n        self.processed_sections = self._process_sections(\n            sentences,\n            characters_per_paragraph,\n        )\n\n        # summarise the summaries\n\n    def _split_text(self, text: str) -&gt; List[str]:\n        \"\"\"\n        Splits consecutive text into sentences.\n        \"\"\"\n        nltk.download(\"punkt\")\n        tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n        return tokenizer.tokenize(text)\n\n    def _title_and_authors(self, text: str) -&gt; str:\n        \"\"\"\n        Extracts title and authors from document.\n\n        Args:\n            text (str): text to extract title and authors from\n\n        Returns:\n            str: title and authors\n        \"\"\"\n        # first sentence - extract title, authors\n        c_first = GptConversation(\n            model_name=self.model_name,\n            prompts={},\n            correct=False,\n        )\n        c_first.set_api_key(api_key=os.getenv(\"OPENAI_API_KEY\"), user=\"podcast\")\n        c_first.append_system_message(FIRST_PROMPT)\n        msg, token_usage, correction = c_first.query(text)\n        # split at authors ('Authors:' or '\\nAuthors:')\n        if \"Authors:\" in msg:\n            title = msg.split(\"Title:\")[1].split(\"Authors:\")[0].strip()\n            authors = msg.split(\"Authors:\")[1].strip()\n            return f\"{title}, by {authors}, podcasted by biochatter.\"\n        else:\n            return \"A podcast by biochatter.\"\n\n    def _process_section(self, text: str, summarise: bool = False) -&gt; str:\n        \"\"\"\n        Processes a section of the document. Summarises if summarise is True,\n        otherwise just makes the text more listenable.\n\n        Args:\n            text (str): text to summarise\n\n            summarise (bool): whether to summarise the text\n\n        Returns:\n            str: summarised text\n        \"\"\"\n        # summarise section\n        c = GptConversation(\n            model_name=self.model_name,\n            prompts={},\n            correct=False,\n        )\n        c.set_api_key(api_key=os.getenv(\"OPENAI_API_KEY\"), user=\"podcast\")\n        if summarise:\n            c.append_system_message(SUMMARISE_PROMPT)\n        else:\n            c.append_system_message(PROCESS_PROMPT)\n        msg, token_usage, correction = c.query(text)\n        return msg\n\n    def _process_sections(\n        self, sentences: list, characters_per_paragraph: int\n    ) -&gt; list:\n        \"\"\"\n\n        Processes sections of the document. Concatenates sentences until\n        characters_per_paragraph is reached, removing each sentence from the\n        list as it is added to the section to be processed.\n\n        Args:\n            sentences (list): list of sentences to summarise\n\n            characters_per_paragraph (int): number of characters per paragraph\n\n        Returns:\n            list: list of processed sections\n        \"\"\"\n        summarised_sections = []\n        section = \"\"\n        while sentences:\n            sentence = sentences.pop(0)\n            tmp = section + sentence\n            if len(tmp) &lt; characters_per_paragraph and sentences:\n                section += sentence\n            else:\n                if sentences:\n                    sentences.insert(0, sentence)\n                summarised_section = self._process_section(section)\n                # filter \"no content\" sections\n                if not (\n                    \"no content\" in summarised_section.lower()\n                    and len(summarised_section) &lt; 30\n                ):\n                    summarised_sections.append(summarised_section)\n                section = \"\"\n\n        return summarised_sections\n\n    def podcast_to_file(\n        self,\n        path: str,\n        model: str = \"gtts\",\n        voice: str = \"alloy\",\n    ) -&gt; None:\n        \"\"\"\n        Uses text-to-speech to generate audio for the summarised paper podcast.\n\n        Args:\n            path (str): path to save audio file to\n\n            model (str): model to use for text-to-speech. Currently supported:\n                'gtts' (Google Text-to-Speech, free),\n                'tts-1' (OpenAI API, paid, prioritises speed),\n                'tts-1-hd' (OpenAI API, paid, prioritises quality)\n\n            voice (str): voice to use for text-to-speech. See OpenAI API\n                documentation for available voices.\n        \"\"\"\n\n        full_text = self.podcast_to_text()\n\n        if model == \"gtts\":\n            audio = gTTS(text=full_text)\n            audio.save(path)\n        else:\n            client = OpenAI()\n\n            # Save the intro to the original file\n            response = client.audio.speech.create(\n                model=model,\n                voice=voice,\n                input=(\n                    \"You are listening to: \\n\\n\"\n                    + self.podcast_info\n                    + \"\\n\\n\"\n                    + \" Text-to-speech generated by OpenAI.\"\n                ),\n            )\n            first_path = path.rsplit(\".\", 1)[0] + \"_0.mp3\"\n            response.stream_to_file(first_path)\n\n            # Concatenate the sections\n            full_text = \"\"\n            for i, section in enumerate(self.processed_sections):\n                full_text += section + \"\\n\\n\"\n\n            # Make sections of 4000 characters max (at sentence boundaries)\n            nltk.download(\"punkt\")\n            tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n            sentences = deque(\n                tokenizer.tokenize(full_text)\n            )  # Use a deque instead of a list\n\n            # Split the text into sections by filling one section until it\n            # exceeds 4000 characters, then starting a new section (not adding\n            # the sentence that would exceed the limit)\n            sections = []\n            section = \"\"\n            while sentences:\n                sentence = sentences[0]\n                tmp = section + sentence\n                if len(tmp) &lt; 4000:\n                    section += sentences.popleft()\n                else:\n                    sections.append(section)\n                    section = \"\"\n\n            sections.append(section)  # Add the penultimate section\n\n            # Last section: conclude the podcast\n            sections.append(\n                f\"This was {self.podcast_info}. Thank you for listening.\"\n            )\n\n            # Save each section to a separate file with an integer suffix\n            for i, section in enumerate(sections):\n                response = client.audio.speech.create(\n                    model=model,\n                    voice=voice,\n                    input=section,\n                )\n                # Insert the integer suffix just before the .mp3 extension\n                section_path = path.rsplit(\".\", 1)[0] + f\"_{i+1}.mp3\"\n                response.stream_to_file(section_path)\n\n    def podcast_to_text(self):\n        \"\"\"\n        Returns the summarised paper podcast as text.\n        \"\"\"\n        full_text = \"You are listening to: \" + self.podcast_info + \"\\n\\n\"\n        for section in self.processed_sections:\n            full_text += section + \"\\n\\n\"\n        return full_text\n</code></pre>"},{"location":"podcast-reference/#biochatter.podcast.Podcaster.__init__","title":"<code>__init__(document, model_name='gpt-3.5-turbo')</code>","text":"<p>Orchestrates the podcasting of a document.</p> Source code in <code>biochatter/podcast.py</code> <pre><code>def __init__(\n    self,\n    document: Document,\n    model_name: str = \"gpt-3.5-turbo\",\n) -&gt; None:\n    \"\"\"\n    Orchestrates the podcasting of a document.\n    \"\"\"\n    self.document = document\n    self.model_name = model_name\n</code></pre>"},{"location":"podcast-reference/#biochatter.podcast.Podcaster.generate_podcast","title":"<code>generate_podcast(characters_per_paragraph)</code>","text":"<p>Podcasts the document.</p> <p>TODO: - chain of density prompting for variable summary length</p> Source code in <code>biochatter/podcast.py</code> <pre><code>def generate_podcast(self, characters_per_paragraph: int) -&gt; None:\n    \"\"\"\n    Podcasts the document.\n\n    TODO:\n    - chain of density prompting for variable summary length\n    \"\"\"\n    full_text = self.document[0].page_content\n\n    # split text by sentence\n    sentences = self._split_text(full_text)\n\n    # could embed sentences and cluster on cosine similarity to identify\n    # paragraphs here\n\n    # preprocess text\n    for i, sentence in enumerate(sentences):\n        # special cases i.e. and e.g. - if sentence ends with one of these,\n        # append next sentence\n        special_cases = [\"i.e.\", \"e.g.\"]\n        if sentence.endswith(tuple(special_cases)):\n            sentences[i] = sentence + \" \" + sentences[i + 1]\n            del sentences[i + 1]\n\n    # concatenate first 5 sentences for title and author extraction\n    first_5 = \"\\n\".join(sentences[:5])\n    self.podcast_info = self._title_and_authors(first_5)\n\n    # LLM to determine section breaks?\n\n    # go through sections and summarise each\n    self.processed_sections = self._process_sections(\n        sentences,\n        characters_per_paragraph,\n    )\n</code></pre>"},{"location":"podcast-reference/#biochatter.podcast.Podcaster.podcast_to_file","title":"<code>podcast_to_file(path, model='gtts', voice='alloy')</code>","text":"<p>Uses text-to-speech to generate audio for the summarised paper podcast.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path to save audio file to</p> required <code>model</code> <code>str</code> <p>model to use for text-to-speech. Currently supported: 'gtts' (Google Text-to-Speech, free), 'tts-1' (OpenAI API, paid, prioritises speed), 'tts-1-hd' (OpenAI API, paid, prioritises quality)</p> <code>'gtts'</code> <code>voice</code> <code>str</code> <p>voice to use for text-to-speech. See OpenAI API documentation for available voices.</p> <code>'alloy'</code> Source code in <code>biochatter/podcast.py</code> <pre><code>def podcast_to_file(\n    self,\n    path: str,\n    model: str = \"gtts\",\n    voice: str = \"alloy\",\n) -&gt; None:\n    \"\"\"\n    Uses text-to-speech to generate audio for the summarised paper podcast.\n\n    Args:\n        path (str): path to save audio file to\n\n        model (str): model to use for text-to-speech. Currently supported:\n            'gtts' (Google Text-to-Speech, free),\n            'tts-1' (OpenAI API, paid, prioritises speed),\n            'tts-1-hd' (OpenAI API, paid, prioritises quality)\n\n        voice (str): voice to use for text-to-speech. See OpenAI API\n            documentation for available voices.\n    \"\"\"\n\n    full_text = self.podcast_to_text()\n\n    if model == \"gtts\":\n        audio = gTTS(text=full_text)\n        audio.save(path)\n    else:\n        client = OpenAI()\n\n        # Save the intro to the original file\n        response = client.audio.speech.create(\n            model=model,\n            voice=voice,\n            input=(\n                \"You are listening to: \\n\\n\"\n                + self.podcast_info\n                + \"\\n\\n\"\n                + \" Text-to-speech generated by OpenAI.\"\n            ),\n        )\n        first_path = path.rsplit(\".\", 1)[0] + \"_0.mp3\"\n        response.stream_to_file(first_path)\n\n        # Concatenate the sections\n        full_text = \"\"\n        for i, section in enumerate(self.processed_sections):\n            full_text += section + \"\\n\\n\"\n\n        # Make sections of 4000 characters max (at sentence boundaries)\n        nltk.download(\"punkt\")\n        tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n        sentences = deque(\n            tokenizer.tokenize(full_text)\n        )  # Use a deque instead of a list\n\n        # Split the text into sections by filling one section until it\n        # exceeds 4000 characters, then starting a new section (not adding\n        # the sentence that would exceed the limit)\n        sections = []\n        section = \"\"\n        while sentences:\n            sentence = sentences[0]\n            tmp = section + sentence\n            if len(tmp) &lt; 4000:\n                section += sentences.popleft()\n            else:\n                sections.append(section)\n                section = \"\"\n\n        sections.append(section)  # Add the penultimate section\n\n        # Last section: conclude the podcast\n        sections.append(\n            f\"This was {self.podcast_info}. Thank you for listening.\"\n        )\n\n        # Save each section to a separate file with an integer suffix\n        for i, section in enumerate(sections):\n            response = client.audio.speech.create(\n                model=model,\n                voice=voice,\n                input=section,\n            )\n            # Insert the integer suffix just before the .mp3 extension\n            section_path = path.rsplit(\".\", 1)[0] + f\"_{i+1}.mp3\"\n            response.stream_to_file(section_path)\n</code></pre>"},{"location":"podcast-reference/#biochatter.podcast.Podcaster.podcast_to_text","title":"<code>podcast_to_text()</code>","text":"<p>Returns the summarised paper podcast as text.</p> Source code in <code>biochatter/podcast.py</code> <pre><code>def podcast_to_text(self):\n    \"\"\"\n    Returns the summarised paper podcast as text.\n    \"\"\"\n    full_text = \"You are listening to: \" + self.podcast_info + \"\\n\\n\"\n    for section in self.processed_sections:\n        full_text += section + \"\\n\\n\"\n    return full_text\n</code></pre>"},{"location":"podcast/","title":"Podcast my Paper","text":"<p>We provide a module to perform document processing and text-to-speech to enable listening to any document in podcast style.  The functionality can be accessed through the podcast API or by running the script <code>scripts/podcast_single_document.py</code>.</p>"},{"location":"podcast/#api-access","title":"API access","text":"<p>The podcast API is available through the <code>podcast</code> module. An end-to-end workflow looks like this (modified from the test module):</p> <pre><code>from biochatter.podcast import Podcaster\nfrom biochatter.vectorstore import DocumentReader\n\n# Load document\nreader = DocumentReader()\ndocument = reader.load_document(\"test/dcn.pdf\")\n\n# Initialise podcaster\npodcaster = Podcaster(document)\n\n# Generate podcast (LLM task)\npodcaster.generate_podcast(characters_per_paragraph=5000)\n\n# Employ text-to-speech to generate audio file (optional)\npodcaster.podcast_to_file(\"test/test.mp3\", model=\"tts-1-hd\", voice=\"alloy\")\n</code></pre> <p>If you do not want audio output, you can simply access the generated text though the function <code>podcaster.podcast_to_text()</code>. </p> <p>This example uses the paid OpenAI text-to-speech API to generate the audio file. The default of the <code>podcast_to_file</code> function is to use the free Google text-to-speech API.  When using OpenAI, due to the input length limit of 4096 characters, the podcast is split into multiple parts indicated by integer suffixes.</p>"},{"location":"podcast/#command-line-access","title":"Command line access","text":"<p>To generate a podcast from a single document more quickly, you can use the <code>scripts/podcast_single_document.py</code> script.  It accepts two arguments, the path to the document and the path to the desired output file.  If the output file ends in <code>.mp3</code>, the OpenAI text-to-speech API will be used to generate an audio file.  Otherwise, the script will generate a text file and skip the text-to-speech step.  If using the OpenAI text-to-speech API, multiple files will be generated with integer suffixes.  If you installed BioChatter with poetry, you can run the script like this (from the root directory of the repository):</p> <pre><code>poetry run python scripts/podcast_single_document.py test/dcn.pdf test/test.mp3\n</code></pre>"},{"location":"prompts-reference/","title":"Prompts module","text":"<p>Here we handle generation of use case-specific prompts.</p>"},{"location":"prompts-reference/#biochatter.prompts.BioCypherPromptEngine","title":"<code>BioCypherPromptEngine</code>","text":"Source code in <code>biochatter/prompts.py</code> <pre><code>class BioCypherPromptEngine:\n    def __init__(\n        self,\n        schema_config_or_info_path: Optional[str] = None,\n        schema_config_or_info_dict: Optional[dict] = None,\n        model_name: str = \"gpt-3.5-turbo\",\n        conversation_factory: Optional[callable] = None,\n    ) -&gt; None:\n        \"\"\"\n\n        Given a biocypher schema configuration, extract the entities and\n        relationships, and for each extract their mode of representation (node\n        or edge), properties, and identifier namespace. Using these data, allow\n        the generation of prompts for a large language model, informing it of\n        the schema constituents and their properties, to enable the\n        parameterisation of function calls to a knowledge graph.\n\n        Args:\n            schema_config_or_info_path: Path to a biocypher schema configuration\n                file or the extended schema information output generated by\n                BioCypher's `write_schema_info` function (preferred).\n\n            schema_config_or_info_dict: A dictionary containing the schema\n                configuration file or the extended schema information output\n                generated by BioCypher's `write_schema_info` function\n                (preferred).\n\n            model_name: The name of the model to use for the conversation.\n                DEPRECATED: This should now be set in the conversation factory.\n\n            conversation_factory: A function used to create a conversation for\n                creating the KG query. If not provided, a default function is\n                used (creating an OpenAI conversation with the specified model,\n                see `_get_conversation`).\n        \"\"\"\n\n        if not schema_config_or_info_path and not schema_config_or_info_dict:\n            raise ValueError(\n                \"Please provide the schema configuration or schema info as a \"\n                \"path to a file or as a dictionary.\"\n            )\n\n        if schema_config_or_info_path and schema_config_or_info_dict:\n            raise ValueError(\n                \"Please provide the schema configuration or schema info as a \"\n                \"path to a file or as a dictionary, not both.\"\n            )\n\n        # set conversation factory or use default\n        self.conversation_factory = (\n            conversation_factory\n            if conversation_factory is not None\n            else self._get_conversation\n        )\n\n        if schema_config_or_info_path:\n            # read the schema configuration\n            with open(schema_config_or_info_path, \"r\") as f:\n                schema_config = yaml.safe_load(f)\n        elif schema_config_or_info_dict:\n            schema_config = schema_config_or_info_dict\n\n        # check whether it is the original schema config or the output of\n        # biocypher info\n        is_schema_info = schema_config.get(\"is_schema_info\", False)\n\n        # extract the entities and relationships: each top level key that has\n        # a 'represented_as' key\n        self.entities = {}\n        self.relationships = {}\n        if not is_schema_info:\n            for key, value in schema_config.items():\n                # hacky, better with biocypher output\n                name_indicates_relationship = (\n                    \"interaction\" in key.lower() or \"association\" in key.lower()\n                )\n                if \"represented_as\" in value:\n                    if (\n                        value[\"represented_as\"] == \"node\"\n                        and not name_indicates_relationship\n                    ):\n                        self.entities[sentencecase_to_pascalcase(key)] = value\n                    elif (\n                        value[\"represented_as\"] == \"node\"\n                        and name_indicates_relationship\n                    ):\n                        self.relationships[sentencecase_to_pascalcase(key)] = (\n                            value\n                        )\n                    elif value[\"represented_as\"] == \"edge\":\n                        self.relationships[sentencecase_to_pascalcase(key)] = (\n                            value\n                        )\n        else:\n            for key, value in schema_config.items():\n                if not isinstance(value, dict):\n                    continue\n                if value.get(\"present_in_knowledge_graph\", None) == False:\n                    continue\n                if value.get(\"is_relationship\", None) == False:\n                    self.entities[sentencecase_to_pascalcase(key)] = value\n                elif value.get(\"is_relationship\", None) == True:\n                    value = self._capitalise_source_and_target(value)\n                    self.relationships[sentencecase_to_pascalcase(key)] = value\n\n        self.question = \"\"\n        self.selected_entities = []\n        self.selected_relationships = []  # used in property selection\n        self.selected_relationship_labels = {}  # copy to deal with labels that\n        # are not the same as the relationship name, used in query generation\n        # dictionary to also include source and target types\n        self.rel_directions = {}\n        self.model_name = model_name\n\n    def _capitalise_source_and_target(self, relationship: dict) -&gt; dict:\n        \"\"\"\n        Make sources and targets PascalCase to match the entities. Sources and\n        targets can be strings or lists of strings.\n        \"\"\"\n        if \"source\" in relationship:\n            if isinstance(relationship[\"source\"], str):\n                relationship[\"source\"] = sentencecase_to_pascalcase(\n                    relationship[\"source\"]\n                )\n            elif isinstance(relationship[\"source\"], list):\n                relationship[\"source\"] = [\n                    sentencecase_to_pascalcase(s)\n                    for s in relationship[\"source\"]\n                ]\n        if \"target\" in relationship:\n            if isinstance(relationship[\"target\"], str):\n                relationship[\"target\"] = sentencecase_to_pascalcase(\n                    relationship[\"target\"]\n                )\n            elif isinstance(relationship[\"target\"], list):\n                relationship[\"target\"] = [\n                    sentencecase_to_pascalcase(t)\n                    for t in relationship[\"target\"]\n                ]\n        return relationship\n\n    def generate_query(\n        self, question: str, query_language: Optional[str] = \"Cypher\"\n    ) -&gt; str:\n        \"\"\"\n        Wrap entity and property selection and query generation; return the\n        generated query.\n\n        Args:\n            question: A user's question.\n\n            query_language: The language of the query to generate.\n\n        Returns:\n            A database query that could answer the user's question.\n        \"\"\"\n\n        success1 = self._select_entities(\n            question=question, conversation=self.conversation_factory()\n        )\n        if not success1:\n            raise ValueError(\n                \"Entity selection failed. Please try again with a different \"\n                \"question.\"\n            )\n        success2 = self._select_relationships(\n            conversation=self.conversation_factory()\n        )\n        if not success2:\n            raise ValueError(\n                \"Relationship selection failed. Please try again with a \"\n                \"different question.\"\n            )\n        success3 = self._select_properties(\n            conversation=self.conversation_factory()\n        )\n        if not success3:\n            raise ValueError(\n                \"Property selection failed. Please try again with a different \"\n                \"question.\"\n            )\n\n        return self._generate_query(\n            question=question,\n            entities=self.selected_entities,\n            relationships=self.selected_relationship_labels,\n            properties=self.selected_properties,\n            query_language=query_language,\n            conversation=self.conversation_factory(),\n        )\n\n    def _get_conversation(\n        self, model_name: Optional[str] = None\n    ) -&gt; \"Conversation\":\n        \"\"\"\n        Create a conversation object given a model name.\n\n        Args:\n            model_name: The name of the model to use for the conversation.\n\n        Returns:\n            A BioChatter Conversation object for connecting to the LLM.\n\n        Todo:\n            Genericise to models outside of OpenAI.\n        \"\"\"\n\n        conversation = GptConversation(\n            model_name=model_name or self.model_name,\n            prompts={},\n            correct=False,\n        )\n        conversation.set_api_key(\n            api_key=os.getenv(\"OPENAI_API_KEY\"), user=\"test_user\"\n        )\n        return conversation\n\n    def _select_entities(\n        self, question: str, conversation: \"Conversation\"\n    ) -&gt; bool:\n        \"\"\"\n\n        Given a question, select the entities that are relevant to the question\n        and store them in `selected_entities` and `selected_relationships`. Use\n        LLM conversation to do this.\n\n        Args:\n            question: A user's question.\n\n            conversation: A BioChatter Conversation object for connecting to the\n                LLM.\n\n        Returns:\n            True if at least one entity was selected, False otherwise.\n\n        \"\"\"\n\n        self.question = question\n\n        conversation.append_system_message(\n            (\n                \"You have access to a knowledge graph that contains \"\n                f\"these entity types: {', '.join(self.entities)}. Your task is \"\n                \"to select the entity types that are relevant to the user's question \"\n                \"for subsequent use in a query. Only return the entity types, \"\n                \"comma-separated, without any additional text. Do not return \"\n                \"entity names, relationships, or properties.\"\n            )\n        )\n\n        msg, token_usage, correction = conversation.query(question)\n\n        result = msg.split(\",\") if msg else []\n        # TODO: do we go back and retry if no entities were selected? or ask for\n        # a reason? offer visual selection of entities and relationships by the\n        # user?\n\n        if result:\n            for entity in result:\n                entity = entity.strip()\n                if entity in self.entities:\n                    self.selected_entities.append(entity)\n\n        return bool(result)\n\n    def _select_relationships(self, conversation: \"Conversation\") -&gt; bool:\n        \"\"\"\n        Given a question and the preselected entities, select relationships for\n        the query.\n\n        Args:\n            conversation: A BioChatter Conversation object for connecting to the\n                LLM.\n\n        Returns:\n            True if at least one relationship was selected, False otherwise.\n\n        Todo:\n            Now we have the problem that we discard all relationships that do\n            not have a source and target, if at least one relationship has a\n            source and target. At least communicate this all-or-nothing\n            behaviour to the user.\n        \"\"\"\n\n        if not self.question:\n            raise ValueError(\n                \"No question found. Please make sure to run entity selection \"\n                \"first.\"\n            )\n\n        if not self.selected_entities:\n            raise ValueError(\n                \"No entities found. Please run the entity selection step first.\"\n            )\n\n        rels = {}\n        source_and_target_present = False\n        for key, value in self.relationships.items():\n            if \"source\" in value and \"target\" in value:\n                # if source or target is a list, expand to single pairs\n                source = ensure_iterable(value[\"source\"])\n                target = ensure_iterable(value[\"target\"])\n                pairs = []\n                for s in source:\n                    for t in target:\n                        pairs.append((s, t))\n                rels[key] = pairs\n                source_and_target_present = True\n            else:\n                rels[key] = {}\n\n        # prioritise relationships that have source and target, and discard\n        # relationships that do not have both source and target, if at least one\n        # relationship has both source and target. keep relationships that have\n        # either source or target, if none of the relationships have both source\n        # and target.\n\n        if source_and_target_present:\n            # First, separate the relationships into two groups: those with both\n            # source and target in the selected entities, and those with either\n            # source or target but not both.\n\n            rels_with_both = {}\n            rels_with_either = {}\n            for key, value in rels.items():\n                for pair in value:\n                    if pair[0] in self.selected_entities:\n                        if pair[1] in self.selected_entities:\n                            rels_with_both[key] = value\n                        else:\n                            rels_with_either[key] = value\n                    elif pair[1] in self.selected_entities:\n                        rels_with_either[key] = value\n\n            # If there are any relationships with both source and target,\n            # discard the others.\n\n            if rels_with_both:\n                rels = rels_with_both\n            else:\n                rels = rels_with_either\n\n            selected_rels = []\n            for key, value in rels.items():\n                if not value:\n                    continue\n\n                for pair in value:\n                    if (\n                        pair[0] in self.selected_entities\n                        or pair[1] in self.selected_entities\n                    ):\n                        selected_rels.append((key, pair))\n\n            rels = json.dumps(selected_rels)\n        else:\n            rels = json.dumps(self.relationships)\n\n        msg = (\n            \"You have access to a knowledge graph that contains \"\n            f\"these entities: {', '.join(self.selected_entities)}. \"\n            \"Your task is to select the relationships that are relevant \"\n            \"to the user's question for subsequent use in a query. Only \"\n            \"return the relationships without their sources or targets, \"\n            \"comma-separated, and without any additional text. Here are the \"\n            \"possible relationships and their source and target entities: \"\n            f\"{rels}.\"\n        )\n\n        conversation.append_system_message(msg)\n\n        res, token_usage, correction = conversation.query(self.question)\n\n        result = res.split(\",\") if msg else []\n\n        if result:\n            for relationship in result:\n                relationship = relationship.strip()\n                if relationship in self.relationships:\n                    self.selected_relationships.append(relationship)\n                    rel_dict = self.relationships[relationship]\n                    label = rel_dict.get(\"label_as_edge\", relationship)\n                    if \"source\" in rel_dict and \"target\" in rel_dict:\n                        self.selected_relationship_labels[label] = {\n                            \"source\": rel_dict[\"source\"],\n                            \"target\": rel_dict[\"target\"],\n                        }\n                    else:\n                        self.selected_relationship_labels[label] = {\n                            \"source\": None,\n                            \"target\": None,\n                        }\n\n        # if we selected relationships that have either source or target which\n        # is not in the selected entities, we add those entities to the selected\n        # entities.\n\n        if self.selected_relationship_labels:\n            for key, value in self.selected_relationship_labels.items():\n                sources = ensure_iterable(value[\"source\"])\n                targets = ensure_iterable(value[\"target\"])\n                for source in sources:\n                    if source is None:\n                        continue\n                    if source not in self.selected_entities:\n                        self.selected_entities.append(source)\n                for target in targets:\n                    if target is None:\n                        continue\n                    if target not in self.selected_entities:\n                        self.selected_entities.append(target)\n\n        return bool(result)\n\n    def _select_properties(self, conversation: \"Conversation\") -&gt; bool:\n        \"\"\"\n\n        Given a question (optionally provided, but in the standard use case\n        reused from the entity selection step) and the selected entities, select\n        the properties that are relevant to the question and store them in\n        the dictionary `selected_properties`.\n\n        Returns:\n            True if at least one property was selected, False otherwise.\n\n        \"\"\"\n\n        if not self.question:\n            raise ValueError(\n                \"No question found. Please make sure to run entity and \"\n                \"relationship selection first.\"\n            )\n\n        if not self.selected_entities and not self.selected_relationships:\n            raise ValueError(\n                \"No entities or relationships provided, and none available \"\n                \"from entity selection step. Please provide \"\n                \"entities/relationships or run the entity selection \"\n                \"(`select_entities()`) step first.\"\n            )\n\n        e_props = {}\n        for entity in self.selected_entities:\n            if self.entities[entity].get(\"properties\"):\n                e_props[entity] = list(\n                    self.entities[entity][\"properties\"].keys()\n                )\n\n        r_props = {}\n        for relationship in self.selected_relationships:\n            if self.relationships[relationship].get(\"properties\"):\n                r_props[relationship] = list(\n                    self.relationships[relationship][\"properties\"].keys()\n                )\n\n        msg = (\n            \"You have access to a knowledge graph that contains entities and \"\n            \"relationships. They have the following properties. Entities:\"\n            f\"{e_props}, Relationships: {r_props}. \"\n            \"Your task is to select the properties that are relevant to the \"\n            \"user's question for subsequent use in a query. Only return the \"\n            \"entities and relationships with their relevant properties in JSON \"\n            \"format, without any additional text. Return the \"\n            \"entities/relationships as top-level dictionary keys, and their \"\n            \"properties as dictionary values. \"\n            \"Do not return properties that are not relevant to the question.\"\n        )\n\n        conversation.append_system_message(msg)\n\n        msg, token_usage, correction = conversation.query(self.question)\n\n        try:\n            self.selected_properties = json.loads(msg) if msg else {}\n        except json.decoder.JSONDecodeError:\n            self.selected_properties = {}\n\n        return bool(self.selected_properties)\n\n    def _generate_query(\n        self,\n        question: str,\n        entities: list,\n        relationships: dict,\n        properties: dict,\n        query_language: str,\n        conversation: \"Conversation\",\n    ) -&gt; str:\n        \"\"\"\n        Generate a query in the specified query language that answers the user's\n        question.\n\n        Args:\n            question: A user's question.\n\n            entities: A list of entities that are relevant to the question.\n\n            relationships: A list of relationships that are relevant to the\n                question.\n\n            properties: A dictionary of properties that are relevant to the\n                question.\n\n            query_language: The language of the query to generate.\n\n            conversation: A BioChatter Conversation object for connecting to the\n                LLM.\n\n        Returns:\n            A database query that could answer the user's question.\n        \"\"\"\n        msg = (\n            f\"Generate a database query in {query_language} that answers \"\n            f\"the user's question. \"\n            f\"You can use the following entities: {entities}, \"\n            f\"relationships: {list(relationships.keys())}, and \"\n            f\"properties: {properties}. \"\n        )\n\n        for relationship, values in relationships.items():\n            self._expand_pairs(relationship, values)\n\n        if self.rel_directions:\n            msg += \"Given the following valid combinations of source, relationship, and target: \"\n            for key, value in self.rel_directions.items():\n                for pair in value:\n                    msg += f\"'(:{pair[0]})-(:{key})-&gt;(:{pair[1]})', \"\n            msg += f\"generate a {query_language} query using one of these combinations. \"\n\n        msg += \"Only return the query, without any additional text.\"\n\n        conversation.append_system_message(msg)\n\n        out_msg, token_usage, correction = conversation.query(question)\n\n        return out_msg.strip()\n\n    def _expand_pairs(self, relationship, values) -&gt; None:\n        if not self.rel_directions.get(relationship):\n            self.rel_directions[relationship] = []\n        if isinstance(values[\"source\"], list):\n            for source in values[\"source\"]:\n                if isinstance(values[\"target\"], list):\n                    for target in values[\"target\"]:\n                        self.rel_directions[relationship].append(\n                            (source, target)\n                        )\n                else:\n                    self.rel_directions[relationship].append(\n                        (source, values[\"target\"])\n                    )\n        elif isinstance(values[\"target\"], list):\n            for target in values[\"target\"]:\n                self.rel_directions[relationship].append(\n                    (values[\"source\"], target)\n                )\n        else:\n            self.rel_directions[relationship].append(\n                (values[\"source\"], values[\"target\"])\n            )\n</code></pre>"},{"location":"prompts-reference/#biochatter.prompts.BioCypherPromptEngine.__init__","title":"<code>__init__(schema_config_or_info_path=None, schema_config_or_info_dict=None, model_name='gpt-3.5-turbo', conversation_factory=None)</code>","text":"<p>Given a biocypher schema configuration, extract the entities and relationships, and for each extract their mode of representation (node or edge), properties, and identifier namespace. Using these data, allow the generation of prompts for a large language model, informing it of the schema constituents and their properties, to enable the parameterisation of function calls to a knowledge graph.</p> <p>Parameters:</p> Name Type Description Default <code>schema_config_or_info_path</code> <code>Optional[str]</code> <p>Path to a biocypher schema configuration file or the extended schema information output generated by BioCypher's <code>write_schema_info</code> function (preferred).</p> <code>None</code> <code>schema_config_or_info_dict</code> <code>Optional[dict]</code> <p>A dictionary containing the schema configuration file or the extended schema information output generated by BioCypher's <code>write_schema_info</code> function (preferred).</p> <code>None</code> <code>model_name</code> <code>str</code> <p>The name of the model to use for the conversation. DEPRECATED: This should now be set in the conversation factory.</p> <code>'gpt-3.5-turbo'</code> <code>conversation_factory</code> <code>Optional[callable]</code> <p>A function used to create a conversation for creating the KG query. If not provided, a default function is used (creating an OpenAI conversation with the specified model, see <code>_get_conversation</code>).</p> <code>None</code> Source code in <code>biochatter/prompts.py</code> <pre><code>def __init__(\n    self,\n    schema_config_or_info_path: Optional[str] = None,\n    schema_config_or_info_dict: Optional[dict] = None,\n    model_name: str = \"gpt-3.5-turbo\",\n    conversation_factory: Optional[callable] = None,\n) -&gt; None:\n    \"\"\"\n\n    Given a biocypher schema configuration, extract the entities and\n    relationships, and for each extract their mode of representation (node\n    or edge), properties, and identifier namespace. Using these data, allow\n    the generation of prompts for a large language model, informing it of\n    the schema constituents and their properties, to enable the\n    parameterisation of function calls to a knowledge graph.\n\n    Args:\n        schema_config_or_info_path: Path to a biocypher schema configuration\n            file or the extended schema information output generated by\n            BioCypher's `write_schema_info` function (preferred).\n\n        schema_config_or_info_dict: A dictionary containing the schema\n            configuration file or the extended schema information output\n            generated by BioCypher's `write_schema_info` function\n            (preferred).\n\n        model_name: The name of the model to use for the conversation.\n            DEPRECATED: This should now be set in the conversation factory.\n\n        conversation_factory: A function used to create a conversation for\n            creating the KG query. If not provided, a default function is\n            used (creating an OpenAI conversation with the specified model,\n            see `_get_conversation`).\n    \"\"\"\n\n    if not schema_config_or_info_path and not schema_config_or_info_dict:\n        raise ValueError(\n            \"Please provide the schema configuration or schema info as a \"\n            \"path to a file or as a dictionary.\"\n        )\n\n    if schema_config_or_info_path and schema_config_or_info_dict:\n        raise ValueError(\n            \"Please provide the schema configuration or schema info as a \"\n            \"path to a file or as a dictionary, not both.\"\n        )\n\n    # set conversation factory or use default\n    self.conversation_factory = (\n        conversation_factory\n        if conversation_factory is not None\n        else self._get_conversation\n    )\n\n    if schema_config_or_info_path:\n        # read the schema configuration\n        with open(schema_config_or_info_path, \"r\") as f:\n            schema_config = yaml.safe_load(f)\n    elif schema_config_or_info_dict:\n        schema_config = schema_config_or_info_dict\n\n    # check whether it is the original schema config or the output of\n    # biocypher info\n    is_schema_info = schema_config.get(\"is_schema_info\", False)\n\n    # extract the entities and relationships: each top level key that has\n    # a 'represented_as' key\n    self.entities = {}\n    self.relationships = {}\n    if not is_schema_info:\n        for key, value in schema_config.items():\n            # hacky, better with biocypher output\n            name_indicates_relationship = (\n                \"interaction\" in key.lower() or \"association\" in key.lower()\n            )\n            if \"represented_as\" in value:\n                if (\n                    value[\"represented_as\"] == \"node\"\n                    and not name_indicates_relationship\n                ):\n                    self.entities[sentencecase_to_pascalcase(key)] = value\n                elif (\n                    value[\"represented_as\"] == \"node\"\n                    and name_indicates_relationship\n                ):\n                    self.relationships[sentencecase_to_pascalcase(key)] = (\n                        value\n                    )\n                elif value[\"represented_as\"] == \"edge\":\n                    self.relationships[sentencecase_to_pascalcase(key)] = (\n                        value\n                    )\n    else:\n        for key, value in schema_config.items():\n            if not isinstance(value, dict):\n                continue\n            if value.get(\"present_in_knowledge_graph\", None) == False:\n                continue\n            if value.get(\"is_relationship\", None) == False:\n                self.entities[sentencecase_to_pascalcase(key)] = value\n            elif value.get(\"is_relationship\", None) == True:\n                value = self._capitalise_source_and_target(value)\n                self.relationships[sentencecase_to_pascalcase(key)] = value\n\n    self.question = \"\"\n    self.selected_entities = []\n    self.selected_relationships = []  # used in property selection\n    self.selected_relationship_labels = {}  # copy to deal with labels that\n    # are not the same as the relationship name, used in query generation\n    # dictionary to also include source and target types\n    self.rel_directions = {}\n    self.model_name = model_name\n</code></pre>"},{"location":"prompts-reference/#biochatter.prompts.BioCypherPromptEngine.generate_query","title":"<code>generate_query(question, query_language='Cypher')</code>","text":"<p>Wrap entity and property selection and query generation; return the generated query.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>A user's question.</p> required <code>query_language</code> <code>Optional[str]</code> <p>The language of the query to generate.</p> <code>'Cypher'</code> <p>Returns:</p> Type Description <code>str</code> <p>A database query that could answer the user's question.</p> Source code in <code>biochatter/prompts.py</code> <pre><code>def generate_query(\n    self, question: str, query_language: Optional[str] = \"Cypher\"\n) -&gt; str:\n    \"\"\"\n    Wrap entity and property selection and query generation; return the\n    generated query.\n\n    Args:\n        question: A user's question.\n\n        query_language: The language of the query to generate.\n\n    Returns:\n        A database query that could answer the user's question.\n    \"\"\"\n\n    success1 = self._select_entities(\n        question=question, conversation=self.conversation_factory()\n    )\n    if not success1:\n        raise ValueError(\n            \"Entity selection failed. Please try again with a different \"\n            \"question.\"\n        )\n    success2 = self._select_relationships(\n        conversation=self.conversation_factory()\n    )\n    if not success2:\n        raise ValueError(\n            \"Relationship selection failed. Please try again with a \"\n            \"different question.\"\n        )\n    success3 = self._select_properties(\n        conversation=self.conversation_factory()\n    )\n    if not success3:\n        raise ValueError(\n            \"Property selection failed. Please try again with a different \"\n            \"question.\"\n        )\n\n    return self._generate_query(\n        question=question,\n        entities=self.selected_entities,\n        relationships=self.selected_relationship_labels,\n        properties=self.selected_properties,\n        query_language=query_language,\n        conversation=self.conversation_factory(),\n    )\n</code></pre>"},{"location":"rag/","title":"Retrieval-Augmented Generation","text":"<p>To connect to a vector database for using semantic similarity search and retrieval-augmented generation (RAG), we provide an implementation that connects to a Milvus instance (local or remote).  These functions are provided by the modules <code>vectorstore.py</code> (for performing embeddings) and <code>vectorstore_agent.py</code> (for maintaining the connection and search).</p> <p>This is implemented in the ChatGSE Docker workflow and the BioChatter Docker compose found in this repository.  To start Milvus on its own in these repositories, you can call <code>docker compose up -d standalone</code> (<code>standalone</code> being the Milvus endpoint, which starts two other services alongside it).</p>"},{"location":"rag/#connecting","title":"Connecting","text":"<p>To connect to a vector DB host, we can use the corresponding class:</p> <pre><code>from biochatter.vectorstore_agent import VectorDatabaseAgentMilvus\n\ndbHost = VectorDatabaseAgentMilvus(\n    embedding_func=OpenAIEmbeddings(),\n    connection_args={\"host\": _HOST, \"port\": _PORT},\n    embedding_collection_name=EMBEDDING_NAME,\n    metadata_collection_name=METADATA_NAME\n)\n</code></pre> <p>This establishes a connection with the vector database (using a host IP and port) and uses two collections, one for the embeddings and one for the metadata of embedded text (e.g. the title and authors of the paper that was embedded).</p>"},{"location":"rag/#embedding-documents","title":"Embedding documents","text":"<p>To embed text from documents, we use the LangChain and BioChatter functionalities for processing and passing the text to the vector database.</p> <pre><code>from biochatter.vectorstore import DocumentReader()\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# read and split document at `pdf_path`\nreader = DocumentReader()\ndocs = reader.load_document(pdf_path)\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=chunk_size,\n    chunk_overlap=chunk_overlap,\n    separators=[\" \", \",\", \"\\n\"],\n)\nsplit_text = text_splitter.split_documents(docs)\n\n# embed and store embeddings in the connected vector DB\ndoc_id = dbHost.store_embeddings(splitted_docs)\n</code></pre> <p>The dbHost class takes care of calling an embedding model, storing the embedding in the database, and returning a document ID that can be used to refer to the stored document.</p>"},{"location":"rag/#semantic-search","title":"Semantic search","text":"<p>To perform a semantic similarity search, all that is left to do is pass a question or statement to the <code>dbHost</code>, which will be embedded and compared to the present embeddings, returning a number <code>k</code> most similar text fragments.</p> <pre><code>results = dbHost.similarity_search(\n    query=\"Semantic similarity search query\",\n    k=3,\n)\n</code></pre>"},{"location":"rag/#vectorstore-management","title":"Vectorstore management","text":"<p>Using the collections we created at setup, we can delete entries in the vector database using their IDs. We can also return a list of all collected docs to determine which we want to delete.</p> <pre><code>docs = dbHost.get_all_documents()\nres = dbHost.remove_document(docs[0][\"id\"])\n</code></pre>"},{"location":"vectorstore-reference/","title":"Vectorstore module","text":"<p>Here we handle the application of vectorstore services to retrieval-augmented generation tasks by embedding documents.</p>"},{"location":"vectorstore-reference/#biochatter.vectorstore.DocumentEmbedder","title":"<code>DocumentEmbedder</code>","text":"Source code in <code>biochatter/vectorstore.py</code> <pre><code>class DocumentEmbedder:\n    def __init__(\n        self,\n        used: bool = False,\n        online: bool = False,\n        chunk_size: int = 1000,\n        chunk_overlap: int = 0,\n        split_by_characters: bool = True,\n        separators: Optional[list] = None,\n        n_results: int = 3,\n        model: Optional[str] = \"text-embedding-ada-002\",\n        vector_db_vendor: Optional[str] = None,\n        connection_args: Optional[dict] = None,\n        embedding_collection_name: Optional[str] = None,\n        metadata_collection_name: Optional[str] = None,\n        api_key: Optional[str] = None,\n        is_azure: Optional[bool] = False,\n        azure_deployment: Optional[str] = None,\n        azure_endpoint: Optional[str] = None,\n        base_url: Optional[str] = None,\n        embeddings: Optional[OpenAIEmbeddings | XinferenceEmbeddings] = None,\n        documentids_workspace: Optional[List[str]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Class that handles the retrieval-augmented generation (RAG) functionality\n        of BioChatter. It splits text into chunks, embeds them, and stores them in\n        a vector database. It can then be used to do similarity search on the\n        database.\n\n        Args:\n\n            used (bool, optional): whether RAG has been used (ChatGSE setting).\n                Defaults to False.\n\n            online (bool, optional): whether we are running ChatGSE online.\n                Defaults to False.\n\n            chunk_size (int, optional): size of chunks to split text into.\n                Defaults to 1000.\n\n            chunk_overlap (int, optional): overlap between chunks. Defaults to 0.\n\n            split_by_characters (bool, optional): whether to split by characters\n                or tokens. Defaults to True.\n\n            separators (Optional[list], optional): list of separators to use when\n                splitting by characters. Defaults to [\" \", \",\", \"\\n\"].\n\n            n_results (int, optional): number of results to return from\n                similarity search. Defaults to 3.\n\n            model (Optional[str], optional): name of model to use for embeddings.\n                Defaults to 'text-embedding-ada-002'.\n\n            vector_db_vendor (Optional[str], optional): name of vector database\n                to use. Defaults to Milvus.\n\n            connection_args (Optional[dict], optional): arguments to pass to\n                vector database connection. Defaults to None.\n\n            api_key (Optional[str], optional): OpenAI API key. Defaults to None.\n\n            base_url (Optional[str], optional): base url of OpenAI API.\n\n            embeddings (Optional[OpenAIEmbeddings | XinferenceEmbeddings],\n                optional): Embeddings object to use. Defaults to OpenAI.\n\n            documentids_workspace (Optional[List[str]], optional): a list of document IDs\n                that defines the scope within which rag operations (remove, similarity search,\n                and get all) occur. Defaults to None, which means the operations will be\n                performed across all documents in the database.\n\n            is_azure (Optional[bool], optional): if we are using Azure\n            azure_deployment (Optional[str], optional): Azure embeddings model deployment,\n                should work with azure_endpoint when is_azure is True\n            azure_endpoint (Optional[str], optional): Azure endpoint, should work with\n                azure_deployment when is_azure is True\n\n        \"\"\"\n        self.used = used\n        self.online = online\n        self.chunk_size = chunk_size\n        self.chunk_overlap = chunk_overlap\n        self.separators = separators or [\" \", \",\", \"\\n\"]\n        self.n_results = n_results\n        self.split_by_characters = split_by_characters\n        self.model_name = model\n\n        # TODO API Key handling to central config?\n        if base_url:\n            openai.api_base = base_url\n\n        if embeddings:\n            self.embeddings = embeddings\n        else:\n            if not self.online:\n                self.embeddings = (\n                    OpenAIEmbeddings(openai_api_key=api_key, model=model)\n                    if not is_azure\n                    else AzureOpenAIEmbeddings(\n                        api_key=api_key,\n                        azure_deployment=azure_deployment,\n                        azure_endpoint=azure_endpoint,\n                        model=model,\n                    )\n                )\n            else:\n                self.embeddings = None\n\n        # connection arguments\n        self.connection_args = connection_args or {\n            \"host\": \"127.0.0.1\",\n            \"port\": \"19530\",\n        }\n        self.embedding_collection_name = embedding_collection_name\n        self.metadata_collection_name = metadata_collection_name\n        self.documentids_workspace = documentids_workspace\n\n        # TODO: vector db selection\n        self.vector_db_vendor = vector_db_vendor or \"milvus\"\n        # instantiate VectorDatabaseHost\n        self.database_host = None\n        self._init_database_host()\n\n    def _set_embeddings(self, embeddings):\n        print(\"setting embedder\")\n        self.embeddings = embeddings\n\n    def _init_database_host(self):\n        if self.vector_db_vendor == \"milvus\":\n            self.database_host = VectorDatabaseAgentMilvus(\n                embedding_func=self.embeddings,\n                connection_args=self.connection_args,\n                embedding_collection_name=self.embedding_collection_name,\n                metadata_collection_name=self.metadata_collection_name,\n            )\n        else:\n            raise NotImplementedError(self.vector_db_vendor)\n\n    def set_chunk_siue(self, chunk_size: int) -&gt; None:\n        self.chunk_size = chunk_size\n\n    def set_chunk_overlap(self, chunk_overlap: int) -&gt; None:\n        self.chunk_overlap = chunk_overlap\n\n    def set_separators(self, separators: list) -&gt; None:\n        self.separators = separators\n\n    def _characters_splitter(self) -&gt; RecursiveCharacterTextSplitter:\n        return RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=self.separators,\n        )\n\n    def _tokens_splitter(self) -&gt; RecursiveCharacterTextSplitter:\n        DEFAULT_OPENAI_MODEL = \"gpt-3.5-turbo\"\n        HUGGINGFACE_MODELS = [\"bigscience/bloom\"]\n        if self.model_name and self.model_name in HUGGINGFACE_MODELS:\n            tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n            return RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n                tokenizer,\n                chunk_size=self.chunk_size,\n                chunk_overlap=self.chunk_overlap,\n                separators=self.separators,\n            )\n        else:\n            return RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n                encoding_name=\"\",\n                model_name=(\n                    DEFAULT_OPENAI_MODEL\n                    if not self.model_name\n                    else self.model_name\n                ),\n                chunk_size=self.chunk_size,\n                chunk_overlap=self.chunk_overlap,\n                separators=self.separators,\n            )\n\n    def _text_splitter(self) -&gt; RecursiveCharacterTextSplitter:\n        return (\n            self._characters_splitter()\n            if self.split_by_characters\n            else self._tokens_splitter()\n        )\n\n    def save_document(self, doc: List[Document]) -&gt; str:\n        \"\"\"\n        This function saves document to the vector database\n        Args:\n            doc List[Document]: document content, read with DocumentReader load_document(),\n                or document_from_pdf(), document_from_txt()\n        Returns:\n            str: document id, which can be used to remove an uploaded document with remove_document()\n        \"\"\"\n        splitted = self._split_document(doc)\n        return self._store_embeddings(splitted)\n\n    def _split_document(self, document: List[Document]) -&gt; List[Document]:\n        text_splitter = self._text_splitter()\n        return text_splitter.split_documents(document)\n\n    def _store_embeddings(self, doc: List[Document]) -&gt; str:\n        return self.database_host.store_embeddings(documents=doc)\n\n    def connect(self) -&gt; None:\n        self.database_host.connect()\n\n    def get_all_documents(self) -&gt; List[Dict]:\n        return self.database_host.get_all_documents(\n            doc_ids=self.documentids_workspace\n        )\n\n    def remove_document(self, doc_id: str) -&gt; None:\n        return self.database_host.remove_document(\n            doc_id, self.documentids_workspace\n        )\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.DocumentEmbedder.__init__","title":"<code>__init__(used=False, online=False, chunk_size=1000, chunk_overlap=0, split_by_characters=True, separators=None, n_results=3, model='text-embedding-ada-002', vector_db_vendor=None, connection_args=None, embedding_collection_name=None, metadata_collection_name=None, api_key=None, is_azure=False, azure_deployment=None, azure_endpoint=None, base_url=None, embeddings=None, documentids_workspace=None)</code>","text":"<pre><code>    Class that handles the retrieval-augmented generation (RAG) functionality\n    of BioChatter. It splits text into chunks, embeds them, and stores them in\n    a vector database. It can then be used to do similarity search on the\n    database.\n\n    Args:\n\n        used (bool, optional): whether RAG has been used (ChatGSE setting).\n            Defaults to False.\n\n        online (bool, optional): whether we are running ChatGSE online.\n            Defaults to False.\n\n        chunk_size (int, optional): size of chunks to split text into.\n            Defaults to 1000.\n\n        chunk_overlap (int, optional): overlap between chunks. Defaults to 0.\n\n        split_by_characters (bool, optional): whether to split by characters\n            or tokens. Defaults to True.\n\n        separators (Optional[list], optional): list of separators to use when\n            splitting by characters. Defaults to [\" \", \",\", \"\n</code></pre> <p>\"].</p> <pre><code>        n_results (int, optional): number of results to return from\n            similarity search. Defaults to 3.\n\n        model (Optional[str], optional): name of model to use for embeddings.\n            Defaults to 'text-embedding-ada-002'.\n\n        vector_db_vendor (Optional[str], optional): name of vector database\n            to use. Defaults to Milvus.\n\n        connection_args (Optional[dict], optional): arguments to pass to\n            vector database connection. Defaults to None.\n\n        api_key (Optional[str], optional): OpenAI API key. Defaults to None.\n\n        base_url (Optional[str], optional): base url of OpenAI API.\n\n        embeddings (Optional[OpenAIEmbeddings | XinferenceEmbeddings],\n            optional): Embeddings object to use. Defaults to OpenAI.\n\n        documentids_workspace (Optional[List[str]], optional): a list of document IDs\n            that defines the scope within which rag operations (remove, similarity search,\n            and get all) occur. Defaults to None, which means the operations will be\n            performed across all documents in the database.\n\n        is_azure (Optional[bool], optional): if we are using Azure\n        azure_deployment (Optional[str], optional): Azure embeddings model deployment,\n            should work with azure_endpoint when is_azure is True\n        azure_endpoint (Optional[str], optional): Azure endpoint, should work with\n            azure_deployment when is_azure is True\n</code></pre> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def __init__(\n    self,\n    used: bool = False,\n    online: bool = False,\n    chunk_size: int = 1000,\n    chunk_overlap: int = 0,\n    split_by_characters: bool = True,\n    separators: Optional[list] = None,\n    n_results: int = 3,\n    model: Optional[str] = \"text-embedding-ada-002\",\n    vector_db_vendor: Optional[str] = None,\n    connection_args: Optional[dict] = None,\n    embedding_collection_name: Optional[str] = None,\n    metadata_collection_name: Optional[str] = None,\n    api_key: Optional[str] = None,\n    is_azure: Optional[bool] = False,\n    azure_deployment: Optional[str] = None,\n    azure_endpoint: Optional[str] = None,\n    base_url: Optional[str] = None,\n    embeddings: Optional[OpenAIEmbeddings | XinferenceEmbeddings] = None,\n    documentids_workspace: Optional[List[str]] = None,\n) -&gt; None:\n    \"\"\"\n    Class that handles the retrieval-augmented generation (RAG) functionality\n    of BioChatter. It splits text into chunks, embeds them, and stores them in\n    a vector database. It can then be used to do similarity search on the\n    database.\n\n    Args:\n\n        used (bool, optional): whether RAG has been used (ChatGSE setting).\n            Defaults to False.\n\n        online (bool, optional): whether we are running ChatGSE online.\n            Defaults to False.\n\n        chunk_size (int, optional): size of chunks to split text into.\n            Defaults to 1000.\n\n        chunk_overlap (int, optional): overlap between chunks. Defaults to 0.\n\n        split_by_characters (bool, optional): whether to split by characters\n            or tokens. Defaults to True.\n\n        separators (Optional[list], optional): list of separators to use when\n            splitting by characters. Defaults to [\" \", \",\", \"\\n\"].\n\n        n_results (int, optional): number of results to return from\n            similarity search. Defaults to 3.\n\n        model (Optional[str], optional): name of model to use for embeddings.\n            Defaults to 'text-embedding-ada-002'.\n\n        vector_db_vendor (Optional[str], optional): name of vector database\n            to use. Defaults to Milvus.\n\n        connection_args (Optional[dict], optional): arguments to pass to\n            vector database connection. Defaults to None.\n\n        api_key (Optional[str], optional): OpenAI API key. Defaults to None.\n\n        base_url (Optional[str], optional): base url of OpenAI API.\n\n        embeddings (Optional[OpenAIEmbeddings | XinferenceEmbeddings],\n            optional): Embeddings object to use. Defaults to OpenAI.\n\n        documentids_workspace (Optional[List[str]], optional): a list of document IDs\n            that defines the scope within which rag operations (remove, similarity search,\n            and get all) occur. Defaults to None, which means the operations will be\n            performed across all documents in the database.\n\n        is_azure (Optional[bool], optional): if we are using Azure\n        azure_deployment (Optional[str], optional): Azure embeddings model deployment,\n            should work with azure_endpoint when is_azure is True\n        azure_endpoint (Optional[str], optional): Azure endpoint, should work with\n            azure_deployment when is_azure is True\n\n    \"\"\"\n    self.used = used\n    self.online = online\n    self.chunk_size = chunk_size\n    self.chunk_overlap = chunk_overlap\n    self.separators = separators or [\" \", \",\", \"\\n\"]\n    self.n_results = n_results\n    self.split_by_characters = split_by_characters\n    self.model_name = model\n\n    # TODO API Key handling to central config?\n    if base_url:\n        openai.api_base = base_url\n\n    if embeddings:\n        self.embeddings = embeddings\n    else:\n        if not self.online:\n            self.embeddings = (\n                OpenAIEmbeddings(openai_api_key=api_key, model=model)\n                if not is_azure\n                else AzureOpenAIEmbeddings(\n                    api_key=api_key,\n                    azure_deployment=azure_deployment,\n                    azure_endpoint=azure_endpoint,\n                    model=model,\n                )\n            )\n        else:\n            self.embeddings = None\n\n    # connection arguments\n    self.connection_args = connection_args or {\n        \"host\": \"127.0.0.1\",\n        \"port\": \"19530\",\n    }\n    self.embedding_collection_name = embedding_collection_name\n    self.metadata_collection_name = metadata_collection_name\n    self.documentids_workspace = documentids_workspace\n\n    # TODO: vector db selection\n    self.vector_db_vendor = vector_db_vendor or \"milvus\"\n    # instantiate VectorDatabaseHost\n    self.database_host = None\n    self._init_database_host()\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.DocumentEmbedder.save_document","title":"<code>save_document(doc)</code>","text":"<p>This function saves document to the vector database Args:     doc List[Document]: document content, read with DocumentReader load_document(),         or document_from_pdf(), document_from_txt() Returns:     str: document id, which can be used to remove an uploaded document with remove_document()</p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def save_document(self, doc: List[Document]) -&gt; str:\n    \"\"\"\n    This function saves document to the vector database\n    Args:\n        doc List[Document]: document content, read with DocumentReader load_document(),\n            or document_from_pdf(), document_from_txt()\n    Returns:\n        str: document id, which can be used to remove an uploaded document with remove_document()\n    \"\"\"\n    splitted = self._split_document(doc)\n    return self._store_embeddings(splitted)\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.DocumentReader","title":"<code>DocumentReader</code>","text":"Source code in <code>biochatter/vectorstore.py</code> <pre><code>class DocumentReader:\n    def load_document(self, path: str) -&gt; List[Document]:\n        \"\"\"\n        Loads a document from a path; accepts txt and pdf files. Txt files are\n        loaded as-is, pdf files are converted to text using fitz.\n\n        Args:\n            path (str): path to document\n\n        Returns:\n            List[Document]: list of documents\n        \"\"\"\n        if path.endswith(\".txt\"):\n            loader = TextLoader(path)\n            return loader.load()\n\n        elif path.endswith(\".pdf\"):\n            doc = fitz.open(path)\n            text = \"\"\n            for page in doc:\n                text += page.get_text()\n\n            meta = {k: v for k, v in doc.metadata.items() if v}\n            meta.update({\"source\": path})\n\n            return [\n                Document(\n                    page_content=text,\n                    metadata=meta,\n                )\n            ]\n\n    def document_from_pdf(self, pdf: bytes) -&gt; List[Document]:\n        \"\"\"\n        Receive a byte representation of a pdf file and return a list of Documents\n        with metadata.\n\n        Args:\n            pdf (bytes): byte representation of pdf file\n\n        Returns:\n            List[Document]: list of documents\n        \"\"\"\n        doc = fitz.open(stream=pdf, filetype=\"pdf\")\n        text = \"\"\n        for page in doc:\n            text += page.get_text()\n\n        meta = {k: v for k, v in doc.metadata.items() if v}\n        meta.update({\"source\": \"pdf\"})\n\n        return [\n            Document(\n                page_content=text,\n                metadata=meta,\n            )\n        ]\n\n    def document_from_txt(self, txt: bytes) -&gt; List[Document]:\n        \"\"\"\n        Receive a byte representation of a txt file and return a list of Documents\n        with metadata.\n\n        Args:\n            txt (bytes): byte representation of txt file\n\n        Returns:\n            List[Document]: list of documents\n        \"\"\"\n        meta = {\"source\": \"txt\"}\n        return [\n            Document(\n                page_content=txt,\n                metadata=meta,\n            )\n        ]\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.DocumentReader.document_from_pdf","title":"<code>document_from_pdf(pdf)</code>","text":"<p>Receive a byte representation of a pdf file and return a list of Documents with metadata.</p> <p>Parameters:</p> Name Type Description Default <code>pdf</code> <code>bytes</code> <p>byte representation of pdf file</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: list of documents</p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def document_from_pdf(self, pdf: bytes) -&gt; List[Document]:\n    \"\"\"\n    Receive a byte representation of a pdf file and return a list of Documents\n    with metadata.\n\n    Args:\n        pdf (bytes): byte representation of pdf file\n\n    Returns:\n        List[Document]: list of documents\n    \"\"\"\n    doc = fitz.open(stream=pdf, filetype=\"pdf\")\n    text = \"\"\n    for page in doc:\n        text += page.get_text()\n\n    meta = {k: v for k, v in doc.metadata.items() if v}\n    meta.update({\"source\": \"pdf\"})\n\n    return [\n        Document(\n            page_content=text,\n            metadata=meta,\n        )\n    ]\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.DocumentReader.document_from_txt","title":"<code>document_from_txt(txt)</code>","text":"<p>Receive a byte representation of a txt file and return a list of Documents with metadata.</p> <p>Parameters:</p> Name Type Description Default <code>txt</code> <code>bytes</code> <p>byte representation of txt file</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: list of documents</p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def document_from_txt(self, txt: bytes) -&gt; List[Document]:\n    \"\"\"\n    Receive a byte representation of a txt file and return a list of Documents\n    with metadata.\n\n    Args:\n        txt (bytes): byte representation of txt file\n\n    Returns:\n        List[Document]: list of documents\n    \"\"\"\n    meta = {\"source\": \"txt\"}\n    return [\n        Document(\n            page_content=txt,\n            metadata=meta,\n        )\n    ]\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.DocumentReader.load_document","title":"<code>load_document(path)</code>","text":"<p>Loads a document from a path; accepts txt and pdf files. Txt files are loaded as-is, pdf files are converted to text using fitz.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path to document</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: list of documents</p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def load_document(self, path: str) -&gt; List[Document]:\n    \"\"\"\n    Loads a document from a path; accepts txt and pdf files. Txt files are\n    loaded as-is, pdf files are converted to text using fitz.\n\n    Args:\n        path (str): path to document\n\n    Returns:\n        List[Document]: list of documents\n    \"\"\"\n    if path.endswith(\".txt\"):\n        loader = TextLoader(path)\n        return loader.load()\n\n    elif path.endswith(\".pdf\"):\n        doc = fitz.open(path)\n        text = \"\"\n        for page in doc:\n            text += page.get_text()\n\n        meta = {k: v for k, v in doc.metadata.items() if v}\n        meta.update({\"source\": path})\n\n        return [\n            Document(\n                page_content=text,\n                metadata=meta,\n            )\n        ]\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.XinferenceDocumentEmbedder","title":"<code>XinferenceDocumentEmbedder</code>","text":"<p>             Bases: <code>DocumentEmbedder</code></p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>class XinferenceDocumentEmbedder(DocumentEmbedder):\n    def __init__(\n        self,\n        used: bool = False,\n        chunk_size: int = 1000,\n        chunk_overlap: int = 0,\n        split_by_characters: bool = True,\n        separators: Optional[list] = None,\n        n_results: int = 3,\n        model: Optional[str] = \"auto\",\n        vector_db_vendor: Optional[str] = None,\n        connection_args: Optional[dict] = None,\n        embedding_collection_name: Optional[str] = None,\n        metadata_collection_name: Optional[str] = None,\n        api_key: Optional[str] = \"none\",\n        base_url: Optional[str] = None,\n        documentids_workspace: Optional[List[str]] = None,\n    ):\n        \"\"\"\n        Extension of the DocumentEmbedder class that uses Xinference for\n        embeddings.\n\n        Args:\n\n            used (bool, optional): whether RAG has been used (ChatGSE setting).\n\n            chunk_size (int, optional): size of chunks to split text into.\n\n            chunk_overlap (int, optional): overlap between chunks.\n\n            split_by_characters (bool, optional): whether to split by characters\n            or tokens.\n\n            separators (Optional[list], optional): list of separators to use when\n            splitting by characters.\n\n            n_results (int, optional): number of results to return from\n            similarity search.\n\n            model (Optional[str], optional): name of model to use for embeddings.\n            Can be \"auto\" to use the first available model.\n\n            vector_db_vendor (Optional[str], optional): name of vector database\n            to use.\n\n            connection_args (Optional[dict], optional): arguments to pass to\n            vector database connection.\n\n            embedding_collection_name (Optional[str], optional): name of\n            collection to store embeddings in.\n\n            metadata_collection_name (Optional[str], optional): name of\n            collection to store metadata in.\n\n            api_key (Optional[str], optional): Xinference API key.\n\n            base_url (Optional[str], optional): base url of Xinference API.\n\n            documentids_workspace (Optional[List[str]], optional): a list of document IDs\n            that defines the scope within which rag operations (remove, similarity search,\n            and get all) occur. Defaults to None, which means the operations will be\n            performed across all documents in the database.\n\n        \"\"\"\n        from xinference.client import Client\n\n        self.model_name = model\n        self.client = Client(base_url=base_url)\n        self.models = {}\n        self.load_models()\n\n        if self.model_name is None or self.model_name == \"auto\":\n            self.model_name = self.list_models_by_type(\"embedding\")[0]\n        self.model_uid = self.models[self.model_name][\"id\"]\n\n        super().__init__(\n            used=used,\n            online=True,\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap,\n            split_by_characters=split_by_characters,\n            separators=separators,\n            n_results=n_results,\n            model=model,\n            vector_db_vendor=vector_db_vendor,\n            connection_args=connection_args,\n            embedding_collection_name=embedding_collection_name,\n            metadata_collection_name=metadata_collection_name,\n            api_key=api_key,\n            base_url=base_url,\n            embeddings=XinferenceEmbeddings(\n                server_url=base_url, model_uid=self.model_uid\n            ),\n            documentids_workspace=documentids_workspace,\n        )\n\n    def load_models(self):\n        \"\"\"\n        Return all models that are currently available on the Xinference server.\n\n        Returns:\n            dict: dict of models\n        \"\"\"\n        for id, model in self.client.list_models().items():\n            model[\"id\"] = id\n            self.models[model[\"model_name\"]] = model\n\n    def list_models_by_type(self, type: str):\n        \"\"\"\n        Return all models of a certain type that are currently available on the\n        Xinference server.\n\n        Args:\n            type (str): type of model to list (e.g. \"embedding\", \"chat\")\n\n        Returns:\n            List[str]: list of model names\n        \"\"\"\n        names = []\n        for name, model in self.models.items():\n            if \"model_ability\" in model:\n                if type in model[\"model_ability\"]:\n                    names.append(name)\n            elif model[\"model_type\"] == type:\n                names.append(name)\n        return names\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.XinferenceDocumentEmbedder.__init__","title":"<code>__init__(used=False, chunk_size=1000, chunk_overlap=0, split_by_characters=True, separators=None, n_results=3, model='auto', vector_db_vendor=None, connection_args=None, embedding_collection_name=None, metadata_collection_name=None, api_key='none', base_url=None, documentids_workspace=None)</code>","text":"<p>Extension of the DocumentEmbedder class that uses Xinference for embeddings.</p> <p>Args:</p> <pre><code>used (bool, optional): whether RAG has been used (ChatGSE setting).\n\nchunk_size (int, optional): size of chunks to split text into.\n\nchunk_overlap (int, optional): overlap between chunks.\n\nsplit_by_characters (bool, optional): whether to split by characters\nor tokens.\n\nseparators (Optional[list], optional): list of separators to use when\nsplitting by characters.\n\nn_results (int, optional): number of results to return from\nsimilarity search.\n\nmodel (Optional[str], optional): name of model to use for embeddings.\nCan be \"auto\" to use the first available model.\n\nvector_db_vendor (Optional[str], optional): name of vector database\nto use.\n\nconnection_args (Optional[dict], optional): arguments to pass to\nvector database connection.\n\nembedding_collection_name (Optional[str], optional): name of\ncollection to store embeddings in.\n\nmetadata_collection_name (Optional[str], optional): name of\ncollection to store metadata in.\n\napi_key (Optional[str], optional): Xinference API key.\n\nbase_url (Optional[str], optional): base url of Xinference API.\n\ndocumentids_workspace (Optional[List[str]], optional): a list of document IDs\nthat defines the scope within which rag operations (remove, similarity search,\nand get all) occur. Defaults to None, which means the operations will be\nperformed across all documents in the database.\n</code></pre> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def __init__(\n    self,\n    used: bool = False,\n    chunk_size: int = 1000,\n    chunk_overlap: int = 0,\n    split_by_characters: bool = True,\n    separators: Optional[list] = None,\n    n_results: int = 3,\n    model: Optional[str] = \"auto\",\n    vector_db_vendor: Optional[str] = None,\n    connection_args: Optional[dict] = None,\n    embedding_collection_name: Optional[str] = None,\n    metadata_collection_name: Optional[str] = None,\n    api_key: Optional[str] = \"none\",\n    base_url: Optional[str] = None,\n    documentids_workspace: Optional[List[str]] = None,\n):\n    \"\"\"\n    Extension of the DocumentEmbedder class that uses Xinference for\n    embeddings.\n\n    Args:\n\n        used (bool, optional): whether RAG has been used (ChatGSE setting).\n\n        chunk_size (int, optional): size of chunks to split text into.\n\n        chunk_overlap (int, optional): overlap between chunks.\n\n        split_by_characters (bool, optional): whether to split by characters\n        or tokens.\n\n        separators (Optional[list], optional): list of separators to use when\n        splitting by characters.\n\n        n_results (int, optional): number of results to return from\n        similarity search.\n\n        model (Optional[str], optional): name of model to use for embeddings.\n        Can be \"auto\" to use the first available model.\n\n        vector_db_vendor (Optional[str], optional): name of vector database\n        to use.\n\n        connection_args (Optional[dict], optional): arguments to pass to\n        vector database connection.\n\n        embedding_collection_name (Optional[str], optional): name of\n        collection to store embeddings in.\n\n        metadata_collection_name (Optional[str], optional): name of\n        collection to store metadata in.\n\n        api_key (Optional[str], optional): Xinference API key.\n\n        base_url (Optional[str], optional): base url of Xinference API.\n\n        documentids_workspace (Optional[List[str]], optional): a list of document IDs\n        that defines the scope within which rag operations (remove, similarity search,\n        and get all) occur. Defaults to None, which means the operations will be\n        performed across all documents in the database.\n\n    \"\"\"\n    from xinference.client import Client\n\n    self.model_name = model\n    self.client = Client(base_url=base_url)\n    self.models = {}\n    self.load_models()\n\n    if self.model_name is None or self.model_name == \"auto\":\n        self.model_name = self.list_models_by_type(\"embedding\")[0]\n    self.model_uid = self.models[self.model_name][\"id\"]\n\n    super().__init__(\n        used=used,\n        online=True,\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        split_by_characters=split_by_characters,\n        separators=separators,\n        n_results=n_results,\n        model=model,\n        vector_db_vendor=vector_db_vendor,\n        connection_args=connection_args,\n        embedding_collection_name=embedding_collection_name,\n        metadata_collection_name=metadata_collection_name,\n        api_key=api_key,\n        base_url=base_url,\n        embeddings=XinferenceEmbeddings(\n            server_url=base_url, model_uid=self.model_uid\n        ),\n        documentids_workspace=documentids_workspace,\n    )\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.XinferenceDocumentEmbedder.list_models_by_type","title":"<code>list_models_by_type(type)</code>","text":"<p>Return all models of a certain type that are currently available on the Xinference server.</p> <p>Parameters:</p> Name Type Description Default <code>type</code> <code>str</code> <p>type of model to list (e.g. \"embedding\", \"chat\")</p> required <p>Returns:</p> Type Description <p>List[str]: list of model names</p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def list_models_by_type(self, type: str):\n    \"\"\"\n    Return all models of a certain type that are currently available on the\n    Xinference server.\n\n    Args:\n        type (str): type of model to list (e.g. \"embedding\", \"chat\")\n\n    Returns:\n        List[str]: list of model names\n    \"\"\"\n    names = []\n    for name, model in self.models.items():\n        if \"model_ability\" in model:\n            if type in model[\"model_ability\"]:\n                names.append(name)\n        elif model[\"model_type\"] == type:\n            names.append(name)\n    return names\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.XinferenceDocumentEmbedder.load_models","title":"<code>load_models()</code>","text":"<p>Return all models that are currently available on the Xinference server.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>dict of models</p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def load_models(self):\n    \"\"\"\n    Return all models that are currently available on the Xinference server.\n\n    Returns:\n        dict: dict of models\n    \"\"\"\n    for id, model in self.client.list_models().items():\n        model[\"id\"] = id\n        self.models[model[\"model_name\"]] = model\n</code></pre>"},{"location":"vectorstore_agent-reference/","title":"Vectorstore agent module","text":"<p>Here we handle connections and management of various vectorstore services and semantic seach.</p>"},{"location":"vectorstore_agent-reference/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus","title":"<code>VectorDatabaseAgentMilvus</code>","text":"<p>The VectorDatabaseAgentMilvus class manages vector databases in a connected host database. It manages an embedding collection <code>_col_embeddings:langchain.vectorstores.Milvus</code>, which is the main information on the embedded text fragments and the basis for similarity search, and a metadata collection <code>_col_metadata:pymilvus.Collection</code>, which stores the metadata of the embedded text fragments. A typical workflow includes the following operations:</p> <ol> <li>connect to a host using <code>connect()</code></li> <li>get all documents in the active database using <code>get_all_documents()</code></li> <li>save a number of fragments, usually from a specific document, using     <code>store_embeddings()</code></li> <li>do similarity search among all fragments of the currently active database     using <code>similarity_search()</code></li> <li>remove a document from the currently active database using     <code>remove_document()</code></li> </ol> Source code in <code>biochatter/vectorstore_agent.py</code> <pre><code>class VectorDatabaseAgentMilvus:\n    \"\"\"\n    The VectorDatabaseAgentMilvus class manages vector databases in a connected\n    host database. It manages an embedding collection\n    `_col_embeddings:langchain.vectorstores.Milvus`, which is the main\n    information on the embedded text fragments and the basis for similarity\n    search, and a metadata collection `_col_metadata:pymilvus.Collection`, which\n    stores the metadata of the embedded text fragments. A typical workflow\n    includes the following operations:\n\n    1. connect to a host using `connect()`\n    2. get all documents in the active database using `get_all_documents()`\n    3. save a number of fragments, usually from a specific document, using\n        `store_embeddings()`\n    4. do similarity search among all fragments of the currently active database\n        using `similarity_search()`\n    5. remove a document from the currently active database using\n        `remove_document()`\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding_func: OpenAIEmbeddings,\n        connection_args: Optional[Dict] = None,\n        embedding_collection_name: Optional[str] = None,\n        metadata_collection_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Args:\n            embedding_func OpenAIEmbeddings: Function used to embed the text\n\n            connection_args Optional dict: args to connect Vector Database\n\n            embedding_collection_name Optional str: exposed for test\n\n            metadata_collection_name Optional str: exposed for test\n        \"\"\"\n        self._embedding_func = embedding_func\n        self._col_embeddings: Optional[Milvus] = None\n        self._col_metadata: Optional[Collection] = None\n        self._connection_args = validate_connection_args(connection_args)\n        self._embedding_name = (\n            embedding_collection_name or DOCUMENT_EMBEDDINGS_COLLECTION_NAME\n        )\n        self._metadata_name = (\n            metadata_collection_name or DOCUMENT_METADATA_COLLECTION_NAME\n        )\n\n    def connect(self) -&gt; None:\n        \"\"\"\n        Connect to a host and read two document collections (the default names\n        are `DocumentEmbeddings` and `DocumentMetadata`) in the currently active\n        database (default database name is `default`); if those document\n        collections don't exist, create the two collections.\n        \"\"\"\n        self._connect(**self._connection_args)\n        self._init_host()\n\n    def _connect(self, host: str, port: str, user: str, password: str) -&gt; None:\n        self.alias = self._create_connection_alias(host, port, user, password)\n\n    def _init_host(self) -&gt; None:\n        \"\"\"\n        Initialize host. Will read/create document collection inside currently\n        active database.\n        \"\"\"\n        self._create_collections()\n\n    def _create_connection_alias(\n        self, host: str, port: str, user: str, password: str\n    ) -&gt; str:\n        \"\"\"\n        Connect to host and create a connection alias for metadata collection\n        using a random uuid.\n\n        Args:\n            host (str): host ip address\n            port (str): host port\n\n        Returns:\n            str: connection alias\n        \"\"\"\n        alias = uuid.uuid4().hex\n        try:\n            connections.connect(\n                host=host, port=port, user=user, password=password, alias=alias\n            )\n            logger.debug(f\"Created new connection using: {alias}\")\n            return alias\n        except MilvusException as e:\n            logger.error(f\"Failed to create  new connection using: {alias}\")\n            raise e\n\n    def _create_collections(self) -&gt; None:\n        \"\"\"\n        Create or load the embedding and metadata collections from the currently\n        active database.\n        \"\"\"\n        embedding_exists = utility.has_collection(\n            self._embedding_name, using=self.alias\n        )\n        meta_exists = utility.has_collection(\n            self._metadata_name,\n            using=self.alias,\n        )\n\n        if embedding_exists:\n            self._load_embeddings_collection()\n        else:\n            self._create_embeddings_collection()\n\n        if meta_exists:\n            self._load_metadata_collection()\n        else:\n            self._create_metadata_collection()\n\n        self._create_metadata_collection_index()\n        self._col_metadata.load()\n\n    def _load_embeddings_collection(self) -&gt; None:\n        \"\"\"\n        Load embeddings collection from currently active database.\n        \"\"\"\n        try:\n            self._col_embeddings = Milvus(\n                embedding_function=self._embedding_func,\n                collection_name=self._embedding_name,\n                connection_args=self._connection_args,\n            )\n        except MilvusException as e:\n            logger.error(\n                f\"Failed to load embeddings collection {self._embedding_name}.\"\n            )\n            raise e\n\n    def _create_embeddings_collection(self) -&gt; None:\n        \"\"\"\n        Create embedding collection.\n        All fields: \"meta_id\", \"vector\"\n        \"\"\"\n        try:\n            self._col_embeddings = Milvus(\n                embedding_function=self._embedding_func,\n                collection_name=self._embedding_name,\n                connection_args=self._connection_args,\n            )\n        except MilvusException as e:\n            logger.error(\n                f\"Failed to create embeddings collection {self._embedding_name}\"\n            )\n            raise e\n\n    def _load_metadata_collection(self) -&gt; None:\n        \"\"\"\n        Load metadata collection from currently active database.\n        \"\"\"\n        self._col_metadata = Collection(\n            self._metadata_name,\n            using=self.alias,\n        )\n        self._col_metadata.load()\n\n    def _create_metadata_collection(self) -&gt; None:\n        \"\"\"\n        Create metadata collection.\n\n        All fields: \"id\", \"name\", \"author\", \"title\", \"format\", \"subject\",\n        \"creator\", \"producer\", \"creationDate\", \"modDate\", \"source\", \"embedding\",\n        \"isDeleted\".\n\n        As the vector database requires a vector field, we will create a fake\n        vector \"embedding\". The field \"isDeleted\" is used to specify if the\n        document is deleted.\n        \"\"\"\n        doc_id = FieldSchema(\n            name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True\n        )\n        doc_name = FieldSchema(\n            name=\"name\", dtype=DataType.VARCHAR, max_length=255\n        )\n        doc_author = FieldSchema(\n            name=\"author\", dtype=DataType.VARCHAR, max_length=255\n        )\n        doc_title = FieldSchema(\n            name=\"title\", dtype=DataType.VARCHAR, max_length=1000\n        )\n        doc_format = FieldSchema(\n            name=\"format\", dtype=DataType.VARCHAR, max_length=255\n        )\n        doc_subject = FieldSchema(\n            name=\"subject\", dtype=DataType.VARCHAR, max_length=255\n        )\n        doc_creator = FieldSchema(\n            name=\"creator\", dtype=DataType.VARCHAR, max_length=255\n        )\n        doc_producer = FieldSchema(\n            name=\"producer\", dtype=DataType.VARCHAR, max_length=255\n        )\n        doc_creationDate = FieldSchema(\n            name=\"creationDate\", dtype=DataType.VARCHAR, max_length=64\n        )\n        doc_modDate = FieldSchema(\n            name=\"modDate\", dtype=DataType.VARCHAR, max_length=64\n        )\n        doc_source = FieldSchema(\n            name=\"source\", dtype=DataType.VARCHAR, max_length=1000\n        )\n        embedding = FieldSchema(\n            name=\"embedding\",\n            dtype=DataType.FLOAT_VECTOR,\n            dim=METADATA_VECTOR_DIM,\n        )\n        isDeleted = FieldSchema(\n            name=\"isDeleted\",\n            dtype=DataType.BOOL,\n        )\n        fields = [\n            doc_id,\n            doc_name,\n            doc_author,\n            doc_title,\n            doc_format,\n            doc_subject,\n            doc_creator,\n            doc_producer,\n            doc_creationDate,\n            doc_modDate,\n            doc_source,\n            embedding,\n            isDeleted,\n        ]\n        schema = CollectionSchema(fields=fields)\n        try:\n            self._col_metadata = Collection(\n                name=self._metadata_name, schema=schema, using=self.alias\n            )\n        except MilvusException as e:\n            logger.error(f\"Failed to create collection {self._metadata_name}\")\n            raise e\n\n    def _create_metadata_collection_index(self) -&gt; None:\n        \"\"\"\n        Create index for metadata collection in currently active database.\n        \"\"\"\n        if (\n            not isinstance(self._col_metadata, Collection)\n            or len(self._col_metadata.indexes) &gt; 0\n        ):\n            return\n\n        index_params = {\n            \"metric_type\": \"L2\",\n            \"index_type\": \"HNSW\",\n            \"params\": {\"M\": 8, \"efConstruction\": 64},\n        }\n\n        try:\n            self._col_metadata.create_index(\n                field_name=\"embedding\",\n                index_params=index_params,\n                using=self.alias,\n            )\n        except MilvusException as e:\n            logger.error(\n                \"Failed to create index for meta collection \"\n                f\"{self._metadata_name}.\"\n            )\n            raise e\n\n    def _insert_data(self, documents: List[Document]) -&gt; str:\n        \"\"\"\n        Insert documents into the currently active database.\n\n        Args:\n            documents (List[Documents]): documents array, usually from\n                DocumentReader.load_document, DocumentReader.document_from_pdf,\n                DocumentReader.document_from_txt\n\n        Returns:\n            str: document id\n        \"\"\"\n        if len(documents) == 0:\n            return None\n        metadata = [documents[0].metadata]\n        aligned_metadata = align_metadata(metadata)\n        try:\n            result = self._col_metadata.insert(aligned_metadata)\n            meta_id = str(result.primary_keys[0])\n            self._col_metadata.flush()\n        except MilvusException as e:\n            logger.error(f\"Failed to insert meta data\")\n            raise e\n        aligned_docs = align_embeddings(documents, meta_id)\n        try:\n            # As we passed collection_name, documents will be added to existed collection\n            self._col_embeddings = Milvus.from_documents(\n                embedding=self._embedding_func,\n                collection_name=self._embedding_name,\n                connection_args=self._connection_args,\n                documents=aligned_docs,\n            )\n        except MilvusException as e:\n            logger.error(\n                \"Failed to insert data to embedding collection \"\n                f\"{self._embedding_name}.\"\n            )\n            raise e\n        return meta_id\n\n    def store_embeddings(self, documents: List[Document]) -&gt; str:\n        \"\"\"\n        Store documents in the currently active database.\n\n        Args:\n            documents (List[Documents]): documents array, usually from\n                DocumentReader.load_document, DocumentReader.document_from_pdf,\n                DocumentReader.document_from_txt\n\n        Returns:\n            str: document id\n        \"\"\"\n        if len(documents) == 0:\n            return\n        return self._insert_data(documents)\n\n    def _build_embedding_search_expression(\n        self, meta_ids: List[Dict]\n    ) -&gt; Optional[str]:\n        \"\"\"\n        Build search expression for embedding collection. The generated\n        expression follows the pattern: \"meta_id in [{id1}, {id2}, ...]\n\n        Args:\n            meta_ids: the array of metadata id in metadata collection\n\n        Returns:\n            str: search expression or None\n        \"\"\"\n        if len(meta_ids) == 0:\n            return \"meta_id in []\"\n        built_expr = \"\"\"meta_id in [\"\"\"\n        for item in meta_ids:\n            id = f'\"{item[\"id\"]}\",'\n            built_expr += id\n        built_expr = built_expr[:-1]\n        built_expr += \"\"\"]\"\"\"\n        return built_expr\n\n    def _join_embedding_and_metadata_results(\n        self, result_embedding: List[Document], result_meta: List[Dict]\n    ) -&gt; List[Document]:\n        \"\"\"\n        Join the search results of embedding collection and results of metadata.\n\n        Args:\n            result_embedding (List[Document]): search result of embedding\n                collection\n\n            result_meta (List[Dict]): search result of metadata collection\n\n        Returns:\n            List[Document]: combined results like\n                [{page_content: str, metadata: {...}}]\n        \"\"\"\n\n        def _find_metadata_by_id(\n            metadata: List[Dict], id: str\n        ) -&gt; Optional[Dict]:\n            for d in metadata:\n                if str(d[\"id\"]) == id:\n                    return d\n            return None\n\n        joined_docs = []\n        for res in result_embedding:\n            found = _find_metadata_by_id(result_meta, res.metadata[\"meta_id\"])\n            if found is None:  # discard\n                logger.error(\n                    f\"Failed to join meta_id {res.metadata['meta_id']}\"\n                )\n                continue\n            joined_docs.append(\n                Document(page_content=res.page_content, metadata=found)\n            )\n        return joined_docs\n\n    @staticmethod\n    def _build_meta_col_query_expr_for_all_documents(\n        doc_ids: Optional[List[str]] = None,\n    ) -&gt; str:\n        \"\"\"\n        Build metadata collection query expression to obtain all documents.\n\n        Args:\n            doc_ids: the list of document ids (metadata ids), if thie argument is None,\n                     that is, the query is to get all undeleted documents in metadata collection.\n                     Otherwise, the query is to getr all undeleted documents form provided doc_ids\n\n        Returns:\n            query: str\n        \"\"\"\n        expr = (\n            f\"id in {doc_ids} and isDeleted == false\"\n            if doc_ids is not None\n            else \"isDeleted == false\"\n        )\n        return expr.replace('\"', \"\").replace(\"'\", \"\")\n\n    def similarity_search(\n        self, query: str, k: int = 3, doc_ids: List[str] = None\n    ) -&gt; List[Document]:\n        \"\"\"\n        Perform similarity search insider the currently active database\n        according to the input query.\n\n        This method will:\n        1. get all non-deleted meta_id and build to search expression for\n            the currently active embedding collection\n        2. do similarity search in the embedding collection\n        3. combine metadata and embeddings\n\n        Args:\n            query (str): query string\n\n            k (int): the number of results to return\n\n            doc_ids(List[str] optional): the list of document ids, do similarity search across the\n            specified documents\n\n        Returns:\n            List[Document]: search results\n        \"\"\"\n        result_metadata = []\n        expr = VectorDatabaseAgentMilvus._build_meta_col_query_expr_for_all_documents(\n            doc_ids\n        )\n        result_metadata = self._col_metadata.query(\n            expr=expr, output_fields=METADATA_FIELDS\n        )\n        expr = self._build_embedding_search_expression(result_metadata)\n        result_embedding = self._col_embeddings.similarity_search(\n            query=query, k=k, expr=expr\n        )\n        return self._join_embedding_and_metadata_results(\n            result_embedding, result_metadata\n        )\n\n    def remove_document(\n        self, doc_id: str, doc_ids: Optional[List[str]] = None\n    ) -&gt; bool:\n        \"\"\"\n        Remove the document include meta data and its embeddings.\n\n        Args:\n            doc_id (str): the document to be deleted\n\n            doc_ids(List[str] optional): the list of document ids, defines documents scope\n            within which remove operation occurs.\n\n        Returns:\n            bool: True if the document is deleted, False otherwise\n        \"\"\"\n        if not self._col_metadata:\n            return False\n        if doc_ids is not None and (\n            len(doc_ids) == 0 or (len(doc_ids) &gt; 0 and not doc_id in doc_ids)\n        ):\n            return False\n        try:\n            expr = f\"id in [{doc_id}]\"\n            res = self._col_metadata.query(\n                expr=expr, output_fields=METADATA_FIELDS\n            )\n            if len(res) == 0:\n                return False\n            del_res = self._col_metadata.delete(expr)\n            self._col_metadata.flush()\n\n            res = self._col_embeddings.col.query(f'meta_id in [\"{doc_id}\"]')\n            if len(res) == 0:\n                return True\n            ids = [item[\"pk\"] for item in res]\n            embedding_expr = f\"pk in {ids}\"\n            del_res = self._col_embeddings.col.delete(expr=embedding_expr)\n            self._col_embeddings.col.flush()\n            return True\n        except MilvusException as e:\n            logger.error(e)\n            raise e\n\n    def get_all_documents(\n        self, doc_ids: Optional[List[str]] = None\n    ) -&gt; List[Dict]:\n        \"\"\"\n        Get all non-deleted documents from the currently active database.\n\n        Args:\n            doc_ids(List[str] optional): the list of document ids, defines documents scope within\n            which the operation of obaining all documents occurs\n\n        Returns:\n            List[Dict]: the metadata of all non-deleted documents in the form\n                [{{id}, {author}, {source}, ...}]\n        \"\"\"\n        try:\n            expr = VectorDatabaseAgentMilvus._build_meta_col_query_expr_for_all_documents(\n                doc_ids\n            )\n            result_metadata = self._col_metadata.query(\n                expr=expr, output_fields=METADATA_FIELDS\n            )\n            return result_metadata\n        except MilvusException as e:\n            logger.error(e)\n            raise e\n</code></pre>"},{"location":"vectorstore_agent-reference/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus.__init__","title":"<code>__init__(embedding_func, connection_args=None, embedding_collection_name=None, metadata_collection_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>embedding_func</code> <code>OpenAIEmbeddings</code> <p>Function used to embed the text</p> required <code>connection_args</code> <code>Optional dict</code> <p>args to connect Vector Database</p> <code>None</code> <code>embedding_collection_name</code> <code>Optional str</code> <p>exposed for test</p> <code>None</code> <code>metadata_collection_name</code> <code>Optional str</code> <p>exposed for test</p> <code>None</code> Source code in <code>biochatter/vectorstore_agent.py</code> <pre><code>def __init__(\n    self,\n    embedding_func: OpenAIEmbeddings,\n    connection_args: Optional[Dict] = None,\n    embedding_collection_name: Optional[str] = None,\n    metadata_collection_name: Optional[str] = None,\n):\n    \"\"\"\n    Args:\n        embedding_func OpenAIEmbeddings: Function used to embed the text\n\n        connection_args Optional dict: args to connect Vector Database\n\n        embedding_collection_name Optional str: exposed for test\n\n        metadata_collection_name Optional str: exposed for test\n    \"\"\"\n    self._embedding_func = embedding_func\n    self._col_embeddings: Optional[Milvus] = None\n    self._col_metadata: Optional[Collection] = None\n    self._connection_args = validate_connection_args(connection_args)\n    self._embedding_name = (\n        embedding_collection_name or DOCUMENT_EMBEDDINGS_COLLECTION_NAME\n    )\n    self._metadata_name = (\n        metadata_collection_name or DOCUMENT_METADATA_COLLECTION_NAME\n    )\n</code></pre>"},{"location":"vectorstore_agent-reference/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus.connect","title":"<code>connect()</code>","text":"<p>Connect to a host and read two document collections (the default names are <code>DocumentEmbeddings</code> and <code>DocumentMetadata</code>) in the currently active database (default database name is <code>default</code>); if those document collections don't exist, create the two collections.</p> Source code in <code>biochatter/vectorstore_agent.py</code> <pre><code>def connect(self) -&gt; None:\n    \"\"\"\n    Connect to a host and read two document collections (the default names\n    are `DocumentEmbeddings` and `DocumentMetadata`) in the currently active\n    database (default database name is `default`); if those document\n    collections don't exist, create the two collections.\n    \"\"\"\n    self._connect(**self._connection_args)\n    self._init_host()\n</code></pre>"},{"location":"vectorstore_agent-reference/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus.get_all_documents","title":"<code>get_all_documents(doc_ids=None)</code>","text":"<p>Get all non-deleted documents from the currently active database.</p> <p>Parameters:</p> Name Type Description Default <code>doc_ids(List[str]</code> <code>optional</code> <p>the list of document ids, defines documents scope within</p> required <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List[Dict]: the metadata of all non-deleted documents in the form [{{id}, {author}, {source}, ...}]</p> Source code in <code>biochatter/vectorstore_agent.py</code> <pre><code>def get_all_documents(\n    self, doc_ids: Optional[List[str]] = None\n) -&gt; List[Dict]:\n    \"\"\"\n    Get all non-deleted documents from the currently active database.\n\n    Args:\n        doc_ids(List[str] optional): the list of document ids, defines documents scope within\n        which the operation of obaining all documents occurs\n\n    Returns:\n        List[Dict]: the metadata of all non-deleted documents in the form\n            [{{id}, {author}, {source}, ...}]\n    \"\"\"\n    try:\n        expr = VectorDatabaseAgentMilvus._build_meta_col_query_expr_for_all_documents(\n            doc_ids\n        )\n        result_metadata = self._col_metadata.query(\n            expr=expr, output_fields=METADATA_FIELDS\n        )\n        return result_metadata\n    except MilvusException as e:\n        logger.error(e)\n        raise e\n</code></pre>"},{"location":"vectorstore_agent-reference/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus.remove_document","title":"<code>remove_document(doc_id, doc_ids=None)</code>","text":"<p>Remove the document include meta data and its embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>doc_id</code> <code>str</code> <p>the document to be deleted</p> required <code>doc_ids(List[str]</code> <code>optional</code> <p>the list of document ids, defines documents scope</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the document is deleted, False otherwise</p> Source code in <code>biochatter/vectorstore_agent.py</code> <pre><code>def remove_document(\n    self, doc_id: str, doc_ids: Optional[List[str]] = None\n) -&gt; bool:\n    \"\"\"\n    Remove the document include meta data and its embeddings.\n\n    Args:\n        doc_id (str): the document to be deleted\n\n        doc_ids(List[str] optional): the list of document ids, defines documents scope\n        within which remove operation occurs.\n\n    Returns:\n        bool: True if the document is deleted, False otherwise\n    \"\"\"\n    if not self._col_metadata:\n        return False\n    if doc_ids is not None and (\n        len(doc_ids) == 0 or (len(doc_ids) &gt; 0 and not doc_id in doc_ids)\n    ):\n        return False\n    try:\n        expr = f\"id in [{doc_id}]\"\n        res = self._col_metadata.query(\n            expr=expr, output_fields=METADATA_FIELDS\n        )\n        if len(res) == 0:\n            return False\n        del_res = self._col_metadata.delete(expr)\n        self._col_metadata.flush()\n\n        res = self._col_embeddings.col.query(f'meta_id in [\"{doc_id}\"]')\n        if len(res) == 0:\n            return True\n        ids = [item[\"pk\"] for item in res]\n        embedding_expr = f\"pk in {ids}\"\n        del_res = self._col_embeddings.col.delete(expr=embedding_expr)\n        self._col_embeddings.col.flush()\n        return True\n    except MilvusException as e:\n        logger.error(e)\n        raise e\n</code></pre>"},{"location":"vectorstore_agent-reference/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus.similarity_search","title":"<code>similarity_search(query, k=3, doc_ids=None)</code>","text":"<p>Perform similarity search insider the currently active database according to the input query.</p> <p>This method will: 1. get all non-deleted meta_id and build to search expression for     the currently active embedding collection 2. do similarity search in the embedding collection 3. combine metadata and embeddings</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>query string</p> required <code>k</code> <code>int</code> <p>the number of results to return</p> <code>3</code> <code>doc_ids(List[str]</code> <code>optional</code> <p>the list of document ids, do similarity search across the</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: search results</p> Source code in <code>biochatter/vectorstore_agent.py</code> <pre><code>def similarity_search(\n    self, query: str, k: int = 3, doc_ids: List[str] = None\n) -&gt; List[Document]:\n    \"\"\"\n    Perform similarity search insider the currently active database\n    according to the input query.\n\n    This method will:\n    1. get all non-deleted meta_id and build to search expression for\n        the currently active embedding collection\n    2. do similarity search in the embedding collection\n    3. combine metadata and embeddings\n\n    Args:\n        query (str): query string\n\n        k (int): the number of results to return\n\n        doc_ids(List[str] optional): the list of document ids, do similarity search across the\n        specified documents\n\n    Returns:\n        List[Document]: search results\n    \"\"\"\n    result_metadata = []\n    expr = VectorDatabaseAgentMilvus._build_meta_col_query_expr_for_all_documents(\n        doc_ids\n    )\n    result_metadata = self._col_metadata.query(\n        expr=expr, output_fields=METADATA_FIELDS\n    )\n    expr = self._build_embedding_search_expression(result_metadata)\n    result_embedding = self._col_embeddings.similarity_search(\n        query=query, k=k, expr=expr\n    )\n    return self._join_embedding_and_metadata_results(\n        result_embedding, result_metadata\n    )\n</code></pre>"},{"location":"vectorstore_agent-reference/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus.store_embeddings","title":"<code>store_embeddings(documents)</code>","text":"<p>Store documents in the currently active database.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Documents]</code> <p>documents array, usually from DocumentReader.load_document, DocumentReader.document_from_pdf, DocumentReader.document_from_txt</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>document id</p> Source code in <code>biochatter/vectorstore_agent.py</code> <pre><code>def store_embeddings(self, documents: List[Document]) -&gt; str:\n    \"\"\"\n    Store documents in the currently active database.\n\n    Args:\n        documents (List[Documents]): documents array, usually from\n            DocumentReader.load_document, DocumentReader.document_from_pdf,\n            DocumentReader.document_from_txt\n\n    Returns:\n        str: document id\n    \"\"\"\n    if len(documents) == 0:\n        return\n    return self._insert_data(documents)\n</code></pre>"},{"location":"vectorstore_agent-reference/#biochatter.vectorstore_agent.align_embeddings","title":"<code>align_embeddings(docs, meta_id)</code>","text":"<p>Ensure that the metadata id is present in each document.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>List of documents</p> required <code>meta_id</code> <code>int</code> <p>Metadata id to assign to the documents</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: List of documents, with each document having a metadata id.</p> Source code in <code>biochatter/vectorstore_agent.py</code> <pre><code>def align_embeddings(docs: List[Document], meta_id: int) -&gt; List[Document]:\n    \"\"\"\n    Ensure that the metadata id is present in each document.\n\n    Args:\n        docs (List[Document]): List of documents\n\n        meta_id (int): Metadata id to assign to the documents\n\n    Returns:\n        List[Document]: List of documents, with each document having a metadata\n            id.\n    \"\"\"\n    ret = []\n    for doc in docs:\n        ret.append(\n            Document(\n                page_content=doc.page_content,\n                metadata={\"meta_id\": meta_id},\n            )\n        )\n    return ret\n</code></pre>"},{"location":"vectorstore_agent-reference/#biochatter.vectorstore_agent.align_metadata","title":"<code>align_metadata(metadata, isDeleted=False)</code>","text":"<p>Ensure that specific metadata fields are present; if not provided, fill with \"unknown\". Also, add a random vector to each metadata item to simulate an embedding.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>List[Dict]</code> <p>List of metadata items</p> required <code>isDeleted</code> <code>Optional[bool]</code> <p>Whether the document is deleted. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[List]</code> <p>List[List]: List of metadata items, with each item being a list of metadata fields.</p> Source code in <code>biochatter/vectorstore_agent.py</code> <pre><code>def align_metadata(\n    metadata: List[Dict], isDeleted: Optional[bool] = False\n) -&gt; List[List]:\n    \"\"\"\n\n    Ensure that specific metadata fields are present; if not provided, fill with\n    \"unknown\". Also, add a random vector to each metadata item to simulate an\n    embedding.\n\n    Args:\n        metadata (List[Dict]): List of metadata items\n\n        isDeleted (Optional[bool], optional): Whether the document is deleted.\n            Defaults to False.\n\n    Returns:\n        List[List]: List of metadata items, with each item being a list of\n            metadata fields.\n    \"\"\"\n    ret = []\n    fields = METADATA_FIELDS.copy()\n    fields.pop(0)\n    for ix, k in enumerate(fields):\n        ret.append([item[k] if k in item else \"unknown\" for item in metadata])\n\n    ret.append(\n        [\n            [random.random() for _ in range(METADATA_VECTOR_DIM)]\n            for _ in range(len(metadata))\n        ]\n    )\n    ret.append([isDeleted for _ in metadata])\n    return ret\n</code></pre>"},{"location":"vignette-kg/","title":"Vignette: Knowledge Graph RAG","text":"<p>This vignette demonstrates the KG module of BioChatter as used by the BioChatter Next application. We connect to a BioCypher knowledge graph (KG) to retrieve relevant information for a given question. We then use the retrieved information to generate a response to the question. The application can connect to any real-world BioCypher KG by providing the connection details in the <code>KG Settings</code> dialog.</p>"},{"location":"vignette-kg/#background","title":"Background","text":"<p>For the demonstration purposes of this vignette, we include a demo KG based on an open-source dataset of crime statistics in Manchester, because it allows us to redistribute the KG due to its small size and public domain licence, and because it is easily understood. This is non-trivial for most biomedical datasets; however, we are currently working on a synthetic biomedical example to extend this vignette. This is the schema of the demo KG:</p> <pre><code>graph LR;\n    Person(:Person) -- KNOWS --&gt; Person\n    Person -- FAMILY_REL --&gt; Person\n    Person -- LIVES_AT --&gt; Location(:Location)\n    Person -- PARTY_TO --&gt; Crime(:Crime)\n    Person -- MADE_CALL --&gt; PhoneCall(:PhoneCall)\n    Person -- RECEIVED_CALL --&gt; PhoneCall\n    Crime -- INVESTIGATED_BY --&gt; Officer(:Officer)\n    Crime -- OCCURRED_AT --&gt; Location\n    Object(:Object) -- INVOLVED_IN --&gt; Crime</code></pre> <p>The KG is adapted from a Neo4j tutorial, and is available as a BioCypher adapter including a BioChatter Light integration here. We also include it in an optional BioChatter Next Docker Compose configuration to allow trying it out locally.</p>"},{"location":"vignette-kg/#usage","title":"Usage","text":"<p>In BioChatter Next, we first activate the KG functionality by clicking on the <code>KG Settings</code> button in the sidebar. In the settings dialog, we can activate the KG functionality and select how many results we want to retrieve. Returning to the conversation and enabling the KG functionality for the current chat (directly above the send button), we can then ask the model about the KG. The language model we use is, as in the RAG vignette, <code>gpt-3.5-turbo-0613</code>. The conversation is pasted below for convenience, including the queries generated by BioChatter.</p> <p></p> <p>In the background, the RagAgent module of BioChatter receives the question and generates a query to retrieve the desired information. This is then passed back to the primary model, which includes it in its answer generation.</p>"},{"location":"vignette-kg/#conclusion","title":"Conclusion","text":"<p>The native integration of BioCypher KGs into the BioChatter framework allows for a seamless integration of KGs into the conversational AI. This in turn facilitates knowledge accessibility in a wide range of application domains.</p>"},{"location":"vignette-kg/#note-the-apparent-inability-of-gpt-to-understand-certain-directionalities-and-how-biochatter-compensates-for-this","title":"Note: the apparent inability of GPT to understand certain directionalities, and how BioChatter compensates for this","text":"<p>Interestingly, while <code>gpt-3.5-turbo-0613</code> mostly does a formidable job at translating natural language questions into Cypher queries, it is remarkably obtuse in certain instances. For instance, for the relationship <code>INVESTIGATED_BY</code>, which connects a <code>Crime</code> to an <code>Officer</code>, GPT consistently fails to understand that the relationship implies that the <code>Officer</code> is the one who investigates the <code>Crime</code>. Instead, it consistently interprets the relationship as if the <code>Crime</code> investigates the <code>Officer</code>: it consistently proposes the query <code>MATCH (o:Officer)-[:INVESTIGATED_BY]-&gt;(c:Crime) RETURN c, o</code> instead of the correct <code>MATCH (c:Crime)-[:INVESTIGATED_BY]-&gt;(o:Officer) RETURN c, o</code>. We were not able to change this behaviour with any contextual prompt instructions.</p> <p>For this reason, the BioChatter <code>prompts.py</code> module uses the knowledge we have about the directionality of edges in the BioCypher KG to only propose options for patterns that actually exist in the KG. In the instance of <code>INVESTIGATED_BY</code>, this is the corresponding YAML definition in BioCypher:</p> <pre><code>investigated by:\n    is_a: [fact, core]\n    represented_as: edge\n    label_as_edge: INVESTIGATED_BY\n    input_label: INVESTIGATED_BY\n    source: crime\n    target: officer\n</code></pre> <p>The presence of the <code>source</code> and <code>target</code> annotations allows us to provide only the correct options to the LLM, which in turn allows the LLM to generate the correct query.</p> <p>Strikingly, this even applies to <code>GPT-4</code> and <code>ChatGPT</code>, which we used to generate the Mermaid diagram above. The model immediately proposed a correct Mermaid diagram for most of the entire schema definition YAML file, except for the relationship between <code>Crime</code> and <code>Officer</code>, which it reversed. Via the YAML we provide in the prompt, GPT has access to the correct directionality, but it still fails to understand it.</p> <pre><code>graph LR;\n    Person(:Person) -- KNOWS --&gt; Person\n    Person -- FAMILY_REL --&gt; Person\n    Person -- LIVES_AT --&gt; Location(:Location)\n    Person -- PARTY_TO --&gt; Crime(:Crime)\n    Person -- MADE_CALL --&gt; PhoneCall(:PhoneCall)\n    Person -- RECEIVED_CALL --&gt; PhoneCall\n    Officer(:Officer) -- INVESTIGATED_BY --&gt; Crime\n    Crime -- OCCURRED_AT --&gt; Location\n    Object(:Object) -- INVOLVED_IN --&gt; Crime\n\n    classDef node fill:#f9f,stroke:#333,stroke-width:2px;\n    class Person,Officer,Location,Crime,PhoneCall,Object node;\n</code></pre> <p>Here is the full chat for reference.</p>"},{"location":"vignette-kg/#conversation","title":"Conversation","text":"<p>The boxes below the BioChatter responses in the conversation show the queries generated by the RagAgent module.</p>"},{"location":"vignette-kg/#message-from-you","title":"Message From You","text":"<p>How many crimes happened in total?</p>"},{"location":"vignette-kg/#message-from-biochatter","title":"Message From BioChatter","text":"<p>According to the provided statement, the total number of crimes is 28,762. This information suggests that there have been 28,762 reported crimes in the given context. However, without additional information or context, it is difficult to determine the specific time period, location, or nature of these crimes. It's important to note that crime statistics can vary depending on the source, methodology, and reporting practices. Therefore, it is always advisable to consult official crime reports or local law enforcement agencies for the most accurate and up-to-date information on crime statistics.</p> <pre><code>MATCH (c:Crime)\nRETURN count(c) AS numberOfCrimes\n</code></pre>"},{"location":"vignette-kg/#message-from-you_1","title":"Message From You","text":"<p>Where happened most crimes?</p>"},{"location":"vignette-kg/#message-from-biochatter_1","title":"Message From BioChatter","text":"<p>Based on the provided statement, it appears that the location with the highest number of crimes is a parking area, with a total of 811 reported crimes. However, without additional context or information, it is difficult to determine the consistency or inconsistency of this statement with other available information. It is important to note that crime rates and locations can vary significantly depending on various factors such as the region, city, neighborhood, and specific time period being considered. To provide a more comprehensive analysis, it would be helpful to have access to a broader dataset or additional information about the specific location and its crime statistics.</p> <pre><code>MATCH (c:Crime)-[:OCCURRED_AT]-&gt;(l:Location)\nRETURN l.address AS location, COUNT(c) AS crimeCount\nORDER BY crimeCount DESC\n</code></pre>"},{"location":"vignette-kg/#message-from-you_2","title":"Message From You","text":"<p>Who were the top 3 offenders?</p>"},{"location":"vignette-kg/#message-from-biochatter_2","title":"Message From BioChatter","text":"<p>Based on the provided information, the top three offenders are:</p> <ol> <li> <p>Phillip Williamson: According to the information given, Phillip Williamson has committed 5 crimes.</p> </li> <li> <p>Jessica Kelly: The information states that Jessica Kelly has also committed 5 crimes.</p> </li> <li> <p>Brian Morales: The data indicates that Brian Morales has committed 4 crimes.</p> </li> </ol> <p>These three individuals, Phillip Williamson, Jessica Kelly, and Brian Morales, are the top offenders based on the number of crimes they have committed.</p> <pre><code>MATCH (p:Person)-[:PARTY_TO]-&gt;(c:Crime)\nRETURN p.name, p.surname, COUNT(c) AS num_crimes\nORDER BY num_crimes DESC\nLIMIT 3\n</code></pre>"},{"location":"vignette-rag/","title":"Vignette: Retrieval-Augmented Generation (RAG)","text":"<p>This vignette demonstrates the RAG module of BioChatter as used by the BioChatter Next application. This basic use case involves an LLM manuscript reading assistant and a vector database with embedded scientific manuscripts. The manuscripts are recent reviews on the topic of ABC transporters (active transport proteins that often serve as efflux pumps in many tissues, with a major function of clearing exogenous substances) in drug resistance phenomena, which we currently investigate in our DECIDER cohort.  Manuscripts such as these can be absent from the knowledge base of LLMs for various reasons, such as their recency or the fact that they are not open access. RAG opens up the possibility to retrieve relevant information from these manuscripts, and to inject it into the LLM's generation process.</p>"},{"location":"vignette-rag/#usage","title":"Usage","text":"<p>In BioChatter Next, we first activate the RAG functionality by clicking on the <code>RAG Settings</code> button in the sidebar. In the settings dialog, we can activate the functionality and upload an arbitrary number of documents, which is only limited by the scale of the vector database system. In this case, and for demonstration purposes, we uploaded the four manuscripts, which leads to the state shown in the screenshot below. You can find the citations below.</p> <p></p> <p>Note that we chose to split the documents into fragments of 1000 characters each. We could have split by tokens instead, or changed the fragment length and overlap. Fragment length, overlap, and splitting by tokens or characters are decisions that should be made in tandem with the choice of LLM model, and with the number of fragments that should be retrieved for each query. Most importantly, the total text length should not exceed the input context length of the model. Here, we choose to inject 10 fragments per query.</p> <p>We can now start a new conversation (using the <code>New Persona</code> button in the sidebar), for which we select the <code>Research manuscript helper (RAG)</code> persona, which includes suitable contextual prompts. Upon activating the RAG in the chat (to signal that we wish to perform RAG for our question), we can enter into a dialogue with the assistant. We use <code>gpt-3.5-turbo-0613</code> in this example, which is a very affordable model. The procedure is demonstrated in the GIF below.</p> <p></p>"},{"location":"vignette-rag/#comparison-with-chatgpt","title":"Comparison with ChatGPT","text":"<p>We can ask the same question to ChatGPT 4 (only subscription access).  By employing web search, ChatGPT 4 is able to find the same study that was the RAG result we were asked about in our follow-up question (Xu et al. 2015); however, the answer is not as detailed as the answer provided by the RAG process, and includes fewer mentions of explicit studies. While it includes sensible information, there is no way of validating most of the claims made by the model. You can read the full answer here. Overall, the quality of ChatGPT 4 is comparable to the RAG output, but the user has less control over the knowledge used, and the answer cannot include information that is not easily found on the web (for instance due to access restrictions). Of note, we used <code>gpt-3.5-turbo-0613</code> in this example, which is less recent and much less expensive than GPT-4.</p> <p>Asking the naive ChatGPT 3.5 (free access), the same question yields a very different answer. The free version of ChatGPT cannot search the internet, and does not provide any citations for the general answer it provides. You can read the full answer here. For convenience, all three conversations are copied below. </p>"},{"location":"vignette-rag/#conclusion","title":"Conclusion","text":"<p>In conclusion, the RAG procedure we implement is able to dramatically improve the quality of GPT-3 answers, making them comparable to GPT-4 with web search, and even surpassing it in some aspects. We invite you to try it out yourself, as we provide an instance of BioChatter Next, including GPT access and a vector database instance, free of charge at https://next.biochatter.org.</p>"},{"location":"vignette-rag/#conversations","title":"Conversations","text":""},{"location":"vignette-rag/#biochatter-rag","title":"BioChatter RAG","text":""},{"location":"vignette-rag/#contextual-prompt","title":"Contextual Prompt","text":"<p>You are an assistant to a biomedical researcher.</p>"},{"location":"vignette-rag/#contextual-prompt_1","title":"Contextual Prompt","text":"<p>You have access to text fragments from one or multiple scientific manuscripts, which were retrieved by their semantic similarity to the user's question.</p>"},{"location":"vignette-rag/#contextual-prompt_2","title":"Contextual Prompt","text":"<p>Your task is to help the user understand the manuscripts and their relevance to their questions. Be concise in your explanations, provide quotes from the manuscripts where appropriate, and be open about limitations in cases where the text does not contain pertinent information.</p>"},{"location":"vignette-rag/#message-from-you","title":"Message From You","text":"<p>What is known about the CFTR protein in resistance to ovarian cancer?</p>"},{"location":"vignette-rag/#message-from-biochatter","title":"Message From BioChatter","text":"<p>Based on the provided information, there are several studies that suggest a potential role of the CFTR (Cystic Fibrosis Transmembrane Conductance Regulator) protein in resistance to ovarian cancer. Here are some key findings from the referenced manuscripts:</p> <ol> <li> <p>Gene expression profiling of ovarian tumors before and after chemotherapy revealed molecular signatures of chemoresistant tumors (M, et al., 2006). This study did not specifically mention CFTR, but it provides insights into the molecular changes associated with chemoresistance.</p> </li> <li> <p>ABCF2 expression, which is related to CFTR, was investigated in clear cell adenocarcinoma of the ovary. The study found a relationship between ABCF2 expression and response to chemotherapy or prognosis (Tsuda H, et al., 2010).</p> </li> <li> <p>Alterations in gene copy numbers, including CFTR, were identified as a mechanism for acquired drug resistance in cancer (Yasui K, et al., 2004).</p> </li> <li> <p>ABCF2, an Nrf2 target gene related to CFTR, was found to contribute to cisplatin resistance in ovarian cancer cells (Bao L, et al., 2017).</p> </li> <li> <p>CFTR was not specifically mentioned in the context of ovarian cancer, but a study on hepatocellular carcinoma found that silencing circ-TCF4.85 inhibited cancer progression through microRNA-486-5p-targeted inhibition of ABCF2 (Gao J, et al., 2020).</p> </li> <li> <p>Constitutive activation of nuclear factor \u03baB (NF-\u03baB) was associated with CFTR expression and promoted cancer progression and poor prognosis in cervical cancer (Wu Z, et al., 2013).</p> </li> <li> <p>High levels of CFTR expression were associated with tumor aggression in ovarian cancer, and knockdown of CFTR suppressed ovarian cancer cell proliferation (Xu J, et al., 2015).</p> </li> </ol> <p>It is important to note that while these studies suggest a potential association between CFTR and resistance to ovarian cancer, the exact mechanisms and clinical implications are still being investigated. Further research is needed to fully understand the role of CFTR in ovarian cancer resistance.</p>"},{"location":"vignette-rag/#message-from-you_1","title":"Message From You","text":"<p>Tell me more about the studies of aggression and knockdown of CFTR in vivo.</p>"},{"location":"vignette-rag/#message-from-biochatter_1","title":"Message From BioChatter","text":"<p>Based on the provided information, there are two relevant studies regarding the aggression and knockdown of CFTR in ovarian cancer:</p> <ol> <li> <p>The study by Wu et al. (2013) investigated the constitutive activation of nuclear factor \u03baB (NF-\u03baB) and its contribution to CFTR expression, cancer progression, and poor prognosis in cervical cancer. Although this study is not specific to ovarian cancer, it provides insights into the role of CFTR and NF-\u03baB in promoting cancer progression.</p> </li> <li> <p>The study by Xu et al. (2015) focused specifically on ovarian cancer and found that high levels of CFTR expression were associated with tumor aggression. They also demonstrated that knockdown of CFTR suppressed the proliferation of ovarian cancer cells in vitro and in vivo. This study suggests that CFTR may play a role in promoting ovarian cancer growth.</p> </li> </ol> <p>These studies provide evidence for the involvement of CFTR in cancer aggression and suggest that targeting CFTR could be a potential therapeutic strategy. However, it is important to note that these studies focus on different types of cancer (cervical and ovarian), and the specific mechanisms underlying CFTR's role in ovarian cancer aggression and its knockdown effects are not fully elucidated.</p> <p>It is worth mentioning that the other references provided in the text fragments are not directly related to the aggression and knockdown of CFTR in ovarian cancer. They discuss topics such as the structure and function of ABC transporters, drug resistance, and inflammation in different contexts.</p>"},{"location":"vignette-rag/#chatgpt-4","title":"ChatGPT 4","text":""},{"location":"vignette-rag/#anonymous","title":"Anonymous","text":"<p>What is known about the CFTR protein in resistance to ovarian cancer?</p>"},{"location":"vignette-rag/#chatgpt","title":"ChatGPT","text":"<p>The CFTR protein, known as the cystic fibrosis transmembrane conductance regulator, plays a significant role in ovarian cancer. Research has shown that CFTR is expressed more in ovarian cancer tissues compared to benign ovarian tumors and normal ovaries. This overexpression of CFTR is associated with more advanced stages of ovarian cancer, poor histopathological grade, and higher serum Ca-125 levels, which are all markers of more aggressive disease. Interestingly, the serous type of ovarian cancer shows stronger CFTR staining compared to other types.</p> <p>Moreover, the role of CFTR extends beyond its traditional function as an ion channel. It interacts with various intracellular signaling pathways that are crucial in cancer progression. For example, in ovarian cancer, CFTR overexpression is linked to interactions with the c-Src signaling pathway, which is involved in cellular growth.</p> <p>The implications of CFTR expression in ovarian cancer are significant. In studies where CFTR was downregulated, there was a decrease in cell invasion, migration, proliferation, adhesion, and colony formation in vitro. In vivo experiments also showed that the tumorigenic abilities of ovarian cancer cells were significantly repressed when CFTR expression was reduced.</p> <p>These findings highlight the complex role of CFTR in ovarian cancer and suggest that it could be a potential target for therapeutic intervention. However, it is important to note that the role of CFTR in cancer is multifaceted and varies among different types of cancers. Therefore, a detailed understanding of its function in specific cancer types, like ovarian cancer, is crucial for developing targeted treatments\u200b\u200b\u200b\u200b (https://pubmed.ncbi.nlm.nih.gov/25738998/, https://jmhg.springeropen.com/articles/10.1186/s43042-022-00308-7).</p>"},{"location":"vignette-rag/#chatgpt-35","title":"ChatGPT 3.5","text":""},{"location":"vignette-rag/#anonymous_1","title":"Anonymous","text":"<p>What is known about the CFTR protein in resistance to ovarian cancer?</p>"},{"location":"vignette-rag/#chatgpt_1","title":"ChatGPT","text":"<p>As of my last knowledge update in January 2022, there is limited direct information about the CFTR (Cystic Fibrosis Transmembrane Conductance Regulator) protein's role in resistance to ovarian cancer. CFTR is primarily associated with cystic fibrosis, a genetic disorder that affects the lungs and other organs. However, some research suggests potential links between CFTR and cancer, including ovarian cancer, although the mechanisms are not fully understood.</p> <p>Here are some key points to consider:</p> <p>CFTR Expression: Studies have shown that CFTR is expressed in various tissues, including the female reproductive system. Abnormal CFTR expression or function could potentially impact the development or progression of ovarian cancer. Research is ongoing to understand the role of CFTR in cancer biology. CFTR Mutations: Certain mutations in the CFTR gene have been associated with an increased risk of cancer, although the evidence for ovarian cancer specifically is limited. Mutations in CFTR are more commonly known for their role in cystic fibrosis. Individuals with cystic fibrosis have an increased risk of various cancers, including gastrointestinal cancers, but the link to ovarian cancer is not well-established. Ion Transport and Cancer: CFTR is involved in ion transport across cell membranes, and this function could potentially influence cellular processes relevant to cancer. Changes in ion transport can affect cell proliferation, apoptosis, and other cellular functions that are important in cancer development and progression. Potential Therapeutic Target: Some research suggests that modulating CFTR activity or expression could be a potential target for cancer therapy, including ovarian cancer. However, more research is needed to determine the feasibility and effectiveness of such approaches. It's important to note that the understanding of the role of CFTR in cancer, including ovarian cancer, is an evolving field, and new research findings may have emerged since my last update in January 2022. To get the most up-to-date information on this topic, I recommend consulting recent scientific literature and speaking with medical experts or researchers actively working in the field of ovarian cancer and CFTR biology.</p>"},{"location":"vignette-rag/#references","title":"References","text":"<ul> <li> <p>[1] ABC transporters affects tumor immune microenvironment to regulate cancer immunotherapy and multidrug resistance</p> </li> <li> <p>[2] ABC transporters and the hallmarks of cancer: roles in cancer aggressiveness beyond multidrug resistance</p> </li> <li> <p>[3] Advances in the structure, mechanism and targeting of chemoresistance-linked ABC transporters</p> </li> <li> <p>[4] ATP-binding cassette (ABC) transporters in cancer: A review of recent updates</p> </li> </ul>"},{"location":"wasm/","title":"LLM in your Browser - WebAssembly","text":"<p>Coming soon.</p>"}]}