{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Generative AI models have shown tremendous usefulness in increasing accessibility and automation of a wide range of tasks. Yet, their application to the biomedical domain is still limited, in part due to the lack of a common framework for deploying, testing, and evaluating the diverse models and auxiliary technologies that are needed. <code>biochatter</code> is a Python package implementing a generic backend library for the connection of biomedical applications to conversational AI. We describe the framework in this preprint; for a more hands-on experience, check out our two web app implementations:</p> <ul> <li> <p> BioChatter Light</p> <p>Agile framework in pure Python built with Streamlit, for fast prototyping and iteration.</p> <p> Go To BioChatter Light</p> </li> <li> <p> BioChatter Next</p> <p>Advanced client-server architecture based on Flask and Next.js.</p> <p> Go To BioChatter Next</p> </li> </ul> <p>BioChatter is part of the BioCypher ecosystem, connecting natively to BioCypher knowledge graphs. The BioChatter paper is being written here and the current version can be read here.</p> <p></p> <p>Hot Topics</p> <p>BioChatter natively extends BioCypher knowledge graphs. Check there for more information.</p> <p>We have also recently published a perspective on connecting knowledge and machine learning to enable causal reasoning in biomedicine, with a particular focus on the currently emerging \"foundation models.\" You can read it here.</p>"},{"location":"#installation","title":"Installation","text":"<p>To use the package, install it from PyPI, for instance using pip (<code>pip install biochatter</code>) or Poetry (<code>poetry add biochatter</code>).</p>"},{"location":"#extras","title":"Extras","text":"<p>The package has some optional dependencies that can be installed using the following extras (e.g. <code>pip install biochatter[xinference]</code>):</p> <ul> <li> <p><code>xinference</code>: support for querying open-source LLMs through Xorbits Inference</p> </li> <li> <p><code>ollama</code>: support for querying open-source LLMs through Ollama</p> </li> <li> <p><code>podcast</code>: support for podcast text-to-speech (for the free Google TTS; the paid OpenAI TTS can be used without this extra)</p> </li> <li> <p><code>streamlit</code>: support for streamlit UI functions (used in BioChatter Light)</p> </li> </ul> <p> </p>"},{"location":"#documentation-and-tutorials","title":"Documentation and Tutorials","text":"<p>For a description of the features of the framework, see the <code>Features</code> option in the main menu. For examples of usage and customisation, check out the  <code>Vignettes</code> section. For a more detailed reference of functions and classes, see the <code>API Reference</code>. </p>"},{"location":"api/","title":"API Calling","text":""},{"location":"api/#overview","title":"Overview","text":"<p>BioChatter provides access to biological databases through chat via the API Agent. It is designed to interact with various external APIs and provides a structured approach to generating queries, fetching results, and interpreting the responses from different API services.</p>"},{"location":"api/#example-oncokb-integration","title":"Example: OncoKB Integration","text":"<p>The OncoKB API integration allows users to query oncologically relevant genomic information. Here's a minimal working example:</p> <pre><code>from biochatter.llm_connect import GptConversation\nfrom biochatter.api_agent.api_agent import APIAgent\nfrom biochatter.api_agent.oncokb import OncoKBQueryBuilder, OncoKBFetcher, OncoKBInterpreter\n\n# Set up a conversation factory (you might need to adjust this based on your setup)\ndef conversation_factory():\n    return GptConversation(model_name=\"gpt-4\", prompts={}, correct=False)\n\n# Create an API agent for OncoKB\noncokb_agent = APIAgent(\n    conversation_factory=conversation_factory,\n    query_builder=OncoKBQueryBuilder(),\n    fetcher=OncoKBFetcher(),\n    interpreter=OncoKBInterpreter()\n)\n\n# Execute a query\nquestion = \"What is the oncogenic potential of BRAF V600E mutation?\"\nresult = oncokb_agent.execute(question)\n\nprint(result)\n</code></pre> <p>This example demonstrates how to set up and use the OncoKB API agent to query genomic information. The execute method handles the entire process of query generation, API interaction, and result interpretation.</p>"},{"location":"api/#core-components","title":"Core components","text":""},{"location":"api/#api-agent","title":"API Agent","text":"<p>The main class that orchestrates the API interaction process.</p> <p>Key Methods:</p> <ul> <li> <p><code>execute(question: str) -&gt; Optional[str]</code>: Executes the full API interaction pipeline.</p> </li> <li> <p><code>parameterise_query(question: str) -&gt; Optional[BaseModel]</code>: Generates a structured query based on the input question.</p> </li> <li> <p><code>fetch_results(query_model: \"BaseModel\") -&gt; Optional[str]</code>: Retrieves results from the API.</p> </li> <li> <p><code>summarise_results(question: str, response_text: str) -&gt; Optional[str]</code>: Interprets and summarizes the API response with respect to the question.</p> </li> </ul>"},{"location":"api/#queryparameters","title":"QueryParameters","text":"<p>The <code>QueryParameters</code> class is a Pydantic model that defines the structure and validation rules for API query parameters. It serves several important purposes:</p> <ul> <li> <p>Structure Definition: It clearly defines all the parameters required for an API call, including their types and default values.</p> </li> <li> <p>Validation: Pydantic automatically validates the data, ensuring that all required fields are present and that the data types are correct.</p> </li> <li> <p>Documentation: Each field can include a description, providing clear documentation for developers and assisting the LLM in understanding how to use each parameter.</p> </li> <li> <p>Type Hinting: It provides type hints, improving code readability and enabling better IDE support.</p> </li> <li> <p>Serialization: The Pydantic model can easily be serialized to and deserialized from JSON, facilitating data transfer.</p> </li> </ul>"},{"location":"api/#basequerybuilder","title":"BaseQueryBuilder","text":"<p>Abstract base class for creating query builders specific to different APIs.</p> <p>Key Methods:</p> <ul> <li> <p><code>create_runnable(query_parameters: BaseModel, conversation: Conversation) -&gt; Callable</code>: Creates runnable object for executing queries.</p> </li> <li> <p><code>parameterise_query(question: str, conversation: Conversation) -&gt; BaseModel</code>: Generates a parameterized query object based on the input question.</p> </li> </ul>"},{"location":"api/#basefetcher","title":"BaseFetcher","text":"<p>An abstract base class for implementing API-specific data fetching logic.</p> <p>Key Method:</p> <ul> <li><code>fetch_results(query_model)</code>: Submits the query to the API and retrieves the results.</li> </ul>"},{"location":"api/#baseinterpreter","title":"BaseInterpreter","text":"<p>An abstract base class for implementing API-specific response interpretation logic.</p> <p>Key Method:</p> <ul> <li><code>summarise_results(question: str, conversation_factory: Callable, response_text: str) -&gt; str</code>: Summarizes and interprets the API response.</li> </ul>"},{"location":"api/#setting-up-a-new-api-integration","title":"Setting up a new API integration","text":""},{"location":"api/#overview_1","title":"Overview","text":"<p>Create specific implementations of QueryParameters, BaseQueryBuilder, BaseFetcher, and BaseInterpreter for your target API and design prompts. Instantiate an APIAgent with these components.  Use the execute method of APIAgent to process questions and interact with the API.</p>"},{"location":"api/#1-specific-queryparameters","title":"1. Specific QueryParameters","text":"<p>For each field in your API call a Pydantic field is created. The description must be clear and concise to be understandable by a LLM so that it will fill the field with the appropriate arguments.</p> <pre><code>from pydantic import BaseModel, Field\n\nclass NewAPIQueryParameters(BaseModel):\n    endpoint: str = Field(..., description=\"API endpoint\")\n    param1: str = Field(..., description=\"Description of param1\")\n    param2: int = Field(default=0, description=\"Description of param2\")\n    # Add more parameters as needed\n</code></pre>"},{"location":"api/#2-prompt-design","title":"2. Prompt design","text":""},{"location":"api/#query_prompt-instructions-for-structured-output-to-write-newapi-call","title":"QUERY_PROMPT: instructions for structured output to write NewAPI call.","text":"<p>Create a prompt that will allow the LLM to obtain sufficient context to fill the Fields in the NewAPIQueryParameters class. Always keep the first two sentences. Secondly, provide instructions on how to interpret questions. Finally, for LLMs with large context windows the entire API documentation can be copy pasted inside of the prompt. Examples of API calls to retrieve the relevant information to a question should also be provided. </p> <pre><code>NewAPI_QUERY_PROMPT = \"\"\"\nYou are a world class algorithm for creating queries in structured formats. Your task is to use OncoKB Web APIs to answer genomic questions.\n\nAPI DOCUMENTATION AND EXAMPLES\"\"\"\n</code></pre>"},{"location":"api/#summary_prompt-instructions-on-how-to-interpret-and-answer-user-questions-based-on-retrieved-information","title":"SUMMARY_PROMPT: Instructions on how to interpret and answer user questions based on retrieved information","text":"<p>Follow prompt design below, replace NewAPI accordingly. If results are not satisfactory, engineer the prompts further.</p> <pre><code>NewAPI_SUMMARY_PROMPT = \"\"\"\nYou have to answer this question in a clear and concise manner: {question} Be factual!\\n\\\nYou are a world leading NewAPI domain specialist who knows everything about NewAPI results.\\n\\\nDo not make up information, only use the provided information and mention how relevant the found information is based on your knowledge about NewAPI\\n\\\nHere is the information relevant to the question found on NewAPI:\\n\\\n{context}\n\"\"\"\n</code></pre>"},{"location":"api/#3-implement-querybuilder","title":"3. Implement QueryBuilder","text":"<p>Create a class that inherits from BaseQueryBuilder:</p> <pre><code>from biochatter.api_agent.abc import BaseQueryBuilder\n\nclass NewAPIQueryBuilder(BaseQueryBuilder):\n    def create_runnable(self, \n        query_parameters: NewAPIQueryParameters, \n        conversation: Conversation\n        ) -&gt; Callable:\n        # Implement method to create a runnable query object\n        return create_structured_output_runnable(\n            output_schema=query_parameters,\n            llm=conversation.chat,\n            prompt=self.structured_output_prompt,\n        )\n    def parameterise_query(self, \n        question: str, \n        conversation: Conversation\n        ) -&gt; NewAPIQueryParameters:\n        # Implement method to generate API parameters from a question\n        runnable = self.create_runnable(\n            query_parameters=NewAPIQueryParameters,\n            conversation=conversation,\n        )\n        NewAPI_call_obj = runnable.invoke(\n            {\"input\": f\"Answer:\\n{question} based on:\\n {NewAPI_QUERY_PROMPT}\"}\n        )\n       NewAPI_call_obj.question_uuid = str(uuid.uuid4())\n        return NewAPI_call_obj\n</code></pre>"},{"location":"api/#4-implement-fetcher","title":"4. Implement Fetcher","text":"<p>Create a class that inherits from BaseFetcher. Adapt request header in initiation if specific API tokens are required, and implement a logic to build the full URL required for the API call from NewAPIQueryParameters, and execute it.</p> <p>The <code>fetch_results</code> method should return the response text from the API. If this is a two-stage process (e.g., submitting a query and then retrieving the results), you should implement the necessary logic to handle this. You can look at the BLAST fetcher for an example (<code>blast.py</code>).</p> <p>NOTE: if the response is too large for your LLM context window you may have to reduce its size in some way.</p> <pre><code>from biochatter.api_agent.abc import BaseFetcher\n\nclass NewAPIFetcher(BaseFetcher):\n        def __init__(self,):\n        self.headers = {\n        }\n        self.base_url = \"https://api.new.org/api/\"\n\n    def fetch_results(self, \n    query_model: NewAPIQueryParameters\n    ) -&gt; str:\n    #implement your logic here \n    return results_response.text\n</code></pre>"},{"location":"api/#5-implement-interpreter","title":"5. Implement Interpreter","text":"<p>Create a class that inherits from BaseInterpreter and adapt the system prompt to NewAPI.</p> <pre><code>from biochatter.api_agent.abc import BaseInterpreter\n\nclass NewAPIInterpreter(BaseInterpreter):\n    def summarise_results(self,\n        question: str, \n        conversation_factory: Callable, \n        response_text: str) -&gt; str:\n\n        prompt = ChatPromptTemplate.from_messages(\n            [\n                (\n                    \"system\",\n                    \"You are a world NewAPI specialist who knows everything about NewAPI topic and its results.\",\n                ),\n                (\"user\", \"{input}\"),\n            ]\n        )\n        summary_prompt = NewAPI_SUMMARY_PROMPT.format(\n            question=question, context=response_text\n        )\n        output_parser = StrOutputParser()\n        conversation = conversation_factory()\n        chain = prompt | conversation.chat | output_parser\n        answer = chain.invoke({\"input\": {summary_prompt}})\n        return answer\n</code></pre>"},{"location":"api/#6-integrate-with-apiagent","title":"6. Integrate with APIAgent","text":"<p>Once you have implemented all components, you can use them with the APIAgent:</p> <pre><code>from biochatter.api_agent.api_agent import APIAgent\n\nnew_api_agent = APIAgent(\n    conversation_factory=your_conversation_factory,\n    query_builder=NewAPIQueryBuilder(),\n    fetcher=NewAPIFetcher(),\n    interpreter=NewAPIInterpreter()\n)\n\nresult = new_api_agent.execute(\"Your question here\")\n</code></pre>"},{"location":"api/#7-integrate-with-ragagent","title":"7. Integrate with RagAgent","text":"<p>TO DO </p>"},{"location":"benchmark-developer/","title":"Benchmark - Developer Notes","text":"<p>To understand the benchmarking procedure, you should be familiar with Pytest. The benchmark test matrix is executed via Pytest fixtures that iterate through the combinations of test parameters such as model name and size.  This basic setup happens in the <code>conftest.py</code> file in the <code>benchmark</code> directory.  The benchmark Pytest setup is distinct from the Pytest setup we use for our continuous integration (in the <code>test</code> folder).</p> <p>You can imagine the benchmark as a set of unit tests, with the only difference being that the test subject is not our codebase, but the behaviour and performance of the various LLMs, prompts, etc. These tests are defined in two parts: the data and the method. Data are collected in a set of YAML files in the <code>benchmark/data</code> directory, while the benchmark methods are implemented in the Pytest functions in the individual Python modules (file names starting with <code>test_</code>). We also have some Python modules for auxiliary functions, such as <code>load_dataset.py</code>.</p> <p>In the following, we will describe a walkthrough of how to implement your own tests according to our benchmark philosophy.</p>"},{"location":"benchmark-developer/#test-setup","title":"Test setup","text":"<p>Tests are collected in the typical Pytest manner at the start of the procedure. In <code>conftest.py</code>, we define the model combinations we want to use in the benchmark.  We distinguish between closed-source and open-source models, since open-source models offer more flexibility, e.g., by setting their size and quantisation. In contrast, for OpenAI models, all we need is the name.</p>"},{"location":"benchmark-developer/#quickstart","title":"Quickstart","text":"<p>For getting started with developing your own benchmark, OpenAI models offer the simplest way, only requiring an API key and an internet connection. If you don't want to run open-source models right away, which is tied to setting up an external service for deploying the models, we can remove the <code>XINFERENCE_MODEL_NAMES</code> from the list of models to be benchmarked in <code>conftest.py</code> (by deleting or commenting them out):</p> <pre><code>BENCHMARKED_MODELS = OPENAI_MODEL_NAMES\n</code></pre> <p>In addition, we can reduce the number of OpenAI models to call to one for development; <code>gpt-3.5-turbo-0125</code> is a well-performing and economical initial choice (in <code>conftest.py</code>).</p> <pre><code>OPENAI_MODEL_NAMES = [\n    \"gpt-3.5-turbo-0125\",\n]\n</code></pre> <p>The last thing to look out for when running the benchmark is to reduce the number of iterations for each test to one. We run iterations to account for stochasticity in LLM responses when we run the benchmark for real, but in development, this iteration brings no benefit and just increases computational cost. Set <code>N_ITERATIONS</code> to 1 in <code>conftest.py</code>.</p> <pre><code># how often should each benchmark be run?\nN_ITERATIONS = 1\n</code></pre> <p>This setup should allow you to run and debug a newly developed benchmark dataset or method effectively. For more explanation on how to do this, please read on.</p>"},{"location":"benchmark-developer/#debugging-the-benchmark","title":"Debugging the benchmark","text":"<p>You can get some insight into how the benchmark works by debugging the existing test cases and stepping through the code line-by-line. For this, it is necessary that you are familiar with the debugging procedure in your programming environment of choice, for instance, VSCode. You can set breakpoints in the initial setup (e.g., in <code>conftest.py</code> and <code>load_dataset.py</code>) as well as the test functions (e.g., <code>test_rag_interpretation.py</code>). Stepping through the code will give you insights into how the benchmark is designed and also how the LLMs respond in detail to each specific task. This is particularly helpful for ensuring that your newly developed benchmark test cases behave as expected and test accurately the functionality you aim to test.</p>"},{"location":"benchmark-developer/#creating-new-test-cases-for-existing-tests","title":"Creating new test cases for existing tests","text":"<p>Our test cases are collected in YAML files that follow a simple formalism for defining each test. A test case consists of</p> <ul> <li> <p>a descriptive name</p> </li> <li> <p>a set of input data to simulate a real-world task (e.g., the question of a user and some auxiliary information)</p> </li> <li> <p>a set of expected results to assess the performance of the model's response, akin to assert statements in regular unit tests</p> </li> </ul> <p>Here is a simple example test case:</p> <pre><code>rag_interpretation:\n# test simple relevance judgement\n  - case: explicit_relevance_yes\n    input:\n      prompt: Which molecular pathways are associated with cancer?\n      system_messages:\n        [\n          \"You will receive a text fragment to help answer the user's question. Your task is to judge this text fragment for  relevance to the user's question, and return either 'yes' or 'no'; only respond with one word, do not offer explanation  or justification! Here is the fragment: \",\n          \"The EGFR pathway is deregulated in a number of cancers.\",\n        ]\n    expected:\n      answer: \"yes\"\n</code></pre> <p>In this test, we benchmark the model's ability to judge the relevance of text fragments to be used in a Retrieval-Augmented Generation (RAG) scenario in an explicit fashion (i.e., we directly ask the model to judge the relevance of the given fragments). Thus, we choose the descriptive name (<code>case</code>) <code>explicit_relevance_yes</code>. The <code>input</code>s are a simulated user question (<code>prompt</code>) and the <code>system_messages</code> that simulate the prompt engineering and RAG fragments (that in the real application would be retrieved by some suitable mechanism). Finally, we define the <code>expected</code> output, which in this case is only the <code>answer</code> we expect from the LLM.</p> <p>The way that these test cases are evaluated is defined in the Pytest functions, which are tuned to the specific fields we define in the YAML. We can freely define test definitions and testing implementation by adjusting the YAML alongside the test Python code. The coordination between cases and Pytest functions is done by name of the test category, in this example <code>rag_interpretation</code> in the YAML and <code>test_rag_interpretation.py</code> in the <code>benchmark</code> directory.</p> <p>New tests can be arbitrarily complex as long as the test function is synchronised with the content of the YAML test dataset's fields (see below).</p>"},{"location":"benchmark-developer/#combinatorial-expansion-of-tests","title":"Combinatorial expansion of tests","text":"<p>Sometimes, it can be efficient to run the same test with slightly different variations of input data. For instance, LLMs are very sensitive to the prompt setup, and thus, we can run the same test with varying approaches to the prompt engineering. To simplify this, we can define input data as dictionaries (with keys being a descriptive name of the sub-test, and values being the content), which will lead to expansion of these test cases into full cases according to the definition. For instance, we can define a RAG interpretation test with prompts at three different levels of detail (<code>simple</code>, <code>more_explicit</code>, and <code>repeat_instruction</code>):</p> <pre><code>rag_interpretation:\n  # test simple irrelevance judgement\n  - case: explicit_relevance_no\n    input:\n      prompt: Which molecular pathways are associated with cancer?\n      system_messages:\n        simple:\n          [\n            \"You will receive a text fragment to help answer the user's question. Your task is to judge this text fragment for relevance to the user's question, and return either 'yes' or 'no'! Here is the fragment: \",\n            \"The earth is a globe.\",\n          ]\n        more_explicit:\n          [\n            \"You will receive a text fragment to help answer the user's question. Your task is to judge this text fragment for relevance to the user's question, and return either 'yes' or 'no'; only respond with one word, do not offer explanation or justification! Here is the fragment: \",\n            \"The earth is a globe.\",\n          ]\n        repeat_instruction:\n          [\n            \"You will receive a text fragment to help answer the user's question. You should only respond with 'yes' or 'no' without additional words. Your task is to judge this text fragment for relevance to the user's question, and return either 'yes' or 'no'; only respond with one word, do not offer explanation or justification! Here is the fragment: \",\n            \"The earth is a globe.\",\n          ]\n    expected:\n      answer: \"no\"\n</code></pre> <p>Upon instantiation of the test matrix, this definition will be expanded into three full tests, each with their respective prompt setup. You can define as many combinations as you like (for instance, you could also define a list of prompts in this example), but be aware that the number of tests will grow exponentially with the number of combinations.</p>"},{"location":"benchmark-developer/#setting-up-the-test-data-pipeline","title":"Setting up the test data pipeline","text":"<p>Test data are provided to the test functions via fixtures. The fixtures are defined in the <code>conftest.py</code> file and are used to load the test data from the YAML files. If you add a new test module or a function with a new kind of test data, you need to add the corresponding fixture to the <code>pytest_generate_tests</code> function in <code>conftest.py</code>. This function is responsible for loading the test data and providing appropriately named fixtures to the test functions. For the tests defined above, this equates to:</p> <pre><code>def pytest_generate_tests(metafunc):\n    data_file = BENCHMARK_DATASET[\"benchmark_data.yaml\"]\n    if \"test_data_rag_interpretation\" in metafunc.fixturenames:\n        metafunc.parametrize(\n            \"test_data_rag_interpretation\",\n            data_file[\"rag_interpretation\"],\n        )\n</code></pre> <p>We prepend the fixtures with <code>test_data_</code> for consistency and higher code readability. For more information, see the Pytest Documentation.</p>"},{"location":"benchmark-developer/#creating-new-test-procedures","title":"Creating new test procedures","text":"<p>If a new kind of test requires a bespoke procedure, such as evaluating a newly introduced functionality or calculating a score in a distinct way, we can introduce new methods to the test modules or even entire new modules. Following the layout of the existing tests, the newly created test functions should refer to fixtures for their data inputs.  Such a test function typically has as parameters:</p> <ul> <li> <p>the <code>model_name</code> fixture, to be able to record a model-specific benchmark metric;</p> </li> <li> <p>a <code>test_data</code> object that is generated from the benchmark dataset according to the name of the test module (e.g., <code>test_data_rag_interpretation</code>). This is the fixture you defined above in <code>conftest.py</code>;</p> </li> <li> <p>a <code>conversation</code> instance (the connection to the LLM to be tested);</p> </li> <li> <p>the <code>multiple_testing</code> fixture that implements running the test multiple times and averaging the results;</p> </li> <li> <p>any number of additional inputs that may be required for the tests.</p> </li> </ul> <p>For instance, the knowledge graph query generation tests acquire additional tests inputs from the YAML definition (the schema of the BioCypher knowledge graph underlying the test) and additional functionality from BioChatter (an instance of the prompt engine class that generates the knowledge graph query using the aforementioned schema).</p>"},{"location":"benchmark-developer/#running-the-benchmark","title":"Running the benchmark","text":"<p>If everything is set up correctly, you can run the benchmark by executing the following command in the root directory of the repository:</p> <pre><code>poetry run pytest benchmark\n</code></pre> <p>We need to specify the <code>benchmark</code> directory to run the benchmark tests, because we also have regular tests in the <code>test</code> directory. If you want to run only a specific test module, you can specify the file name (or use any other Pytest workflow).</p> <p>Skipping tests</p> <p>For efficiency reasons, we by default do not rerun tests that have already been executed for a given model and test case. For this purpose, we store the results in the <code>benchmark/results</code> directory, including, for every test case, an md5 hash of the input data. If you want to rerun a test, you can delete the corresponding line (or entire file) in the <code>results</code> directory.</p> <p>We re-run the benchmark automatically if a test case has changed (reflected in a different md5 hash) or if there is a new <code>biochatter</code> version (potentially introducing changes in the behaviour of the framework). If a test case has changed, the old result is automatically removed from the result files. You can also force a rerun of all tests by using the <code>--run-all</code> flag:</p> <pre><code>poetry run pytest benchmark --run-all\n</code></pre>"},{"location":"benchmark-developer/#running-open-source-models","title":"Running open-source models","text":"<p>To execute the benchmark on any of the open-source models in the test matrix, you need to deploy an Xorbits Inference server at an arbitrary IP, either via Docker (available on Linux machines with dedicated Nvidia GPU) or natively (e.g., on Apple machines). Please refer to the Xinference documentation for details.</p> <p>When you have deployed the Xinference server, you can point the benchmark to the server by setting the <code>BENCHMARK_URL</code> parameter in <code>conftest.py</code>:</p> <pre><code># Xinference IP and port\nBENCHMARK_URL = \"http://localhost:9997\"\n</code></pre>"},{"location":"benchmark-results/","title":"Benchmark - All Results","text":""},{"location":"benchmark-results/#biocypher-query-generation","title":"BioCypher query generation","text":"<p>In this set of tasks, we test LLM abilities to generate queries for a BioCypher Knowledge Graph using BioChatter. The <code>schema_config.yaml</code> of the BioCypher Knowledge Graph and a natural language query are passed to BioChatter.</p> <p>Individual steps of the query generation process are tested separately, as well as the end-to-end performance of the process.</p> Entity selectionRelationship selectionProperty selectionProperty confabulationNaive query generationBioChatter query generationEnd to end query generation Full model name Score achieved Score possible Score SD Accuracy Iterations gpt-4o-2024-05-13 40 40 0 1 5 gpt-3.5-turbo-0125 40 40 0 1 5 openhermes-2.5:7:ggufv2:Q6_K 40 40 0 1 5 openhermes-2.5:7:ggufv2:Q3_K_M 45 45 0 1 5 gpt-3.5-turbo-0613 40 45 0 0.888889 5 gpt-4-0613 40 45 0 0.888889 5 openhermes-2.5:7:ggufv2:Q5_K_M 40 45 0 0.888889 5 openhermes-2.5:7:ggufv2:Q4_K_M 40 45 0 0.888889 5 openhermes-2.5:7:ggufv2:Q8_0 40 45 0 0.888889 5 llama-3-instruct:8:ggufv2:Q6_K 35 40 0 0.875 5 llama-3-instruct:8:ggufv2:Q8_0 35 40 0 0.875 5 llama-3-instruct:8:ggufv2:Q5_K_M 35 40 0 0.875 5 llama-3-instruct:8:ggufv2:Q4_K_M 31 36 0 0.861111 5 gpt-4-0125-preview 35 45 0 0.777778 5 chatglm3:6:ggmlv3:q4_0 30 40 0 0.75 5 openhermes-2.5:7:ggufv2:Q2_K 25 45 0 0.555556 5 code-llama-instruct:7:ggufv2:Q3_K_M 20 40 0 0.5 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 20 40 0 0.5 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 19 40 0 0.475 5 code-llama-instruct:13:ggufv2:Q3_K_M 18 40 0 0.45 5 llama-2-chat:70:ggufv2:Q4_K_M 20 45 0 0.444444 5 llama-2-chat:70:ggufv2:Q5_K_M 20 45 0 0.444444 5 llama-2-chat:7:ggufv2:Q5_K_M 20 45 0 0.444444 5 llama-2-chat:7:ggufv2:Q4_K_M 20 45 0 0.444444 5 llama-2-chat:7:ggufv2:Q8_0 20 45 0 0.444444 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 20 45 0 0.444444 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 19 45 0 0.422222 5 llama-2-chat:7:ggufv2:Q6_K 15 40 0 0.375 5 code-llama-instruct:7:ggufv2:Q4_K_M 15 45 0 0.333333 5 llama-2-chat:7:ggufv2:Q3_K_M 15 45 0 0.333333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 15 45 0 0.333333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 15 45 0 0.333333 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 15 45 0 0.333333 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 15 45 0 0.333333 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 15 45 0 0.333333 5 llama-2-chat:70:ggufv2:Q3_K_M 15 45 0 0.333333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 14 45 0 0.311111 5 code-llama-instruct:7:ggufv2:Q2_K 10 40 0 0.25 5 code-llama-instruct:34:ggufv2:Q8_0 10 40 0 0.25 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 10 45 0 0.222222 5 code-llama-instruct:34:ggufv2:Q6_K 5 40 0 0.125 5 code-llama-instruct:34:ggufv2:Q5_K_M 5 40 0 0.125 5 code-llama-instruct:7:ggufv2:Q5_K_M 5 45 0 0.111111 5 code-llama-instruct:34:ggufv2:Q3_K_M 0 40 0 0 5 code-llama-instruct:34:ggufv2:Q2_K 0 40 0 0 5 code-llama-instruct:13:ggufv2:Q8_0 0 40 0 0 5 llama-2-chat:13:ggufv2:Q8_0 0 45 0 0 5 llama-2-chat:13:ggufv2:Q6_K 0 40 0 0 5 llama-2-chat:13:ggufv2:Q5_K_M 0 45 0 0 5 llama-2-chat:13:ggufv2:Q4_K_M 0 45 0 0 5 llama-2-chat:13:ggufv2:Q3_K_M 0 45 0 0 5 llama-2-chat:13:ggufv2:Q2_K 0 45 0 0 5 code-llama-instruct:7:ggufv2:Q6_K 0 40 0 0 5 code-llama-instruct:7:ggufv2:Q8_0 0 45 0 0 5 code-llama-instruct:13:ggufv2:Q6_K 0 40 0 0 5 code-llama-instruct:13:ggufv2:Q5_K_M 0 40 0 0 5 code-llama-instruct:13:ggufv2:Q4_K_M 0 40 0 0 5 code-llama-instruct:13:ggufv2:Q2_K 0 40 0 0 5 code-llama-instruct:34:ggufv2:Q4_K_M 0 40 0 0 5 llama-2-chat:7:ggufv2:Q2_K 0 45 0 0 5 llama-2-chat:70:ggufv2:Q2_K 0 45 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 0 45 0 0 5 Full model name Score achieved Score possible Score SD Accuracy Iterations gpt-3.5-turbo-0125 60 60 0 1 5 openhermes-2.5:7:ggufv2:Q4_K_M 60 60 0 1 5 openhermes-2.5:7:ggufv2:Q3_K_M 60 60 0 1 5 openhermes-2.5:7:ggufv2:Q5_K_M 60 60 0 1 5 openhermes-2.5:7:ggufv2:Q6_K 60 60 0 1 5 openhermes-2.5:7:ggufv2:Q8_0 60 60 0 1 5 gpt-4-0125-preview 45 60 0 0.75 5 gpt-4-0613 39 60 0 0.65 5 openhermes-2.5:7:ggufv2:Q2_K 30 60 0 0.5 5 gpt-3.5-turbo-0613 30 60 0 0.5 5 code-llama-instruct:34:ggufv2:Q2_K 30 60 0 0.5 5 chatglm3:6:ggmlv3:q4_0 24 60 0 0.4 5 code-llama-instruct:34:ggufv2:Q3_K_M 15 60 0 0.25 5 code-llama-instruct:7:ggufv2:Q2_K 15 60 0 0.25 5 llama-2-chat:70:ggufv2:Q4_K_M 15 60 0 0.25 5 llama-2-chat:70:ggufv2:Q5_K_M 15 60 0 0.25 5 code-llama-instruct:7:ggufv2:Q3_K_M 15 60 0 0.25 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 15 60 0 0.25 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 15 60 0 0.25 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 15 60 0 0.25 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 15 60 0 0.25 5 code-llama-instruct:34:ggufv2:Q4_K_M 0 60 0 0 5 code-llama-instruct:34:ggufv2:Q8_0 0 60 0 0 5 code-llama-instruct:34:ggufv2:Q6_K 0 60 0 0 5 code-llama-instruct:34:ggufv2:Q5_K_M 0 60 0 0 5 code-llama-instruct:13:ggufv2:Q4_K_M 0 60 0 0 5 code-llama-instruct:13:ggufv2:Q6_K 0 60 0 0 5 code-llama-instruct:13:ggufv2:Q5_K_M 0 60 0 0 5 code-llama-instruct:13:ggufv2:Q3_K_M 0 60 0 0 5 code-llama-instruct:13:ggufv2:Q2_K 0 60 0 0 5 llama-2-chat:13:ggufv2:Q8_0 0 60 0 0 5 llama-2-chat:13:ggufv2:Q6_K 0 60 0 0 5 llama-2-chat:13:ggufv2:Q5_K_M 0 60 0 0 5 llama-2-chat:13:ggufv2:Q4_K_M 0 60 0 0 5 llama-2-chat:13:ggufv2:Q3_K_M 0 60 0 0 5 llama-2-chat:13:ggufv2:Q2_K 0 60 0 0 5 gpt-4o-2024-05-13 0 60 0 0 5 code-llama-instruct:7:ggufv2:Q8_0 0 60 0 0 5 code-llama-instruct:7:ggufv2:Q5_K_M 0 60 0 0 5 code-llama-instruct:7:ggufv2:Q4_K_M 0 60 0 0 5 code-llama-instruct:7:ggufv2:Q6_K 0 60 0 0 5 code-llama-instruct:13:ggufv2:Q8_0 0 60 0 0 5 llama-2-chat:7:ggufv2:Q2_K 0 60 0 0 5 llama-2-chat:7:ggufv2:Q3_K_M 0 60 0 0 5 llama-2-chat:70:ggufv2:Q3_K_M 0 60 0 0 5 llama-2-chat:70:ggufv2:Q2_K 0 60 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 0 60 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 0 60 0 0 5 llama-3-instruct:8:ggufv2:Q8_0 0 60 0 0 5 llama-3-instruct:8:ggufv2:Q6_K 0 60 0 0 5 llama-3-instruct:8:ggufv2:Q5_K_M 0 60 0 0 5 llama-3-instruct:8:ggufv2:Q4_K_M 0 60 0 0 5 llama-2-chat:7:ggufv2:Q8_0 0 60 0 0 5 llama-2-chat:7:ggufv2:Q6_K 0 60 0 0 5 llama-2-chat:7:ggufv2:Q5_K_M 0 60 0 0 5 llama-2-chat:7:ggufv2:Q4_K_M 0 60 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 0 60 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 0 60 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 0 60 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 0 60 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 0 60 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 0 60 0 0 5 Full model name Score achieved Score possible Score SD Accuracy Iterations gpt-3.5-turbo-0613 116 320 0 0.3625 5 gpt-4-0613 115 320 0 0.359375 5 gpt-3.5-turbo-0125 114 320 0 0.35625 5 chatglm3:6:ggmlv3:q4_0 92 320 0 0.2875 5 llama-3-instruct:8:ggufv2:Q8_0 90 320 0 0.28125 5 llama-3-instruct:8:ggufv2:Q6_K 90 320 0 0.28125 5 llama-3-instruct:8:ggufv2:Q5_K_M 60 320 0 0.1875 5 llama-2-chat:70:ggufv2:Q3_K_M 55 320 0 0.171875 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 52 320 0 0.1625 5 openhermes-2.5:7:ggufv2:Q3_K_M 40 320 0 0.125 5 openhermes-2.5:7:ggufv2:Q5_K_M 40 320 0 0.125 5 openhermes-2.5:7:ggufv2:Q8_0 40 320 0 0.125 5 llama-3-instruct:8:ggufv2:Q4_K_M 35 320 0 0.109375 5 llama-2-chat:7:ggufv2:Q3_K_M 32 320 0 0.1 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 21 320 0 0.065625 5 code-llama-instruct:7:ggufv2:Q2_K 20 320 0 0.0625 5 openhermes-2.5:7:ggufv2:Q4_K_M 15 320 0 0.046875 5 openhermes-2.5:7:ggufv2:Q6_K 15 320 0 0.046875 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 15 320 0 0.046875 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 15 320 0 0.046875 5 llama-2-chat:7:ggufv2:Q5_K_M 12 320 0 0.0375 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 12 320 0 0.0375 5 code-llama-instruct:13:ggufv2:Q6_K 0 320 0 0 5 code-llama-instruct:13:ggufv2:Q8_0 0 320 0 0 5 code-llama-instruct:34:ggufv2:Q3_K_M 0 320 0 0 5 code-llama-instruct:34:ggufv2:Q2_K 0 320 0 0 5 code-llama-instruct:34:ggufv2:Q4_K_M 0 320 0 0 5 code-llama-instruct:13:ggufv2:Q4_K_M 0 320 0 0 5 code-llama-instruct:13:ggufv2:Q5_K_M 0 320 0 0 5 code-llama-instruct:13:ggufv2:Q3_K_M 0 320 0 0 5 llama-2-chat:13:ggufv2:Q8_0 0 320 0 0 5 llama-2-chat:13:ggufv2:Q6_K 0 320 0 0 5 llama-2-chat:13:ggufv2:Q5_K_M 0 320 0 0 5 llama-2-chat:13:ggufv2:Q4_K_M 0 320 0 0 5 llama-2-chat:13:ggufv2:Q3_K_M 0 320 0 0 5 llama-2-chat:13:ggufv2:Q2_K 0 320 0 0 5 gpt-4o-2024-05-13 0 320 0 0 5 gpt-4-0125-preview 0 320 0 0 5 code-llama-instruct:7:ggufv2:Q4_K_M 0 320 0 0 5 code-llama-instruct:7:ggufv2:Q8_0 0 320 0 0 5 code-llama-instruct:7:ggufv2:Q5_K_M 0 320 0 0 5 code-llama-instruct:7:ggufv2:Q6_K 0 320 0 0 5 code-llama-instruct:7:ggufv2:Q3_K_M 0 320 0 0 5 code-llama-instruct:34:ggufv2:Q5_K_M 0 320 0 0 5 code-llama-instruct:34:ggufv2:Q6_K 0 320 0 0 5 code-llama-instruct:34:ggufv2:Q8_0 0 320 0 0 5 code-llama-instruct:13:ggufv2:Q2_K 0 320 0 0 5 llama-2-chat:7:ggufv2:Q6_K 0 320 0 0 5 llama-2-chat:7:ggufv2:Q8_0 0 320 0 0 5 llama-2-chat:70:ggufv2:Q2_K 0 320 0 0 5 llama-2-chat:70:ggufv2:Q5_K_M 0 320 0 0 5 llama-2-chat:70:ggufv2:Q4_K_M 0 320 0 0 5 llama-2-chat:7:ggufv2:Q2_K 0 320 0 0 5 llama-2-chat:7:ggufv2:Q4_K_M 0 320 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 0 320 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 0 320 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 0 320 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 0 320 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 0 320 0 0 5 openhermes-2.5:7:ggufv2:Q2_K 0 320 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 0 320 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 0 320 0 0 5 Full model name Score achieved Score possible Score SD Accuracy Iterations code-llama-instruct:34:ggufv2:Q4_K_M 39 40 0 0.975 5 code-llama-instruct:34:ggufv2:Q5_K_M 38 40 0 0.95 5 code-llama-instruct:34:ggufv2:Q8_0 37 40 0 0.925 5 code-llama-instruct:34:ggufv2:Q6_K 36 40 0 0.9 5 gpt-4-0613 40 45 0 0.888889 5 code-llama-instruct:13:ggufv2:Q2_K 35 40 0 0.875 5 code-llama-instruct:34:ggufv2:Q3_K_M 35 40 0 0.875 5 gpt-3.5-turbo-0125 39 45 0 0.866667 5 code-llama-instruct:13:ggufv2:Q3_K_M 34 40 0 0.85 5 gpt-4o-2024-05-13 34 40 0 0.85 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 34 40 0 0.85 5 openhermes-2.5:7:ggufv2:Q2_K 38 45 0 0.844444 5 code-llama-instruct:13:ggufv2:Q6_K 33 40 0 0.825 5 code-llama-instruct:7:ggufv2:Q3_K_M 36 45 0 0.8 5 code-llama-instruct:7:ggufv2:Q2_K 32 40 0 0.8 5 llama-2-chat:70:ggufv2:Q3_K_M 35 45 0 0.777778 5 llama-2-chat:70:ggufv2:Q5_K_M 35 45 0 0.777778 5 openhermes-2.5:7:ggufv2:Q5_K_M 35 45 0 0.777778 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 35 45 0 0.777778 5 llama-2-chat:13:ggufv2:Q4_K_M 35 45 0 0.777778 5 code-llama-instruct:13:ggufv2:Q4_K_M 31 40 0 0.775 5 llama-2-chat:13:ggufv2:Q6_K 31 40 0 0.775 5 llama-3-instruct:8:ggufv2:Q4_K_M 31 40 0 0.775 5 llama-3-instruct:8:ggufv2:Q6_K 31 40 0 0.775 5 code-llama-instruct:7:ggufv2:Q6_K 31 40 0 0.775 5 code-llama-instruct:13:ggufv2:Q5_K_M 31 40 0 0.775 5 gpt-3.5-turbo-0613 34 45 0 0.755556 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 34 45 0 0.755556 5 openhermes-2.5:7:ggufv2:Q8_0 34 45 0 0.755556 5 llama-2-chat:70:ggufv2:Q4_K_M 34 45 0 0.755556 5 openhermes-2.5:7:ggufv2:Q4_K_M 34 45 0 0.755556 5 code-llama-instruct:13:ggufv2:Q8_0 30 40 0 0.75 5 code-llama-instruct:34:ggufv2:Q2_K 30 40 0 0.75 5 llama-2-chat:13:ggufv2:Q3_K_M 33 45 0 0.733333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 33 45 0 0.733333 5 openhermes-2.5:7:ggufv2:Q6_K 33 45 0 0.733333 5 gpt-4-0125-preview 33 45 0 0.733333 5 llama-3-instruct:8:ggufv2:Q8_0 29 40 0 0.725 5 openhermes-2.5:7:ggufv2:Q3_K_M 36 50 0 0.72 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 32 45 0 0.711111 5 llama-2-chat:13:ggufv2:Q8_0 32 45 0 0.711111 5 llama-2-chat:7:ggufv2:Q2_K 31 45 0 0.688889 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 31 45 0 0.688889 5 code-llama-instruct:7:ggufv2:Q5_K_M 31 45 0 0.688889 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 31 45 0 0.688889 5 code-llama-instruct:7:ggufv2:Q8_0 30 45 0 0.666667 5 llama-2-chat:70:ggufv2:Q2_K 30 45 0 0.666667 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 30 45 0 0.666667 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 30 45 0 0.666667 5 llama-3-instruct:8:ggufv2:Q5_K_M 26 40 0 0.65 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 26 40 0 0.65 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 29 45 0 0.644444 5 llama-2-chat:13:ggufv2:Q5_K_M 29 45 0 0.644444 5 code-llama-instruct:7:ggufv2:Q4_K_M 27 45 0 0.6 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 27 45 0 0.6 5 llama-2-chat:7:ggufv2:Q4_K_M 22 45 0 0.488889 5 llama-2-chat:7:ggufv2:Q3_K_M 21 45 0 0.466667 5 llama-2-chat:7:ggufv2:Q8_0 16 45 0 0.355556 5 llama-2-chat:7:ggufv2:Q6_K 15 45 0 0.333333 5 llama-2-chat:13:ggufv2:Q2_K 13 45 0 0.288889 5 llama-2-chat:7:ggufv2:Q5_K_M 13 45 0 0.288889 5 chatglm3:6:ggmlv3:q4_0 11 40 0 0.275 5 Full model name Score achieved Score possible Score SD Accuracy Iterations gpt-4-0613 102 150 0 0.68 5 llama-3-instruct:8:ggufv2:Q6_K 100 150 0 0.666667 5 llama-3-instruct:8:ggufv2:Q8_0 100 150 0 0.666667 5 llama-3-instruct:8:ggufv2:Q4_K_M 100 150 0 0.666667 5 code-llama-instruct:7:ggufv2:Q4_K_M 98 150 0 0.653333 5 code-llama-instruct:34:ggufv2:Q3_K_M 90 150 0 0.6 5 llama-3-instruct:8:ggufv2:Q5_K_M 90 150 0 0.6 5 openhermes-2.5:7:ggufv2:Q5_K_M 88 150 0 0.586667 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 86 150 0 0.573333 5 code-llama-instruct:34:ggufv2:Q2_K 85 150 0 0.566667 5 code-llama-instruct:13:ggufv2:Q5_K_M 85 150 0 0.566667 5 code-llama-instruct:13:ggufv2:Q2_K 85 150 0 0.566667 5 code-llama-instruct:13:ggufv2:Q8_0 85 150 0 0.566667 5 code-llama-instruct:13:ggufv2:Q6_K 81 150 0 0.54 5 code-llama-instruct:7:ggufv2:Q2_K 80 150 0 0.533333 5 openhermes-2.5:7:ggufv2:Q6_K 80 150 0 0.533333 5 code-llama-instruct:13:ggufv2:Q4_K_M 80 150 0 0.533333 5 code-llama-instruct:13:ggufv2:Q3_K_M 80 150 0 0.533333 5 gpt-4o-2024-05-13 80 150 0 0.533333 5 gpt-3.5-turbo-0613 75 150 0 0.5 5 gpt-3.5-turbo-0125 73 150 0 0.486667 5 chatglm3:6:ggmlv3:q4_0 72 150 0 0.48 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 72 150 0 0.48 5 llama-2-chat:13:ggufv2:Q3_K_M 72 150 0 0.48 5 llama-2-chat:13:ggufv2:Q8_0 72 150 0 0.48 5 code-llama-instruct:34:ggufv2:Q6_K 71 150 0 0.473333 5 llama-2-chat:70:ggufv2:Q2_K 71 150 0 0.473333 5 code-llama-instruct:34:ggufv2:Q8_0 70 150 0 0.466667 5 code-llama-instruct:34:ggufv2:Q4_K_M 70 150 0 0.466667 5 code-llama-instruct:34:ggufv2:Q5_K_M 70 150 0 0.466667 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 70 150 0 0.466667 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 70 150 0 0.466667 5 openhermes-2.5:7:ggufv2:Q3_K_M 70 150 0 0.466667 5 openhermes-2.5:7:ggufv2:Q4_K_M 70 150 0 0.466667 5 openhermes-2.5:7:ggufv2:Q8_0 70 150 0 0.466667 5 gpt-4-0125-preview 66 150 0 0.44 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 65 150 0 0.433333 5 llama-2-chat:13:ggufv2:Q5_K_M 65 150 0 0.433333 5 openhermes-2.5:7:ggufv2:Q2_K 65 150 0 0.433333 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 65 150 0 0.433333 5 code-llama-instruct:7:ggufv2:Q3_K_M 64 150 0 0.426667 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 64 150 0 0.426667 5 llama-2-chat:70:ggufv2:Q4_K_M 63 150 0 0.42 5 llama-2-chat:70:ggufv2:Q3_K_M 62 150 0 0.413333 5 code-llama-instruct:7:ggufv2:Q8_0 60 150 0 0.4 5 code-llama-instruct:7:ggufv2:Q5_K_M 60 150 0 0.4 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 58 150 0 0.386667 5 llama-2-chat:13:ggufv2:Q6_K 58 150 0 0.386667 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 57 150 0 0.38 5 llama-2-chat:13:ggufv2:Q2_K 55 150 0 0.366667 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 55 150 0 0.366667 5 llama-2-chat:13:ggufv2:Q4_K_M 55 150 0 0.366667 5 llama-2-chat:70:ggufv2:Q5_K_M 54 150 0 0.36 5 code-llama-instruct:7:ggufv2:Q6_K 50 150 0 0.333333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 50 150 0 0.333333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 50 150 0 0.333333 5 llama-2-chat:7:ggufv2:Q5_K_M 44 150 0 0.293333 5 llama-2-chat:7:ggufv2:Q6_K 40 150 0 0.266667 5 llama-2-chat:7:ggufv2:Q8_0 40 150 0 0.266667 5 llama-2-chat:7:ggufv2:Q4_K_M 36 150 0 0.24 5 llama-2-chat:7:ggufv2:Q3_K_M 35 150 0 0.233333 5 llama-2-chat:7:ggufv2:Q2_K 15 150 0 0.1 5 Full model name Score achieved Score possible Score SD Accuracy Iterations gpt-3.5-turbo-0125 145 150 0 0.966667 5 code-llama-instruct:7:ggufv2:Q4_K_M 145 150 0 0.966667 5 gpt-4-0613 145 150 0 0.966667 5 code-llama-instruct:7:ggufv2:Q6_K 144 150 0 0.96 5 code-llama-instruct:7:ggufv2:Q5_K_M 144 150 0 0.96 5 code-llama-instruct:7:ggufv2:Q8_0 144 150 0 0.96 5 gpt-3.5-turbo-0613 142 150 0 0.946667 5 openhermes-2.5:7:ggufv2:Q3_K_M 141 150 0 0.94 5 openhermes-2.5:7:ggufv2:Q2_K 141 150 0 0.94 5 llama-3-instruct:8:ggufv2:Q6_K 139 150 0 0.926667 5 llama-3-instruct:8:ggufv2:Q5_K_M 139 150 0 0.926667 5 code-llama-instruct:7:ggufv2:Q2_K 138 150 0 0.92 5 llama-3-instruct:8:ggufv2:Q8_0 138 150 0 0.92 5 llama-3-instruct:8:ggufv2:Q4_K_M 138 150 0 0.92 5 llama-2-chat:70:ggufv2:Q4_K_M 138 150 0 0.92 5 openhermes-2.5:7:ggufv2:Q5_K_M 137 150 0 0.913333 5 code-llama-instruct:34:ggufv2:Q4_K_M 136 150 0 0.906667 5 llama-2-chat:70:ggufv2:Q5_K_M 136 150 0 0.906667 5 llama-2-chat:70:ggufv2:Q3_K_M 136 150 0 0.906667 5 llama-2-chat:70:ggufv2:Q2_K 135 150 0 0.9 5 code-llama-instruct:34:ggufv2:Q5_K_M 135 150 0 0.9 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 134 150 0 0.893333 5 openhermes-2.5:7:ggufv2:Q8_0 132 150 0 0.88 5 code-llama-instruct:7:ggufv2:Q3_K_M 131 150 0 0.873333 5 openhermes-2.5:7:ggufv2:Q4_K_M 131 150 0 0.873333 5 openhermes-2.5:7:ggufv2:Q6_K 129 150 0 0.86 5 code-llama-instruct:34:ggufv2:Q8_0 129 150 0 0.86 5 code-llama-instruct:34:ggufv2:Q6_K 128 150 0 0.853333 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 127 150 0 0.846667 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 127 150 0 0.846667 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 126 150 0 0.84 5 code-llama-instruct:13:ggufv2:Q4_K_M 125 150 0 0.833333 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 125 150 0 0.833333 5 gpt-4-0125-preview 125 150 0 0.833333 5 code-llama-instruct:13:ggufv2:Q3_K_M 125 150 0 0.833333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 124 150 0 0.826667 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 124 150 0 0.826667 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 124 150 0 0.826667 5 code-llama-instruct:13:ggufv2:Q2_K 123 150 0 0.82 5 llama-2-chat:13:ggufv2:Q6_K 122 150 0 0.813333 5 gpt-4o-2024-05-13 120 150 0 0.8 5 code-llama-instruct:13:ggufv2:Q6_K 119 150 0 0.793333 5 llama-2-chat:13:ggufv2:Q8_0 118 150 0 0.786667 5 code-llama-instruct:34:ggufv2:Q3_K_M 118 150 0 0.786667 5 code-llama-instruct:13:ggufv2:Q5_K_M 117 150 0 0.78 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 116 150 0 0.773333 5 code-llama-instruct:13:ggufv2:Q8_0 115 150 0 0.766667 5 llama-2-chat:13:ggufv2:Q4_K_M 114 150 0 0.76 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 114 150 0 0.76 5 llama-2-chat:13:ggufv2:Q5_K_M 112 150 0 0.746667 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 109 150 0 0.726667 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 104 150 0 0.693333 5 llama-2-chat:7:ggufv2:Q3_K_M 104 150 0 0.693333 5 code-llama-instruct:34:ggufv2:Q2_K 103 150 0 0.686667 5 llama-2-chat:7:ggufv2:Q2_K 103 150 0 0.686667 5 llama-2-chat:13:ggufv2:Q3_K_M 102 150 0 0.68 5 llama-2-chat:7:ggufv2:Q6_K 99 150 0 0.66 5 llama-2-chat:7:ggufv2:Q4_K_M 97 150 0 0.646667 5 llama-2-chat:7:ggufv2:Q8_0 96 150 0 0.64 5 llama-2-chat:7:ggufv2:Q5_K_M 95 150 0 0.633333 5 chatglm3:6:ggmlv3:q4_0 83 150 0 0.553333 5 llama-2-chat:13:ggufv2:Q2_K 65 150 0 0.433333 5 Full model name Score achieved Score possible Score SD Accuracy Iterations gpt-3.5-turbo-0125 139 150 0 0.926667 5 gpt-4-0613 132 150 0 0.88 5 gpt-3.5-turbo-0613 125 150 0 0.833333 5 code-llama-instruct:13:ggufv2:Q3_K_M 0 150 0 0 5 chatglm3:6:ggmlv3:q4_0 0 150 0 0 5 code-llama-instruct:13:ggufv2:Q2_K 0 150 0 0 5 code-llama-instruct:13:ggufv2:Q8_0 0 150 0 0 5 code-llama-instruct:34:ggufv2:Q2_K 0 150 0 0 5 code-llama-instruct:34:ggufv2:Q3_K_M 0 150 0 0 5 code-llama-instruct:34:ggufv2:Q4_K_M 0 150 0 0 5 code-llama-instruct:34:ggufv2:Q5_K_M 0 150 0 0 5 code-llama-instruct:13:ggufv2:Q4_K_M 0 150 0 0 5 code-llama-instruct:13:ggufv2:Q5_K_M 0 150 0 0 5 code-llama-instruct:13:ggufv2:Q6_K 0 150 0 0 5 code-llama-instruct:7:ggufv2:Q2_K 0 150 0 0 5 code-llama-instruct:34:ggufv2:Q8_0 0 150 0 0 5 code-llama-instruct:34:ggufv2:Q6_K 0 150 0 0 5 code-llama-instruct:7:ggufv2:Q3_K_M 0 150 0 0 5 code-llama-instruct:7:ggufv2:Q6_K 0 150 0 0 5 code-llama-instruct:7:ggufv2:Q5_K_M 0 150 0 0 5 code-llama-instruct:7:ggufv2:Q8_0 0 150 0 0 5 code-llama-instruct:7:ggufv2:Q4_K_M 0 150 0 0 5 gpt-4-0125-preview 0 150 0 0 5 gpt-4o-2024-05-13 0 150 0 0 5 llama-2-chat:13:ggufv2:Q2_K 0 150 0 0 5 llama-2-chat:13:ggufv2:Q3_K_M 0 150 0 0 5 llama-2-chat:13:ggufv2:Q4_K_M 0 150 0 0 5 llama-2-chat:13:ggufv2:Q5_K_M 0 150 0 0 5 llama-2-chat:13:ggufv2:Q6_K 0 150 0 0 5 llama-2-chat:13:ggufv2:Q8_0 0 150 0 0 5 llama-2-chat:70:ggufv2:Q2_K 0 150 0 0 5 llama-2-chat:70:ggufv2:Q3_K_M 0 150 0 0 5 llama-2-chat:70:ggufv2:Q4_K_M 0 150 0 0 5 llama-2-chat:70:ggufv2:Q5_K_M 0 150 0 0 5 llama-2-chat:7:ggufv2:Q2_K 0 150 0 0 5 llama-2-chat:7:ggufv2:Q3_K_M 0 150 0 0 5 llama-2-chat:7:ggufv2:Q4_K_M 0 150 0 0 5 llama-2-chat:7:ggufv2:Q5_K_M 0 150 0 0 5 llama-2-chat:7:ggufv2:Q6_K 0 150 0 0 5 llama-2-chat:7:ggufv2:Q8_0 0 150 0 0 5 llama-3-instruct:8:ggufv2:Q4_K_M 0 150 0 0 5 llama-3-instruct:8:ggufv2:Q5_K_M 0 150 0 0 5 llama-3-instruct:8:ggufv2:Q6_K 0 150 0 0 5 llama-3-instruct:8:ggufv2:Q8_0 0 150 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 0 150 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 0 150 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 0 150 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 0 150 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 0 150 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 0 150 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 0 150 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 0 150 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 0 150 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 0 150 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 0 150 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 0 150 0 0 5 openhermes-2.5:7:ggufv2:Q2_K 0 150 0 0 5 openhermes-2.5:7:ggufv2:Q3_K_M 0 150 0 0 5 openhermes-2.5:7:ggufv2:Q4_K_M 0 150 0 0 5 openhermes-2.5:7:ggufv2:Q5_K_M 0 150 0 0 5 openhermes-2.5:7:ggufv2:Q6_K 0 150 0 0 5 openhermes-2.5:7:ggufv2:Q8_0 0 150 0 0 5"},{"location":"benchmark-results/#retrieval-augmented-generation-rag","title":"Retrieval-Augmented Generation (RAG)","text":"<p>In this set of tasks, we test LLM abilities to generate answers to a given question using a RAG agent, or to judge the relevance of a RAG fragment to a given question. Instructions can be explicit (\"is this fragment relevant to the question?\") or implicit (just asking the question without instructions and evaluating whether the model responds with 'not enough information given').</p> Explicit relevance of single fragmentsImplicit relevance of single fragments Full model name Score achieved Score possible Score SD Accuracy Iterations llama-2-chat:13:ggufv2:Q2_K 30 30 0 1 5 llama-2-chat:13:ggufv2:Q8_0 30 30 0 1 5 llama-2-chat:13:ggufv2:Q3_K_M 30 30 0 1 5 llama-2-chat:13:ggufv2:Q6_K 30 30 0 1 5 llama-2-chat:13:ggufv2:Q5_K_M 30 30 0 1 5 llama-2-chat:13:ggufv2:Q4_K_M 30 30 0 1 5 gpt-4-0613 30 30 0 1 5 gpt-4o-2024-05-13 30 30 0 1 5 gpt-3.5-turbo-0613 30 30 0 1 5 gpt-4-0125-preview 30 30 0 1 5 code-llama-instruct:7:ggufv2:Q8_0 30 30 0 1 5 gpt-3.5-turbo-0125 30 30 0 1 5 code-llama-instruct:7:ggufv2:Q4_K_M 30 30 0 1 5 openhermes-2.5:7:ggufv2:Q6_K 30 30 0 1 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 30 30 0 1 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 30 30 0 1 5 llama-3-instruct:8:ggufv2:Q8_0 30 30 0 1 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 30 30 0 1 5 llama-3-instruct:8:ggufv2:Q5_K_M 30 30 0 1 5 llama-3-instruct:8:ggufv2:Q4_K_M 30 30 0 1 5 llama-2-chat:7:ggufv2:Q8_0 30 30 0 1 5 llama-3-instruct:8:ggufv2:Q6_K 30 30 0 1 5 llama-2-chat:7:ggufv2:Q6_K 30 30 0 1 5 llama-2-chat:7:ggufv2:Q5_K_M 30 30 0 1 5 llama-2-chat:7:ggufv2:Q4_K_M 30 30 0 1 5 llama-2-chat:7:ggufv2:Q3_K_M 30 30 0 1 5 llama-2-chat:70:ggufv2:Q5_K_M 30 30 0 1 5 llama-2-chat:70:ggufv2:Q4_K_M 30 30 0 1 5 llama-2-chat:70:ggufv2:Q3_K_M 30 30 0 1 5 llama-2-chat:70:ggufv2:Q2_K 30 30 0 1 5 openhermes-2.5:7:ggufv2:Q8_0 30 30 0 1 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 30 30 0 1 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 30 30 0 1 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 30 30 0 1 5 openhermes-2.5:7:ggufv2:Q4_K_M 30 30 0 1 5 openhermes-2.5:7:ggufv2:Q5_K_M 30 30 0 1 5 openhermes-2.5:7:ggufv2:Q2_K 30 30 0 1 5 openhermes-2.5:7:ggufv2:Q3_K_M 30 30 0 1 5 code-llama-instruct:13:ggufv2:Q6_K 25 30 0 0.833333 5 llama-2-chat:7:ggufv2:Q2_K 25 30 0 0.833333 5 code-llama-instruct:7:ggufv2:Q5_K_M 25 30 0 0.833333 5 code-llama-instruct:13:ggufv2:Q8_0 25 30 0 0.833333 5 code-llama-instruct:7:ggufv2:Q3_K_M 25 30 0 0.833333 5 code-llama-instruct:7:ggufv2:Q6_K 25 30 0 0.833333 5 chatglm3:6:ggmlv3:q4_0 22 30 0 0.733333 5 code-llama-instruct:13:ggufv2:Q5_K_M 20 30 0 0.666667 5 code-llama-instruct:34:ggufv2:Q2_K 15 30 0 0.5 5 code-llama-instruct:34:ggufv2:Q3_K_M 15 30 0 0.5 5 code-llama-instruct:34:ggufv2:Q4_K_M 15 30 0 0.5 5 code-llama-instruct:34:ggufv2:Q8_0 10 30 0 0.333333 5 code-llama-instruct:13:ggufv2:Q4_K_M 10 30 0 0.333333 5 code-llama-instruct:7:ggufv2:Q2_K 10 30 0 0.333333 5 code-llama-instruct:34:ggufv2:Q6_K 10 30 0 0.333333 5 code-llama-instruct:34:ggufv2:Q5_K_M 10 30 0 0.333333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 10 30 0 0.333333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 5 30 0 0.166667 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 4 30 0 0.133333 5 code-llama-instruct:13:ggufv2:Q2_K 1 30 0 0.0333333 5 code-llama-instruct:13:ggufv2:Q3_K_M 0 30 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 0 30 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 0 30 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 0 30 0 0 5 Full model name Score achieved Score possible Score SD Accuracy Iterations chatglm3:6:ggmlv3:q4_0 10 10 0 1 5 code-llama-instruct:34:ggufv2:Q2_K 10 10 0 1 5 code-llama-instruct:34:ggufv2:Q5_K_M 10 10 0 1 5 gpt-4-0613 10 10 0 1 5 gpt-3.5-turbo-0613 10 10 0 1 5 code-llama-instruct:7:ggufv2:Q4_K_M 10 10 0 1 5 llama-3-instruct:8:ggufv2:Q5_K_M 10 10 0 1 5 llama-3-instruct:8:ggufv2:Q4_K_M 10 10 0 1 5 llama-3-instruct:8:ggufv2:Q8_0 10 10 0 1 5 llama-3-instruct:8:ggufv2:Q6_K 10 10 0 1 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 10 10 0 1 5 llama-2-chat:7:ggufv2:Q3_K_M 10 10 0 1 5 llama-2-chat:7:ggufv2:Q2_K 10 10 0 1 5 openhermes-2.5:7:ggufv2:Q4_K_M 10 10 0 1 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 10 10 0 1 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 10 10 0 1 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 10 10 0 1 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 10 10 0 1 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 10 10 0 1 5 openhermes-2.5:7:ggufv2:Q5_K_M 10 10 0 1 5 openhermes-2.5:7:ggufv2:Q6_K 10 10 0 1 5 openhermes-2.5:7:ggufv2:Q8_0 10 10 0 1 5 llama-2-chat:70:ggufv2:Q4_K_M 10 10 0 1 5 gpt-3.5-turbo-0125 9 10 0 0.9 5 code-llama-instruct:34:ggufv2:Q8_0 9 10 0 0.9 5 code-llama-instruct:34:ggufv2:Q6_K 9 10 0 0.9 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 9 10 0 0.9 5 llama-2-chat:70:ggufv2:Q5_K_M 9 10 0 0.9 5 code-llama-instruct:7:ggufv2:Q6_K 9 10 0 0.9 5 code-llama-instruct:7:ggufv2:Q3_K_M 7 10 0 0.7 5 gpt-4o-2024-05-13 7 10 0 0.7 5 code-llama-instruct:7:ggufv2:Q2_K 7 10 0 0.7 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 7 10 0 0.7 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 6 10 0 0.6 5 llama-2-chat:7:ggufv2:Q5_K_M 6 10 0 0.6 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 6 10 0 0.6 5 code-llama-instruct:13:ggufv2:Q4_K_M 5 10 0 0.5 5 llama-2-chat:13:ggufv2:Q4_K_M 5 10 0 0.5 5 code-llama-instruct:7:ggufv2:Q5_K_M 5 10 0 0.5 5 code-llama-instruct:13:ggufv2:Q8_0 5 10 0 0.5 5 code-llama-instruct:13:ggufv2:Q5_K_M 5 10 0 0.5 5 code-llama-instruct:13:ggufv2:Q6_K 5 10 0 0.5 5 code-llama-instruct:34:ggufv2:Q3_K_M 5 10 0 0.5 5 llama-2-chat:13:ggufv2:Q2_K 5 10 0 0.5 5 code-llama-instruct:7:ggufv2:Q8_0 5 10 0 0.5 5 gpt-4-0125-preview 5 10 0 0.5 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 5 10 0 0.5 5 llama-2-chat:7:ggufv2:Q6_K 5 10 0 0.5 5 llama-2-chat:7:ggufv2:Q8_0 5 10 0 0.5 5 llama-2-chat:13:ggufv2:Q8_0 5 10 0 0.5 5 llama-2-chat:13:ggufv2:Q3_K_M 5 10 0 0.5 5 llama-2-chat:13:ggufv2:Q6_K 5 10 0 0.5 5 llama-2-chat:13:ggufv2:Q5_K_M 5 10 0 0.5 5 llama-2-chat:70:ggufv2:Q3_K_M 5 10 0 0.5 5 llama-2-chat:70:ggufv2:Q2_K 5 10 0 0.5 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 5 10 0 0.5 5 openhermes-2.5:7:ggufv2:Q3_K_M 5 10 0 0.5 5 llama-2-chat:7:ggufv2:Q4_K_M 5 10 0 0.5 5 openhermes-2.5:7:ggufv2:Q2_K 5 10 0 0.5 5 code-llama-instruct:13:ggufv2:Q2_K 4 10 0 0.4 5 code-llama-instruct:34:ggufv2:Q4_K_M 4 10 0 0.4 5 code-llama-instruct:13:ggufv2:Q3_K_M 0 10 0 0 5"},{"location":"benchmark-results/#text-extraction","title":"Text Extraction","text":"<p>In this set of tasks, we test LLM abilities to extract text from a given document.</p> Overall PerformanceAssay extractionChemical extractionContext extractionDisease extractionEntity extractionExperiment extraction (yes or no)Hypothesis extractionIntervention extractionGene identifier mappingSignificance extractionStatistics extraction Full model name Score achieved Score possible Score SD Accuracy Iterations gpt-4-0125-preview 341.404 495 0 0.689705 5 gpt-4-0613 331.107 495 0 0.668903 5 gpt-4o-2024-05-13 323.703 495 0 0.653946 5 openhermes-2.5:7:ggufv2:Q6_K 306.488 495 0 0.619167 5 openhermes-2.5:7:ggufv2:Q8_0 297.41 495 0 0.600829 5 openhermes-2.5:7:ggufv2:Q4_K_M 295.654 495 0 0.597281 5 openhermes-2.5:7:ggufv2:Q5_K_M 287.059 495 0 0.579916 5 gpt-3.5-turbo-0613 284.814 495 0 0.575381 5 openhermes-2.5:7:ggufv2:Q3_K_M 274.471 495 0 0.554488 5 gpt-3.5-turbo-0125 252.466 495 0 0.510032 5 openhermes-2.5:7:ggufv2:Q2_K 219.807 495 0 0.444054 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 190.948 495 0 0.385754 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 182.642 495 0 0.368974 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 181.869 495 0 0.367412 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 174.084 495 0 0.351684 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 171.777 495 0 0.347025 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 163.974 495 0 0.331261 5 llama-2-chat:70:ggufv2:Q4_K_M 119.263 495 0 0.240936 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 116.651 495 0 0.235659 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 113.663 495 0 0.229622 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 111.634 495 0 0.225524 5 llama-2-chat:70:ggufv2:Q2_K 106.448 495 0 0.215047 5 llama-2-chat:70:ggufv2:Q5_K_M 104.032 495 0 0.210166 5 llama-2-chat:70:ggufv2:Q3_K_M 97.9593 495 0 0.197898 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 95.9243 495 0 0.193786 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 93.6428 495 0 0.189177 5 llama-3-instruct:8:ggufv2:Q8_0 93.3345 495 0 0.188555 5 chatglm3:6:ggmlv3:q4_0 93.2008 495 0 0.188284 5 llama-3-instruct:8:ggufv2:Q5_K_M 82.3847 495 0 0.166434 5 llama-3-instruct:8:ggufv2:Q6_K 80.5152 495 0 0.162657 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 77.9693 495 0 0.157514 5 code-llama-instruct:7:ggufv2:Q4_K_M 68.6724 495 0 0.138732 5 llama-3-instruct:8:ggufv2:Q4_K_M 57.8514 495 0 0.116871 5 llama-2-chat:13:ggufv2:Q3_K_M 55.7521 495 0 0.112631 5 llama-2-chat:13:ggufv2:Q4_K_M 43.9894 495 0 0.0888675 5 llama-2-chat:7:ggufv2:Q4_K_M 42.1985 495 0 0.0852494 5 llama-2-chat:7:ggufv2:Q8_0 25.1647 297 1.46597e-16 0.0847297 3 llama-2-chat:13:ggufv2:Q6_K 23.2057 297 0.00246731 0.0781337 3 llama-2-chat:13:ggufv2:Q5_K_M 37.9252 495 0 0.0766167 5 llama-2-chat:13:ggufv2:Q8_0 37.7416 495 0 0.0762457 5 llama-2-chat:7:ggufv2:Q5_K_M 34.5308 495 0 0.0697591 5 llama-2-chat:7:ggufv2:Q3_K_M 32.2105 495 0 0.0650717 5 llama-2-chat:13:ggufv2:Q2_K 32.1447 495 0 0.0649389 5 llama-2-chat:7:ggufv2:Q6_K 18.2539 297 2.57076e-16 0.0614608 3 llama-2-chat:7:ggufv2:Q2_K 17.9123 495 0 0.0361865 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations gpt-4o-2024-05-13 assay 6.67307 45 0 0.148291 5 gpt-4-0125-preview assay 6.60264 45 0 0.146725 5 openhermes-2.5:7:ggufv2:Q6_K assay 6.45354 45 0 0.143412 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M assay 6.42156 45 0 0.142701 5 openhermes-2.5:7:ggufv2:Q8_0 assay 6.24141 45 0 0.138698 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 assay 5.8662 45 0 0.13036 5 mistral-instruct-v0.2:7:ggufv2:Q2_K assay 5.84165 45 0 0.129814 5 mistral-instruct-v0.2:7:ggufv2:Q6_K assay 5.83272 45 0 0.129616 5 openhermes-2.5:7:ggufv2:Q5_K_M assay 5.77475 45 0 0.128328 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M assay 5.72421 45 0 0.127205 5 gpt-3.5-turbo-0613 assay 5.71717 45 0 0.127048 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M assay 5.66084 45 0 0.125797 5 gpt-3.5-turbo-0125 assay 5.48324 45 0 0.12185 5 gpt-4-0613 assay 5.47238 45 0 0.121608 5 openhermes-2.5:7:ggufv2:Q4_K_M assay 5.40473 45 0 0.120105 5 openhermes-2.5:7:ggufv2:Q3_K_M assay 4.99329 45 0 0.110962 5 openhermes-2.5:7:ggufv2:Q2_K assay 4.35689 45 0 0.0968198 5 llama-2-chat:7:ggufv2:Q6_K assay 2.34166 27 7.55411e-18 0.0867281 3 llama-2-chat:13:ggufv2:Q6_K assay 2.19772 27 3.77706e-18 0.081397 3 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M assay 3.17543 45 0 0.070565 5 llama-2-chat:7:ggufv2:Q8_0 assay 1.62311 27 0 0.0601152 3 llama-2-chat:70:ggufv2:Q4_K_M assay 1.8509 45 0 0.041131 5 llama-2-chat:70:ggufv2:Q5_K_M assay 1.81844 45 0 0.0404097 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K assay 1.68419 45 0 0.0374265 5 chatglm3:6:ggmlv3:q4_0 assay 1.61672 45 0 0.0359271 5 code-llama-instruct:7:ggufv2:Q4_K_M assay 1.53778 45 0 0.0341728 5 llama-3-instruct:8:ggufv2:Q6_K assay 1.48103 45 0 0.0329118 5 llama-3-instruct:8:ggufv2:Q8_0 assay 1.37088 45 0 0.0304641 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 assay 1.16327 45 0 0.0258505 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M assay 1.15926 45 0 0.0257612 5 llama-2-chat:70:ggufv2:Q2_K assay 1.15095 45 0 0.0255768 5 llama-2-chat:70:ggufv2:Q3_K_M assay 1.07788 45 0 0.023953 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M assay 1.05347 45 0 0.0234104 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K assay 1.02909 45 0 0.0228686 5 llama-2-chat:13:ggufv2:Q2_K assay 0.974441 45 0 0.0216542 5 llama-3-instruct:8:ggufv2:Q5_K_M assay 0.922706 45 0 0.0205046 5 llama-2-chat:7:ggufv2:Q5_K_M assay 0.919259 45 0 0.020428 5 llama-2-chat:13:ggufv2:Q5_K_M assay 0.836349 45 0 0.0185855 5 llama-2-chat:13:ggufv2:Q8_0 assay 0.756302 45 0 0.0168067 5 llama-2-chat:13:ggufv2:Q3_K_M assay 0.750557 45 0 0.016679 5 llama-2-chat:13:ggufv2:Q4_K_M assay 0.647223 45 0 0.0143827 5 llama-2-chat:7:ggufv2:Q4_K_M assay 0.604799 45 0 0.01344 5 llama-3-instruct:8:ggufv2:Q4_K_M assay 0.522273 45 0 0.0116061 5 llama-2-chat:7:ggufv2:Q3_K_M assay 0.455699 45 0 0.0101266 5 llama-2-chat:7:ggufv2:Q2_K assay 0.233824 45 0 0.00519608 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations gpt-4-0613 chemical 6.38889 45 0 0.141975 5 gpt-4-0125-preview chemical 6.22222 45 0 0.138272 5 openhermes-2.5:7:ggufv2:Q6_K chemical 6.16667 45 0 0.137037 5 gpt-4o-2024-05-13 chemical 5.55556 45 0 0.123457 5 gpt-3.5-turbo-0613 chemical 5.44444 45 0 0.120988 5 openhermes-2.5:7:ggufv2:Q3_K_M chemical 5.23309 45 0 0.116291 5 openhermes-2.5:7:ggufv2:Q8_0 chemical 5.16667 45 0 0.114815 5 openhermes-2.5:7:ggufv2:Q5_K_M chemical 5.06667 45 0 0.112593 5 gpt-3.5-turbo-0125 chemical 5.06444 45 0 0.112543 5 openhermes-2.5:7:ggufv2:Q4_K_M chemical 4.95556 45 0 0.110123 5 openhermes-2.5:7:ggufv2:Q2_K chemical 4.66667 45 0 0.103704 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M chemical 4.02332 45 0 0.0894072 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M chemical 3.69824 45 0 0.0821832 5 mistral-instruct-v0.2:7:ggufv2:Q6_K chemical 3.5588 45 0 0.0790845 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M chemical 3.23175 45 0 0.0718166 5 mistral-instruct-v0.2:7:ggufv2:Q2_K chemical 2.9648 45 0 0.0658845 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M chemical 2.85926 45 0 0.0635392 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 chemical 2.80214 45 0 0.0622698 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K chemical 2.28839 45 0 0.050853 5 llama-2-chat:13:ggufv2:Q6_K chemical 1.33748 27 0 0.0495362 3 llama-3-instruct:8:ggufv2:Q6_K chemical 1.99259 45 0 0.0442798 5 llama-3-instruct:8:ggufv2:Q5_K_M chemical 1.98451 45 0 0.0441003 5 llama-3-instruct:8:ggufv2:Q8_0 chemical 1.98451 45 0 0.0441003 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M chemical 1.92687 45 0 0.0428194 5 llama-2-chat:70:ggufv2:Q2_K chemical 1.92403 45 0 0.0427562 5 llama-2-chat:70:ggufv2:Q4_K_M chemical 1.86594 45 0 0.0414653 5 llama-2-chat:7:ggufv2:Q8_0 chemical 1.11429 27 3.77706e-18 0.0412698 3 llama-2-chat:70:ggufv2:Q5_K_M chemical 1.7972 45 0 0.0399378 5 llama-2-chat:70:ggufv2:Q3_K_M chemical 1.65417 45 0 0.0367593 5 llama-2-chat:13:ggufv2:Q4_K_M chemical 1.60885 45 0 0.0357522 5 llama-2-chat:7:ggufv2:Q6_K chemical 0.85 27 1.88853e-18 0.0314815 3 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K chemical 1.37178 45 0 0.030484 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 chemical 1.02473 45 0 0.0227718 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M chemical 0.993896 45 0 0.0220866 5 llama-3-instruct:8:ggufv2:Q4_K_M chemical 0.920791 45 0 0.020462 5 chatglm3:6:ggmlv3:q4_0 chemical 0.839293 45 0 0.018651 5 llama-2-chat:7:ggufv2:Q5_K_M chemical 0.580952 45 0 0.0129101 5 llama-2-chat:13:ggufv2:Q5_K_M chemical 0.473978 45 0 0.0105328 5 llama-2-chat:13:ggufv2:Q8_0 chemical 0.473978 45 0 0.0105328 5 llama-2-chat:13:ggufv2:Q3_K_M chemical 0.447004 45 0 0.00993343 5 code-llama-instruct:7:ggufv2:Q4_K_M chemical 0.44189 45 0 0.00981978 5 llama-2-chat:13:ggufv2:Q2_K chemical 0.429118 45 0 0.00953595 5 llama-2-chat:7:ggufv2:Q4_K_M chemical 0.416702 45 0 0.00926004 5 llama-2-chat:7:ggufv2:Q3_K_M chemical 0.270151 45 0 0.00600336 5 llama-2-chat:7:ggufv2:Q2_K chemical 0.264943 45 0 0.00588762 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations llama-2-chat:7:ggufv2:Q8_0 context 5.70797 27 0 0.211406 3 llama-2-chat:13:ggufv2:Q6_K context 4.88293 27 1.69967e-17 0.180849 3 gpt-4-0613 context 7.90663 45 0 0.175703 5 gpt-4-0125-preview context 7.85253 45 0 0.174501 5 gpt-4o-2024-05-13 context 7.82965 45 0 0.173992 5 gpt-3.5-turbo-0125 context 6.89247 45 0 0.153166 5 openhermes-2.5:7:ggufv2:Q4_K_M context 6.89055 45 0 0.153123 5 openhermes-2.5:7:ggufv2:Q6_K context 6.79989 45 0 0.151109 5 openhermes-2.5:7:ggufv2:Q3_K_M context 6.77271 45 0 0.150505 5 openhermes-2.5:7:ggufv2:Q8_0 context 6.67749 45 0 0.148389 5 gpt-3.5-turbo-0613 context 6.50472 45 0 0.144549 5 openhermes-2.5:7:ggufv2:Q5_K_M context 6.44769 45 0 0.143282 5 llama-2-chat:7:ggufv2:Q6_K context 3.73057 27 0 0.138169 3 mistral-instruct-v0.2:7:ggufv2:Q8_0 context 5.16754 45 0 0.114834 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M context 5.12599 45 0 0.113911 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M context 5.02844 45 0 0.111743 5 mistral-instruct-v0.2:7:ggufv2:Q6_K context 5.0158 45 0 0.111462 5 mistral-instruct-v0.2:7:ggufv2:Q2_K context 4.99362 45 0 0.110969 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K context 4.51314 45 0 0.100292 5 llama-2-chat:70:ggufv2:Q3_K_M context 4.22332 45 0 0.0938516 5 llama-2-chat:70:ggufv2:Q4_K_M context 4.10284 45 0 0.0911743 5 llama-2-chat:70:ggufv2:Q2_K context 4.08979 45 0 0.0908843 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M context 4.06318 45 0 0.090293 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M context 4.01117 45 0 0.0891372 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M context 3.90982 45 0 0.0868849 5 openhermes-2.5:7:ggufv2:Q2_K context 3.86897 45 0 0.0859772 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M context 3.79416 45 0 0.0843146 5 llama-2-chat:70:ggufv2:Q5_K_M context 3.74591 45 0 0.0832424 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 context 3.70126 45 0 0.0822502 5 code-llama-instruct:7:ggufv2:Q4_K_M context 3.32657 45 0 0.0739237 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K context 3.1452 45 0 0.0698933 5 chatglm3:6:ggmlv3:q4_0 context 2.85636 45 0 0.0634747 5 llama-2-chat:7:ggufv2:Q3_K_M context 2.10857 45 0 0.046857 5 llama-2-chat:7:ggufv2:Q4_K_M context 1.89605 45 0 0.0421345 5 llama-2-chat:13:ggufv2:Q3_K_M context 1.78868 45 0 0.0397484 5 llama-2-chat:13:ggufv2:Q5_K_M context 1.78618 45 0 0.0396929 5 llama-2-chat:13:ggufv2:Q4_K_M context 1.77351 45 0 0.0394113 5 llama-3-instruct:8:ggufv2:Q8_0 context 1.67334 45 0 0.0371853 5 llama-3-instruct:8:ggufv2:Q5_K_M context 1.64821 45 0 0.0366268 5 llama-2-chat:13:ggufv2:Q8_0 context 1.58821 45 0 0.0352936 5 llama-3-instruct:8:ggufv2:Q4_K_M context 1.57169 45 0 0.0349264 5 llama-2-chat:13:ggufv2:Q2_K context 1.34289 45 0 0.0298419 5 llama-2-chat:7:ggufv2:Q5_K_M context 1.23881 45 0 0.0275291 5 llama-2-chat:7:ggufv2:Q2_K context 1.12335 45 0 0.0249632 5 llama-3-instruct:8:ggufv2:Q6_K context 1.10292 45 0 0.0245094 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations openhermes-2.5:7:ggufv2:Q6_K disease 6.46667 45 0 0.143704 5 openhermes-2.5:7:ggufv2:Q4_K_M disease 6.46667 45 0 0.143704 5 openhermes-2.5:7:ggufv2:Q5_K_M disease 6.46667 45 0 0.143704 5 openhermes-2.5:7:ggufv2:Q8_0 disease 6.46667 45 0 0.143704 5 openhermes-2.5:7:ggufv2:Q3_K_M disease 6.46667 45 0 0.143704 5 gpt-4-0125-preview disease 6.21333 45 0 0.138074 5 gpt-4o-2024-05-13 disease 6.2 45 0 0.137778 5 gpt-4-0613 disease 6.13333 45 0 0.136296 5 gpt-3.5-turbo-0613 disease 6.06667 45 0 0.134815 5 gpt-3.5-turbo-0125 disease 4.75238 45 0 0.105608 5 openhermes-2.5:7:ggufv2:Q2_K disease 4.32493 45 0 0.0961096 5 mistral-instruct-v0.2:7:ggufv2:Q2_K disease 4.20708 45 0 0.0934906 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K disease 4.14674 45 0 0.0921497 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M disease 4.02927 45 0 0.0895392 5 mistral-instruct-v0.2:7:ggufv2:Q6_K disease 4.01581 45 0 0.0892402 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 disease 3.47244 45 0 0.0771654 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M disease 3.04532 45 0 0.0676737 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M disease 2.92854 45 0 0.0650787 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M disease 2.65437 45 0 0.0589859 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 disease 2.57657 45 0 0.057257 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M disease 2.44785 45 0 0.0543966 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M disease 2.29171 45 0 0.0509269 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K disease 2.29094 45 0 0.0509099 5 llama-3-instruct:8:ggufv2:Q8_0 disease 1.73452 45 0 0.0385449 5 llama-3-instruct:8:ggufv2:Q6_K disease 1.73452 45 0 0.0385449 5 llama-3-instruct:8:ggufv2:Q5_K_M disease 1.73452 45 0 0.0385449 5 llama-2-chat:13:ggufv2:Q6_K disease 0.827524 27 0 0.030649 3 code-llama-instruct:7:ggufv2:Q4_K_M disease 1.33093 45 0 0.0295762 5 chatglm3:6:ggmlv3:q4_0 disease 1.21669 45 0 0.0270376 5 llama-3-instruct:8:ggufv2:Q4_K_M disease 0.995894 45 0 0.022131 5 llama-2-chat:7:ggufv2:Q8_0 disease 0.444887 27 2.36066e-19 0.0164773 3 llama-2-chat:7:ggufv2:Q6_K disease 0.439254 27 0 0.0162687 3 llama-2-chat:13:ggufv2:Q5_K_M disease 0.306386 45 0 0.00680858 5 llama-2-chat:13:ggufv2:Q8_0 disease 0.26663 45 0 0.00592511 5 llama-2-chat:13:ggufv2:Q4_K_M disease 0.250053 45 0 0.00555673 5 llama-2-chat:70:ggufv2:Q5_K_M disease 0.235648 45 0 0.00523663 5 llama-2-chat:7:ggufv2:Q3_K_M disease 0.185035 45 0 0.0041119 5 llama-2-chat:70:ggufv2:Q2_K disease 0.182046 45 0 0.00404548 5 llama-2-chat:70:ggufv2:Q4_K_M disease 0.179398 45 0 0.00398663 5 llama-2-chat:7:ggufv2:Q5_K_M disease 0.150208 45 0 0.00333795 5 llama-2-chat:70:ggufv2:Q3_K_M disease 0.142957 45 0 0.00317683 5 llama-2-chat:13:ggufv2:Q3_K_M disease 0.103277 45 0 0.00229505 5 llama-2-chat:7:ggufv2:Q4_K_M disease 0.0898052 45 0 0.00199567 5 llama-2-chat:13:ggufv2:Q2_K disease 0.0874203 45 0 0.00194267 5 llama-2-chat:7:ggufv2:Q2_K disease 0.0587138 45 0 0.00130475 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations gpt-4o-2024-05-13 entity 5.9909 45 0 0.133131 5 gpt-4-0125-preview entity 4.59502 45 0 0.102112 5 gpt-3.5-turbo-0613 entity 4.57972 45 0 0.101772 5 openhermes-2.5:7:ggufv2:Q4_K_M entity 4.22461 45 0 0.0938803 5 openhermes-2.5:7:ggufv2:Q8_0 entity 4.1344 45 0 0.0918755 5 gpt-4-0613 entity 4.12852 45 0 0.0917448 5 openhermes-2.5:7:ggufv2:Q6_K entity 4.09333 45 0 0.0909629 5 openhermes-2.5:7:ggufv2:Q5_K_M entity 4.02016 45 0 0.0893369 5 gpt-3.5-turbo-0125 entity 3.71195 45 0 0.0824877 5 openhermes-2.5:7:ggufv2:Q3_K_M entity 3.65819 45 0 0.0812932 5 llama-2-chat:13:ggufv2:Q6_K entity 2.14189 27 0 0.0793293 3 llama-2-chat:7:ggufv2:Q6_K entity 2.07106 27 9.44264e-19 0.0767059 3 llama-2-chat:7:ggufv2:Q8_0 entity 1.79733 27 0 0.0665678 3 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M entity 2.42313 45 0 0.0538473 5 openhermes-2.5:7:ggufv2:Q2_K entity 2.33413 45 0 0.0518696 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M entity 2.30597 45 0 0.0512437 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M entity 2.20283 45 0 0.0489518 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K entity 2.10077 45 0 0.0466838 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M entity 2.0607 45 0 0.0457934 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 entity 2.00802 45 0 0.0446226 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M entity 1.99809 45 0 0.044402 5 mistral-instruct-v0.2:7:ggufv2:Q6_K entity 1.99214 45 0 0.0442699 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 entity 1.79999 45 0 0.0399998 5 mistral-instruct-v0.2:7:ggufv2:Q2_K entity 1.77563 45 0 0.0394584 5 chatglm3:6:ggmlv3:q4_0 entity 1.22227 45 0 0.0271617 5 llama-2-chat:70:ggufv2:Q3_K_M entity 1.20851 45 0 0.0268558 5 llama-2-chat:70:ggufv2:Q2_K entity 1.16189 45 0 0.0258197 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M entity 1.10007 45 0 0.0244461 5 llama-2-chat:70:ggufv2:Q4_K_M entity 1.01555 45 0 0.0225677 5 code-llama-instruct:7:ggufv2:Q4_K_M entity 0.948961 45 0 0.021088 5 llama-2-chat:70:ggufv2:Q5_K_M entity 0.903324 45 0 0.0200739 5 llama-2-chat:13:ggufv2:Q2_K entity 0.807379 45 0 0.0179418 5 llama-2-chat:13:ggufv2:Q4_K_M entity 0.785233 45 0 0.0174496 5 llama-3-instruct:8:ggufv2:Q5_K_M entity 0.75253 45 0 0.0167229 5 llama-3-instruct:8:ggufv2:Q6_K entity 0.749495 45 0 0.0166554 5 llama-2-chat:7:ggufv2:Q3_K_M entity 0.699988 45 0 0.0155553 5 llama-3-instruct:8:ggufv2:Q8_0 entity 0.695524 45 0 0.0154561 5 llama-3-instruct:8:ggufv2:Q4_K_M entity 0.694377 45 0 0.0154306 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K entity 0.685368 45 0 0.0152304 5 llama-2-chat:7:ggufv2:Q4_K_M entity 0.685027 45 0 0.0152228 5 llama-2-chat:13:ggufv2:Q8_0 entity 0.629764 45 0 0.0139947 5 llama-2-chat:7:ggufv2:Q5_K_M entity 0.623851 45 0 0.0138634 5 llama-2-chat:13:ggufv2:Q5_K_M entity 0.623813 45 0 0.0138625 5 llama-2-chat:13:ggufv2:Q3_K_M entity 0.56502 45 0 0.012556 5 llama-2-chat:7:ggufv2:Q2_K entity 0.318196 45 0 0.00707101 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations openhermes-2.5:7:ggufv2:Q2_K experiment_yes_or_no 9 45 0 0.2 5 gpt-4-0125-preview experiment_yes_or_no 9 45 0 0.2 5 llama-2-chat:70:ggufv2:Q4_K_M experiment_yes_or_no 9 45 0 0.2 5 chatglm3:6:ggmlv3:q4_0 experiment_yes_or_no 8.6 45 0 0.191111 5 openhermes-2.5:7:ggufv2:Q5_K_M experiment_yes_or_no 8.33333 45 0 0.185185 5 openhermes-2.5:7:ggufv2:Q6_K experiment_yes_or_no 8.33333 45 0 0.185185 5 openhermes-2.5:7:ggufv2:Q4_K_M experiment_yes_or_no 8.33333 45 0 0.185185 5 llama-2-chat:70:ggufv2:Q5_K_M experiment_yes_or_no 8.025 45 0 0.178333 5 gpt-4o-2024-05-13 experiment_yes_or_no 8 45 0 0.177778 5 openhermes-2.5:7:ggufv2:Q3_K_M experiment_yes_or_no 8 45 0 0.177778 5 gpt-4-0613 experiment_yes_or_no 8 45 0 0.177778 5 gpt-3.5-turbo-0613 experiment_yes_or_no 8 45 0 0.177778 5 openhermes-2.5:7:ggufv2:Q8_0 experiment_yes_or_no 8 45 0 0.177778 5 llama-2-chat:7:ggufv2:Q8_0 experiment_yes_or_no 4.67535 27 0 0.173161 3 llama-2-chat:70:ggufv2:Q2_K experiment_yes_or_no 7.05061 45 0 0.15668 5 llama-2-chat:70:ggufv2:Q3_K_M experiment_yes_or_no 6.07336 45 0 0.134964 5 gpt-3.5-turbo-0125 experiment_yes_or_no 6.03333 45 0 0.134074 5 llama-2-chat:13:ggufv2:Q6_K experiment_yes_or_no 3.25916 27 9.44264e-19 0.12071 3 mistral-instruct-v0.2:7:ggufv2:Q3_K_M experiment_yes_or_no 5.23564 45 0 0.116348 5 llama-2-chat:13:ggufv2:Q3_K_M experiment_yes_or_no 5.16593 45 0 0.114799 5 llama-3-instruct:8:ggufv2:Q8_0 experiment_yes_or_no 3.7 45 0 0.0822222 5 llama-3-instruct:8:ggufv2:Q5_K_M experiment_yes_or_no 3.68182 45 0 0.0818182 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 experiment_yes_or_no 3.32028 45 0 0.073784 5 llama-2-chat:7:ggufv2:Q6_K experiment_yes_or_no 1.97565 27 7.08198e-19 0.0731722 3 mistral-instruct-v0.2:7:ggufv2:Q5_K_M experiment_yes_or_no 3.26963 45 0 0.0726584 5 code-llama-instruct:7:ggufv2:Q4_K_M experiment_yes_or_no 3.0913 45 0 0.0686956 5 llama-3-instruct:8:ggufv2:Q6_K experiment_yes_or_no 2.36364 45 0 0.0525253 5 mistral-instruct-v0.2:7:ggufv2:Q6_K experiment_yes_or_no 2.36015 45 0 0.0524479 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M experiment_yes_or_no 2.2851 45 0 0.05078 5 mistral-instruct-v0.2:7:ggufv2:Q2_K experiment_yes_or_no 2.2802 45 0 0.0506711 5 llama-2-chat:7:ggufv2:Q4_K_M experiment_yes_or_no 2.06817 45 0 0.0459593 5 llama-3-instruct:8:ggufv2:Q4_K_M experiment_yes_or_no 1.89935 45 0 0.0422078 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M experiment_yes_or_no 1.45686 45 0 0.0323746 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M experiment_yes_or_no 1.29991 45 0 0.0288868 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M experiment_yes_or_no 1.1661 45 0 0.0259134 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K experiment_yes_or_no 1.15184 45 0 0.0255965 5 llama-2-chat:13:ggufv2:Q8_0 experiment_yes_or_no 1.06643 45 0 0.0236984 5 llama-2-chat:13:ggufv2:Q5_K_M experiment_yes_or_no 1.03147 45 0 0.0229215 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K experiment_yes_or_no 0.785587 45 0 0.0174575 5 llama-2-chat:7:ggufv2:Q3_K_M experiment_yes_or_no 0.726745 45 0 0.0161499 5 llama-2-chat:7:ggufv2:Q5_K_M experiment_yes_or_no 0.618798 45 0 0.0137511 5 llama-2-chat:13:ggufv2:Q4_K_M experiment_yes_or_no 0.468722 45 0 0.010416 5 llama-2-chat:13:ggufv2:Q2_K experiment_yes_or_no 0.267272 45 0 0.00593938 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 experiment_yes_or_no 0.201489 45 0 0.00447753 5 llama-2-chat:7:ggufv2:Q2_K experiment_yes_or_no 0.130285 45 0 0.00289522 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations llama-2-chat:7:ggufv2:Q8_0 hypothesis 2.854 27 3.77706e-18 0.105704 3 mistral-instruct-v0.2:7:ggufv2:Q4_K_M hypothesis 3.67339 45 0 0.0816309 5 llama-2-chat:7:ggufv2:Q6_K hypothesis 2.01944 27 5.19345e-18 0.074794 3 mistral-instruct-v0.2:7:ggufv2:Q6_K hypothesis 3.33681 45 0 0.0741512 5 gpt-4-0613 hypothesis 3.29696 45 0 0.0732657 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 hypothesis 2.9272 45 0 0.0650489 5 gpt-4o-2024-05-13 hypothesis 2.89512 45 0 0.064336 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M hypothesis 2.75585 45 0 0.0612411 5 gpt-3.5-turbo-0125 hypothesis 2.72775 45 0 0.0606168 5 llama-2-chat:13:ggufv2:Q6_K hypothesis 1.61253 27 1.88853e-18 0.0597233 3 gpt-3.5-turbo-0613 hypothesis 2.64497 45 0 0.0587771 5 openhermes-2.5:7:ggufv2:Q4_K_M hypothesis 2.57382 45 0 0.0571961 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M hypothesis 2.47292 45 0 0.0549539 5 openhermes-2.5:7:ggufv2:Q8_0 hypothesis 2.37196 45 0 0.0527103 5 gpt-4-0125-preview hypothesis 2.33518 45 0 0.051893 5 openhermes-2.5:7:ggufv2:Q6_K hypothesis 2.29085 45 0 0.0509077 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M hypothesis 2.23255 45 0 0.0496122 5 openhermes-2.5:7:ggufv2:Q3_K_M hypothesis 2.09626 45 0 0.0465835 5 mistral-instruct-v0.2:7:ggufv2:Q2_K hypothesis 2.05375 45 0 0.045639 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M hypothesis 1.87442 45 0 0.0416537 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 hypothesis 1.83735 45 0 0.04083 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M hypothesis 1.71557 45 0 0.0381237 5 openhermes-2.5:7:ggufv2:Q5_K_M hypothesis 1.52181 45 0 0.033818 5 openhermes-2.5:7:ggufv2:Q2_K hypothesis 1.4915 45 0 0.0331444 5 llama-2-chat:70:ggufv2:Q3_K_M hypothesis 1.44143 45 0 0.0320317 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K hypothesis 1.44009 45 0 0.032002 5 llama-2-chat:70:ggufv2:Q2_K hypothesis 1.4389 45 0 0.0319755 5 llama-2-chat:70:ggufv2:Q4_K_M hypothesis 1.41421 45 0 0.0314268 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K hypothesis 1.39565 45 0 0.0310144 5 llama-3-instruct:8:ggufv2:Q4_K_M hypothesis 1.13596 45 0 0.0252436 5 chatglm3:6:ggmlv3:q4_0 hypothesis 0.98676 45 0 0.021928 5 llama-3-instruct:8:ggufv2:Q8_0 hypothesis 0.878406 45 0 0.0195201 5 llama-3-instruct:8:ggufv2:Q6_K hypothesis 0.876219 45 0 0.0194715 5 llama-2-chat:7:ggufv2:Q5_K_M hypothesis 0.68638 45 0 0.0152529 5 llama-2-chat:70:ggufv2:Q5_K_M hypothesis 0.623758 45 0 0.0138613 5 llama-2-chat:7:ggufv2:Q4_K_M hypothesis 0.62053 45 0 0.0137896 5 llama-3-instruct:8:ggufv2:Q5_K_M hypothesis 0.604423 45 0 0.0134316 5 code-llama-instruct:7:ggufv2:Q4_K_M hypothesis 0.572369 45 0 0.0127193 5 llama-2-chat:13:ggufv2:Q8_0 hypothesis 0.55524 45 0 0.0123387 5 llama-2-chat:7:ggufv2:Q2_K hypothesis 0.520453 45 0 0.0115656 5 llama-2-chat:13:ggufv2:Q2_K hypothesis 0.49279 45 0 0.0109509 5 llama-2-chat:13:ggufv2:Q3_K_M hypothesis 0.424638 45 0 0.00943639 5 llama-2-chat:13:ggufv2:Q5_K_M hypothesis 0.408017 45 0 0.00906704 5 llama-2-chat:7:ggufv2:Q3_K_M hypothesis 0.402337 45 0 0.00894082 5 llama-2-chat:13:ggufv2:Q4_K_M hypothesis 0.366299 45 0 0.00813997 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations gpt-4o-2024-05-13 intervention 5.34631 45 0 0.118807 5 openhermes-2.5:7:ggufv2:Q4_K_M intervention 4.9841 45 0 0.110758 5 gpt-4-0125-preview intervention 4.92171 45 0 0.109371 5 gpt-4-0613 intervention 4.72253 45 0 0.104945 5 openhermes-2.5:7:ggufv2:Q6_K intervention 4.71449 45 0 0.104767 5 openhermes-2.5:7:ggufv2:Q8_0 intervention 4.44465 45 0 0.09877 5 gpt-3.5-turbo-0613 intervention 4.27143 45 0 0.0949206 5 openhermes-2.5:7:ggufv2:Q5_K_M intervention 4.00021 45 0 0.0888935 5 gpt-3.5-turbo-0125 intervention 3.75141 45 0 0.0833647 5 openhermes-2.5:7:ggufv2:Q3_K_M intervention 3.55238 45 0 0.0789418 5 openhermes-2.5:7:ggufv2:Q2_K intervention 2.92766 45 0 0.0650591 5 llama-2-chat:7:ggufv2:Q8_0 intervention 1.56417 27 0 0.0579323 3 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K intervention 2.23683 45 0 0.0497073 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M intervention 2.23319 45 0 0.0496264 5 llama-2-chat:13:ggufv2:Q6_K intervention 1.13241 27 0.000274145 0.0419412 3 mistral-instruct-v0.2:7:ggufv2:Q3_K_M intervention 1.66677 45 0 0.0370393 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M intervention 1.23412 45 0 0.0274249 5 code-llama-instruct:7:ggufv2:Q4_K_M intervention 1.17173 45 0 0.0260384 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M intervention 1.15754 45 0 0.025723 5 llama-2-chat:13:ggufv2:Q4_K_M intervention 1.02157 45 0 0.0227015 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 intervention 0.987919 45 0 0.0219538 5 chatglm3:6:ggmlv3:q4_0 intervention 0.881806 45 0 0.0195957 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 intervention 0.879646 45 0 0.0195477 5 llama-2-chat:7:ggufv2:Q6_K intervention 0.514286 27 3.77706e-18 0.0190476 3 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M intervention 0.723791 45 0 0.0160842 5 mistral-instruct-v0.2:7:ggufv2:Q2_K intervention 0.680182 45 0 0.0151152 5 llama-2-chat:70:ggufv2:Q2_K intervention 0.668995 45 0 0.0148666 5 mistral-instruct-v0.2:7:ggufv2:Q6_K intervention 0.640258 45 0 0.0142279 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M intervention 0.550643 45 0 0.0122365 5 llama-2-chat:70:ggufv2:Q5_K_M intervention 0.542302 45 0 0.0120512 5 llama-2-chat:13:ggufv2:Q2_K intervention 0.502722 45 0 0.0111716 5 llama-2-chat:70:ggufv2:Q4_K_M intervention 0.417501 45 0 0.00927779 5 llama-2-chat:7:ggufv2:Q3_K_M intervention 0.416756 45 0 0.00926124 5 llama-3-instruct:8:ggufv2:Q5_K_M intervention 0.410888 45 0 0.00913085 5 llama-2-chat:70:ggufv2:Q3_K_M intervention 0.402319 45 0 0.00894042 5 llama-3-instruct:8:ggufv2:Q4_K_M intervention 0.37923 45 0 0.00842733 5 llama-2-chat:13:ggufv2:Q5_K_M intervention 0.339683 45 0 0.0075485 5 llama-3-instruct:8:ggufv2:Q6_K intervention 0.327257 45 0 0.00727237 5 llama-3-instruct:8:ggufv2:Q8_0 intervention 0.319187 45 0 0.00709304 5 llama-2-chat:13:ggufv2:Q3_K_M intervention 0.265476 45 0 0.00589947 5 llama-2-chat:7:ggufv2:Q5_K_M intervention 0.24986 45 0 0.00555244 5 llama-2-chat:13:ggufv2:Q8_0 intervention 0.244444 45 0 0.0054321 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K intervention 0.2273 45 0 0.0050511 5 llama-2-chat:7:ggufv2:Q2_K intervention 0.118691 45 0 0.00263758 5 llama-2-chat:7:ggufv2:Q4_K_M intervention 0.0769231 45 0 0.0017094 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations gpt-4-0125-preview ncbi_link 6.48768 45 0 0.144171 5 gpt-4-0613 ncbi_link 6.05933 45 0 0.134652 5 openhermes-2.5:7:ggufv2:Q8_0 ncbi_link 3.5303 45 0 0.0784512 5 openhermes-2.5:7:ggufv2:Q6_K ncbi_link 3.5303 45 0 0.0784512 5 gpt-4o-2024-05-13 ncbi_link 3.51302 45 0 0.078067 5 openhermes-2.5:7:ggufv2:Q5_K_M ncbi_link 3.47436 45 0 0.077208 5 openhermes-2.5:7:ggufv2:Q4_K_M ncbi_link 3.11111 45 0 0.0691358 5 openhermes-2.5:7:ggufv2:Q3_K_M ncbi_link 2.37436 45 0 0.0527635 5 gpt-3.5-turbo-0613 ncbi_link 2.16667 45 0 0.0481481 5 gpt-3.5-turbo-0125 ncbi_link 1.42925 45 0 0.031761 5 llama-2-chat:13:ggufv2:Q6_K ncbi_link 0.690904 27 0 0.0255891 3 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M ncbi_link 1.03429 45 0 0.0229841 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M ncbi_link 0.884957 45 0 0.0196657 5 mistral-instruct-v0.2:7:ggufv2:Q2_K ncbi_link 0.881705 45 0 0.0195934 5 llama-2-chat:7:ggufv2:Q6_K ncbi_link 0.507313 27 9.44264e-19 0.0187894 3 mistral-instruct-v0.2:7:ggufv2:Q5_K_M ncbi_link 0.710989 45 0 0.0157998 5 llama-2-chat:7:ggufv2:Q8_0 ncbi_link 0.410766 27 9.44264e-19 0.0152135 3 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K ncbi_link 0.656812 45 0 0.0145958 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M ncbi_link 0.615714 45 0 0.0136825 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 ncbi_link 0.596131 45 0 0.0132474 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M ncbi_link 0.574422 45 0 0.0127649 5 mistral-instruct-v0.2:7:ggufv2:Q6_K ncbi_link 0.558824 45 0 0.0124183 5 openhermes-2.5:7:ggufv2:Q2_K ncbi_link 0.505458 45 0 0.0112324 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 ncbi_link 0.429927 45 0 0.00955394 5 code-llama-instruct:7:ggufv2:Q4_K_M ncbi_link 0.328564 45 0 0.00730142 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K ncbi_link 0.271548 45 0 0.0060344 5 llama-2-chat:13:ggufv2:Q8_0 ncbi_link 0.255217 45 0 0.00567148 5 llama-2-chat:70:ggufv2:Q2_K ncbi_link 0.253735 45 0 0.00563856 5 llama-2-chat:13:ggufv2:Q4_K_M ncbi_link 0.246231 45 0 0.00547179 5 llama-2-chat:70:ggufv2:Q4_K_M ncbi_link 0.241357 45 0 0.00536348 5 llama-2-chat:13:ggufv2:Q5_K_M ncbi_link 0.236802 45 0 0.00526226 5 llama-3-instruct:8:ggufv2:Q4_K_M ncbi_link 0.233815 45 0 0.00519589 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M ncbi_link 0.230909 45 0 0.00513131 5 llama-2-chat:7:ggufv2:Q4_K_M ncbi_link 0.216341 45 0 0.00480757 5 llama-2-chat:70:ggufv2:Q5_K_M ncbi_link 0.196981 45 0 0.00437735 5 llama-2-chat:13:ggufv2:Q2_K ncbi_link 0.192574 45 0 0.00427942 5 llama-3-instruct:8:ggufv2:Q8_0 ncbi_link 0.179211 45 0 0.00398247 5 llama-2-chat:7:ggufv2:Q3_K_M ncbi_link 0.177339 45 0 0.00394087 5 llama-3-instruct:8:ggufv2:Q6_K ncbi_link 0.173014 45 0 0.00384476 5 llama-2-chat:7:ggufv2:Q5_K_M ncbi_link 0.170952 45 0 0.00379894 5 llama-2-chat:70:ggufv2:Q3_K_M ncbi_link 0.166777 45 0 0.00370615 5 llama-3-instruct:8:ggufv2:Q5_K_M ncbi_link 0.166614 45 0 0.00370254 5 llama-2-chat:7:ggufv2:Q2_K ncbi_link 0.15271 45 0 0.00339354 5 llama-2-chat:13:ggufv2:Q3_K_M ncbi_link 0.150011 45 0 0.00333359 5 chatglm3:6:ggmlv3:q4_0 ncbi_link 0.122857 45 0 0.00273017 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations gpt-4-0613 significance 5.6 45 0 0.124444 5 gpt-4-0125-preview significance 5.18384 45 0 0.115196 5 gpt-4o-2024-05-13 significance 4.22424 45 0 0.0938721 5 openhermes-2.5:7:ggufv2:Q4_K_M significance 3.92996 45 0 0.0873325 5 openhermes-2.5:7:ggufv2:Q6_K significance 3.78182 45 0 0.0840404 5 openhermes-2.5:7:ggufv2:Q8_0 significance 3.78182 45 0 0.0840404 5 openhermes-2.5:7:ggufv2:Q5_K_M significance 3.77787 45 0 0.0839526 5 openhermes-2.5:7:ggufv2:Q3_K_M significance 3.69091 45 0 0.0820202 5 gpt-3.5-turbo-0613 significance 3.58562 45 0 0.0796804 5 gpt-3.5-turbo-0125 significance 3.51717 45 0 0.0781594 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M significance 2.93833 45 0 0.0652963 5 mistral-instruct-v0.2:7:ggufv2:Q6_K significance 2.87928 45 0 0.063984 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M significance 2.79423 45 0 0.062094 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 significance 2.62296 45 0 0.0582881 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M significance 2.56724 45 0 0.0570498 5 openhermes-2.5:7:ggufv2:Q2_K significance 2.48514 45 0 0.0552254 5 mistral-instruct-v0.2:7:ggufv2:Q2_K significance 2.4813 45 0 0.05514 5 llama-2-chat:7:ggufv2:Q8_0 significance 1.10159 27 0 0.0407996 3 llama-2-chat:13:ggufv2:Q6_K significance 1.07015 27 1.0623e-18 0.0396352 3 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 significance 1.50696 45 0 0.033488 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K significance 1.34869 45 0 0.0299709 5 llama-2-chat:7:ggufv2:Q6_K significance 0.806474 27 0 0.0298694 3 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M significance 1.31454 45 0 0.0292119 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M significance 1.2312 45 0 0.0273599 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M significance 1.01129 45 0 0.0224731 5 llama-3-instruct:8:ggufv2:Q6_K significance 0.994971 45 0 0.0221105 5 llama-3-instruct:8:ggufv2:Q8_0 significance 0.957259 45 0 0.0212724 5 llama-2-chat:70:ggufv2:Q3_K_M significance 0.758379 45 0 0.0168529 5 llama-2-chat:70:ggufv2:Q2_K significance 0.716547 45 0 0.0159233 5 llama-2-chat:70:ggufv2:Q4_K_M significance 0.68386 45 0 0.0151969 5 llama-3-instruct:8:ggufv2:Q5_K_M significance 0.636128 45 0 0.0141362 5 llama-2-chat:70:ggufv2:Q5_K_M significance 0.518572 45 0 0.0115238 5 llama-2-chat:7:ggufv2:Q4_K_M significance 0.329457 45 0 0.00732127 5 llama-2-chat:13:ggufv2:Q8_0 significance 0.326026 45 0 0.00724502 5 llama-2-chat:7:ggufv2:Q5_K_M significance 0.281188 45 0 0.00624862 5 llama-3-instruct:8:ggufv2:Q4_K_M significance 0.228461 45 0 0.00507691 5 llama-2-chat:13:ggufv2:Q4_K_M significance 0.213246 45 0 0.0047388 5 llama-2-chat:13:ggufv2:Q2_K significance 0.207957 45 0 0.00462127 5 llama-2-chat:13:ggufv2:Q5_K_M significance 0.205271 45 0 0.00456158 5 llama-2-chat:7:ggufv2:Q3_K_M significance 0.194946 45 0 0.00433214 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K significance 0.178078 45 0 0.00395728 5 llama-2-chat:13:ggufv2:Q3_K_M significance 0.131484 45 0 0.00292186 5 code-llama-instruct:7:ggufv2:Q4_K_M significance 0.123914 45 0 0.00275365 5 chatglm3:6:ggmlv3:q4_0 significance 0.118153 45 0 0.00262562 5 llama-2-chat:7:ggufv2:Q2_K significance 0.103278 45 0 0.00229507 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations gpt-4-0125-preview stats 8.86667 45 0 0.197037 5 openhermes-2.5:7:ggufv2:Q8_0 stats 8.66667 45 0 0.192593 5 openhermes-2.5:7:ggufv2:Q6_K stats 8.66667 45 0 0.192593 5 openhermes-2.5:7:ggufv2:Q5_K_M stats 8.52821 45 0 0.189516 5 gpt-4-0613 stats 8.51282 45 0 0.189174 5 gpt-4o-2024-05-13 stats 8.51282 45 0 0.189174 5 openhermes-2.5:7:ggufv2:Q4_K_M stats 8.25641 45 0 0.183476 5 openhermes-2.5:7:ggufv2:Q3_K_M stats 8.05641 45 0 0.179031 5 openhermes-2.5:7:ggufv2:Q2_K stats 8 45 0 0.177778 5 gpt-3.5-turbo-0613 stats 7.98135 45 0 0.177363 5 gpt-3.5-turbo-0125 stats 7.12976 45 0 0.158439 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M stats 6.89091 45 0 0.153131 5 llama-2-chat:13:ggufv2:Q6_K stats 4.05299 27 2.26623e-17 0.150111 3 llama-2-chat:7:ggufv2:Q8_0 stats 3.87128 27 7.55411e-18 0.143381 3 mistral-instruct-v0.2:7:ggufv2:Q4_K_M stats 6.29908 45 0 0.13998 5 mistral-instruct-v0.2:7:ggufv2:Q6_K stats 6.18322 45 0 0.137405 5 llama-3-instruct:8:ggufv2:Q8_0 stats 5.17406 45 0 0.114979 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M stats 5.1041 45 0 0.113424 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 stats 5.04591 45 0 0.112131 5 llama-2-chat:7:ggufv2:Q6_K stats 2.99816 27 7.55411e-18 0.111043 3 mistral-instruct-v0.2:7:ggufv2:Q2_K stats 4.63496 45 0 0.102999 5 llama-3-instruct:8:ggufv2:Q6_K stats 4.30739 45 0 0.0957198 5 llama-3-instruct:8:ggufv2:Q5_K_M stats 3.9346 45 0 0.0874356 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 stats 3.60737 45 0 0.0801638 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M stats 3.58841 45 0 0.0797425 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K stats 3.21213 45 0 0.0713807 5 llama-2-chat:70:ggufv2:Q4_K_M stats 3.08109 45 0 0.0684688 5 llama-3-instruct:8:ggufv2:Q4_K_M stats 2.98843 45 0 0.0664096 5 llama-2-chat:70:ggufv2:Q2_K stats 2.65216 45 0 0.0589368 5 llama-2-chat:70:ggufv2:Q3_K_M stats 2.44276 45 0 0.0542835 5 llama-2-chat:70:ggufv2:Q5_K_M stats 2.3993 45 0 0.0533177 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M stats 2.21549 45 0 0.049233 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M stats 1.96241 45 0 0.0436091 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K stats 1.76057 45 0 0.0391237 5 llama-2-chat:7:ggufv2:Q4_K_M stats 1.43589 45 0 0.0319086 5 llama-2-chat:13:ggufv2:Q4_K_M stats 1.41695 45 0 0.0314878 5 llama-2-chat:13:ggufv2:Q8_0 stats 1.38608 45 0 0.0308019 5 llama-2-chat:7:ggufv2:Q5_K_M stats 1.3859 45 0 0.0307977 5 llama-2-chat:13:ggufv2:Q3_K_M stats 1.35834 45 0 0.0301854 5 llama-2-chat:13:ggufv2:Q5_K_M stats 1.3371 45 0 0.0297134 5 llama-2-chat:13:ggufv2:Q2_K stats 1.12439 45 0 0.0249865 5 code-llama-instruct:7:ggufv2:Q4_K_M stats 0.860471 45 0 0.0191216 5 llama-2-chat:7:ggufv2:Q3_K_M stats 0.804538 45 0 0.0178786 5 llama-2-chat:7:ggufv2:Q2_K stats 0.558031 45 0 0.0124007 5 chatglm3:6:ggmlv3:q4_0 stats 0.17925 45 0 0.00398332 5 <p></p>"},{"location":"benchmark/","title":"Benchmark Results - Overview","text":"<p>Here we collect the results of the living BioChatter benchmark. For an explanation, see the benchmarking documentation and the developer docs for further reading.</p>"},{"location":"benchmark/#scores-per-model","title":"Scores per model","text":"<p>Table sorted by mean score in descending order. Click the column names to reorder.</p> Model name Size Median Accuracy SD gpt-3.5-turbo-0125 175 0.87 0.21 gpt-4-0613 Unknown 0.78 0.18 gpt-4-0125-preview Unknown 0.73 0.3 gpt-3.5-turbo-0613 175 0.73 0.24 openhermes-2.5 7 0.7 0.32 gpt-4o-2024-05-13 Unknown 0.7 0.35 llama-3-instruct 8 0.64 0.36 chatglm3 6 0.44 0.26 llama-2-chat 70 0.42 0.34 mistral-instruct-v0.2 7 0.4 0.33 code-llama-instruct 7 0.4 0.35 code-llama-instruct 34 0.38 0.35 code-llama-instruct 13 0.38 0.33 llama-2-chat 13 0.38 0.33 llama-2-chat 7 0.34 0.31 mixtral-instruct-v0.1 46,7 0.34 0.28 <p> </p>"},{"location":"benchmark/#scores-per-quantisation","title":"Scores per quantisation","text":"<p>Table sorted by mean score in descending order. Click the column names to reorder.</p> Model name Size Version Quantisation Median Accuracy SD gpt-3.5-turbo-0125 175 nan nan 0.87 0.21 gpt-4-0613 Unknown nan nan 0.78 0.18 gpt-4-0125-preview Unknown nan nan 0.73 0.3 openhermes-2.5 7 ggufv2 Q5_K_M 0.73 0.32 gpt-3.5-turbo-0613 175 nan nan 0.73 0.24 openhermes-2.5 7 ggufv2 Q8_0 0.71 0.32 openhermes-2.5 7 ggufv2 Q4_K_M 0.71 0.33 openhermes-2.5 7 ggufv2 Q6_K 0.7 0.33 gpt-4o-2024-05-13 Unknown nan nan 0.7 0.35 llama-3-instruct 8 ggufv2 Q8_0 0.65 0.35 llama-3-instruct 8 ggufv2 Q4_K_M 0.64 0.38 llama-3-instruct 8 ggufv2 Q6_K 0.64 0.36 llama-3-instruct 8 ggufv2 Q5_K_M 0.62 0.36 openhermes-2.5 7 ggufv2 Q3_K_M 0.59 0.32 openhermes-2.5 7 ggufv2 Q2_K 0.51 0.3 code-llama-instruct 34 ggufv2 Q2_K 0.5 0.33 code-llama-instruct 7 ggufv2 Q3_K_M 0.49 0.31 code-llama-instruct 7 ggufv2 Q4_K_M 0.47 0.39 mistral-instruct-v0.2 7 ggufv2 Q5_K_M 0.46 0.34 mistral-instruct-v0.2 7 ggufv2 Q6_K 0.45 0.34 code-llama-instruct 34 ggufv2 Q3_K_M 0.45 0.31 chatglm3 6 ggmlv3 q4_0 0.44 0.26 llama-2-chat 70 ggufv2 Q4_K_M 0.44 0.35 llama-2-chat 70 ggufv2 Q5_K_M 0.44 0.35 code-llama-instruct 13 ggufv2 Q6_K 0.44 0.35 code-llama-instruct 13 ggufv2 Q8_0 0.44 0.33 code-llama-instruct 13 ggufv2 Q5_K_M 0.43 0.32 llama-2-chat 70 ggufv2 Q3_K_M 0.41 0.33 mistral-instruct-v0.2 7 ggufv2 Q3_K_M 0.41 0.34 mistral-instruct-v0.2 7 ggufv2 Q8_0 0.4 0.33 llama-2-chat 13 ggufv2 Q8_0 0.4 0.34 code-llama-instruct 7 ggufv2 Q8_0 0.4 0.37 code-llama-instruct 7 ggufv2 Q5_K_M 0.39 0.34 llama-2-chat 13 ggufv2 Q3_K_M 0.39 0.33 llama-2-chat 13 ggufv2 Q5_K_M 0.39 0.33 code-llama-instruct 7 ggufv2 Q2_K 0.38 0.29 code-llama-instruct 34 ggufv2 Q4_K_M 0.38 0.35 code-llama-instruct 7 ggufv2 Q6_K 0.38 0.39 code-llama-instruct 34 ggufv2 Q5_K_M 0.38 0.38 llama-2-chat 70 ggufv2 Q2_K 0.38 0.35 llama-2-chat 13 ggufv2 Q6_K 0.37 0.34 code-llama-instruct 34 ggufv2 Q8_0 0.37 0.35 llama-2-chat 7 ggufv2 Q4_K_M 0.37 0.29 mistral-instruct-v0.2 7 ggufv2 Q2_K 0.37 0.29 mistral-instruct-v0.2 7 ggufv2 Q4_K_M 0.37 0.35 code-llama-instruct 34 ggufv2 Q6_K 0.37 0.36 llama-2-chat 13 ggufv2 Q4_K_M 0.36 0.34 llama-2-chat 7 ggufv2 Q3_K_M 0.36 0.34 mixtral-instruct-v0.1 46,7 ggufv2 Q4_K_M 0.35 0.3 llama-2-chat 7 ggufv2 Q8_0 0.35 0.29 mixtral-instruct-v0.1 46,7 ggufv2 Q5_K_M 0.34 0.31 mixtral-instruct-v0.1 46,7 ggufv2 Q6_K 0.34 0.29 mixtral-instruct-v0.1 46,7 ggufv2 Q3_K_M 0.33 0.28 code-llama-instruct 13 ggufv2 Q4_K_M 0.33 0.31 llama-2-chat 7 ggufv2 Q6_K 0.33 0.29 mixtral-instruct-v0.1 46,7 ggufv2 Q8_0 0.33 0.25 llama-2-chat 7 ggufv2 Q5_K_M 0.32 0.29 mixtral-instruct-v0.1 46,7 ggufv2 Q2_K 0.32 0.27 llama-2-chat 13 ggufv2 Q2_K 0.28 0.29 llama-2-chat 7 ggufv2 Q2_K 0.22 0.36 code-llama-instruct 13 ggufv2 Q2_K 0.17 0.34 code-llama-instruct 13 ggufv2 Q3_K_M 0.15 0.34 <p></p>"},{"location":"benchmark/#scores-of-all-tasks","title":"Scores of all tasks","text":"<p>Wide table; you may need to scroll horizontally to see all columns. Table sorted by mean score in descending order. Click the column names to reorder.</p> Full model name explicit_relevance_of_single_fragments sourcedata_info_extraction implicit_relevance_of_multiple_fragments api_calling query_generation naive_query_generation_using_schema entity_selection property_exists medical_exam end_to_end_query_generation property_selection relationship_selection Mean Accuracy Median Accuracy SD gpt-3.5-turbo-0125 1 0.510032 0.9 0.647059 0.966667 0.486667 1 0.866667 0.670401 0.926667 0.35625 1 0.777534 0.866667 0.214646 gpt-4-0613 1 0.668903 1 0.619048 0.966667 0.68 0.888889 0.888889 0.730159 0.88 0.359375 0.65 0.777661 0.777661 0.177558 gpt-4-0125-preview 1 0.689705 0.5 0.793651 0.833333 0.44 0.777778 0.733333 0.77591 0 0 0.75 0.607809 0.733333 0.295129 openhermes-2.5:7:ggufv2:Q5_K_M 1 0.579916 1 nan 0.913333 0.586667 0.888889 0.777778 0.571429 0 0.125 1 0.676637 0.727208 0.318593 gpt-3.5-turbo-0613 1 0.575381 1 nan 0.946667 0.5 0.888889 0.755556 0.291317 0.833333 0.3625 0.5 0.695786 0.725671 0.237079 openhermes-2.5:7:ggufv2:Q8_0 1 0.600829 1 nan 0.88 0.466667 0.888889 0.755556 0.577031 0 0.125 1 0.663088 0.709322 0.319919 openhermes-2.5:7:ggufv2:Q4_K_M 1 0.597281 1 nan 0.873333 0.466667 0.888889 0.755556 0.586368 0 0.046875 1 0.655906 0.705731 0.330932 openhermes-2.5:7:ggufv2:Q6_K 1 0.619167 1 nan 0.86 0.533333 1 0.733333 0.57423 0 0.046875 1 0.669722 0.701528 0.334697 gpt-4o-2024-05-13 1 0.653946 0.7 0.809524 0.8 0.533333 1 0.85 0.762838 0 0 0 0.59247 0.7 0.350859 llama-3-instruct:8:ggufv2:Q8_0 1 0.188555 1 nan 0.92 0.666667 0.875 0.725 0.638655 0 0.28125 0 0.572284 0.652661 0.354494 llama-3-instruct:8:ggufv2:Q4_K_M 1 0.116871 1 nan 0.92 0.666667 0.861111 0.775 0.622782 0 0.109375 0 0.551982 0.644725 0.376704 llama-3-instruct:8:ggufv2:Q6_K 1 0.162657 1 nan 0.926667 0.666667 0.875 0.775 0.621849 0 0.28125 0 0.573554 0.644258 0.359128 llama-3-instruct:8:ggufv2:Q5_K_M 1 0.166434 1 nan 0.926667 0.6 0.875 0.65 0.633053 0 0.1875 0 0.548969 0.616527 0.360513 openhermes-2.5:7:ggufv2:Q3_K_M 1 0.554488 0.5 nan 0.94 0.466667 1 0.72 0.563959 0 0.125 1 0.624556 0.594257 0.318981 openhermes-2.5:7:ggufv2:Q2_K 1 0.444054 0.5 nan 0.94 0.433333 0.555556 0.844444 0.537815 0 0 0.5 0.5232 0.5116 0.298404 code-llama-instruct:34:ggufv2:Q2_K 0.5 nan 1 nan 0.686667 0.566667 0 0.75 nan 0 0 0.5 0.444815 0.5 0.328199 code-llama-instruct:7:ggufv2:Q3_K_M 0.833333 nan 0.7 nan 0.873333 0.426667 0.5 0.8 nan 0 0 0.25 0.487037 0.493519 0.307716 code-llama-instruct:7:ggufv2:Q4_K_M 1 0.138732 1 nan 0.966667 0.653333 0.333333 0.6 nan 0 0 0 0.469207 0.469207 0.38731 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 1 0.385754 1 nan 0.826667 0.466667 0.444444 0.688889 0.364146 0 0 0 0.470597 0.455556 0.34385 mistral-instruct-v0.2:7:ggufv2:Q6_K 1 0.367412 1 nan 0.833333 0.433333 0.5 0.65 0.366947 0 0.046875 0 0.472536 0.452935 0.337974 code-llama-instruct:34:ggufv2:Q3_K_M 0.5 nan 0.5 nan 0.786667 0.6 0 0.875 nan 0 0 0.25 0.390185 0.445093 0.306514 chatglm3:6:ggmlv3:q4_0 0.733333 0.188284 1 nan 0.553333 0.48 0.75 0.275 0.426704 0 0.2875 0.4 0.463105 0.444905 0.260423 llama-2-chat:70:ggufv2:Q4_K_M 1 0.240936 1 nan 0.92 0.42 0.444444 0.755556 nan 0 0 0.25 0.503094 0.444444 0.354692 llama-2-chat:70:ggufv2:Q5_K_M 1 0.210166 0.9 nan 0.906667 0.36 0.444444 0.777778 nan 0 0 0.25 0.484905 0.444444 0.346535 code-llama-instruct:13:ggufv2:Q6_K 0.833333 nan 0.5 nan 0.793333 0.54 0 0.825 nan 0 0 0 0.387963 0.443981 0.345581 code-llama-instruct:13:ggufv2:Q8_0 0.833333 nan 0.5 nan 0.766667 0.566667 0 0.75 nan 0 0 0 0.37963 0.439815 0.334971 code-llama-instruct:13:ggufv2:Q5_K_M 0.666667 nan 0.5 nan 0.78 0.566667 0 0.775 nan 0 0 0 0.36537 0.432685 0.320506 llama-2-chat:70:ggufv2:Q3_K_M 1 0.197898 0.5 nan 0.906667 0.413333 0.333333 0.777778 nan 0 0.171875 0 0.430088 0.413333 0.327267 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 1 0.368974 1 nan 0.773333 0.466667 0.333333 0.666667 0.360411 0 0.046875 0 0.456024 0.412499 0.335885 mistral-instruct-v0.2:7:ggufv2:Q8_0 1 0.351684 0.9 nan 0.846667 0.433333 0.333333 0.644444 0.366947 0 0.0375 0 0.446719 0.40014 0.330107 llama-2-chat:13:ggufv2:Q8_0 1 0.0762457 0.5 nan 0.786667 0.48 0 0.711111 0.431373 0 0 0 0.362309 0.396841 0.335904 code-llama-instruct:7:ggufv2:Q8_0 1 nan 0.5 nan 0.96 0.4 0 0.666667 nan 0 0 0 0.391852 0.395926 0.37338 code-llama-instruct:7:ggufv2:Q5_K_M 0.833333 nan 0.5 nan 0.96 0.4 0.111111 0.688889 nan 0 0 0 0.388148 0.394074 0.340156 llama-2-chat:13:ggufv2:Q3_K_M 1 0.112631 0.5 nan 0.68 0.48 0 0.733333 0.428571 0 0 0 0.357685 0.393128 0.325419 llama-2-chat:13:ggufv2:Q5_K_M 1 0.0766167 0.5 nan 0.746667 0.433333 0 0.644444 0.431373 0 0 0 0.348403 0.389888 0.32518 code-llama-instruct:7:ggufv2:Q2_K 0.333333 nan 0.7 nan 0.92 0.533333 0.25 0.8 nan 0 0.0625 0.25 0.427685 0.380509 0.292686 code-llama-instruct:34:ggufv2:Q4_K_M 0.5 nan 0.4 nan 0.906667 0.466667 0 0.975 nan 0 0 0 0.360926 0.380463 0.350483 code-llama-instruct:7:ggufv2:Q6_K 0.833333 nan 0.9 nan 0.96 0.333333 0 0.775 nan 0 0 0 0.422407 0.37787 0.391629 code-llama-instruct:34:ggufv2:Q5_K_M 0.333333 nan 1 nan 0.9 0.466667 0.125 0.95 nan 0 0 0 0.419444 0.376389 0.384096 llama-2-chat:70:ggufv2:Q2_K 1 0.215047 0.5 nan 0.9 0.473333 0 0.666667 nan 0 0 0 0.375505 0.375505 0.352226 llama-2-chat:13:ggufv2:Q6_K 1 0.0781337 0.5 nan 0.813333 0.386667 0 0.775 0.428571 0 0 0 0.361973 0.37432 0.342819 code-llama-instruct:34:ggufv2:Q8_0 0.333333 nan 0.9 nan 0.86 0.466667 0.25 0.925 nan 0 0 0 0.415 0.374167 0.353285 llama-2-chat:7:ggufv2:Q4_K_M 1 0.0852494 0.5 nan 0.646667 0.24 0.444444 0.488889 0.40056 0 0 0 0.345983 0.373271 0.290686 mistral-instruct-v0.2:7:ggufv2:Q2_K 1 0.331261 0.5 nan 0.693333 0.573333 0.222222 0.6 0.352941 0 0 0 0.388463 0.370702 0.294881 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 1 0.347025 1 nan 0.826667 0.366667 0.333333 0.688889 0.365079 0 0 0 0.447969 0.365873 0.348328 code-llama-instruct:34:ggufv2:Q6_K 0.333333 nan 0.9 nan 0.853333 0.473333 0.125 0.9 nan 0 0 0 0.398333 0.365833 0.356636 llama-2-chat:13:ggufv2:Q4_K_M 1 0.0888675 0.5 nan 0.76 0.366667 0 0.777778 0.428571 0 0 0 0.356535 0.361601 0.336686 llama-2-chat:7:ggufv2:Q3_K_M 1 0.0650717 1 nan 0.693333 0.233333 0.333333 0.466667 0.394958 0 0.1 0 0.3897 0.361517 0.337204 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 0.166667 0.193786 1 nan 0.76 0.426667 0.333333 0.755556 0.368814 0 0.1625 0 0.378848 0.351074 0.301567 llama-2-chat:7:ggufv2:Q8_0 1 0.0847297 0.5 nan 0.64 0.266667 0.444444 0.355556 0.40056 0 0 0 0.335632 0.345594 0.286246 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 0 0.235659 1 nan 0.84 0.333333 0.422222 0.711111 0.352941 0 0 0.25 0.376842 0.343137 0.313874 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 0 0.225524 0.7 nan 0.826667 0.333333 0.475 0.85 0.34267 0 0 0.25 0.363927 0.338002 0.289705 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 0 0.229622 0.5 nan 0.893333 0.38 0.333333 0.777778 nan 0 0.065625 0.25 0.342969 0.333333 0.278279 code-llama-instruct:13:ggufv2:Q4_K_M 0.333333 nan 0.5 nan 0.833333 0.533333 0 0.775 nan 0 0 0 0.330556 0.331944 0.30939 llama-2-chat:7:ggufv2:Q6_K 1 0.0614608 0.5 nan 0.66 0.266667 0.375 0.333333 0.406162 0 0 0 0.327511 0.330422 0.288285 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 0.133333 0.189177 0.6 nan 0.846667 0.386667 0.311111 0.666667 0.358543 0 0 0.25 0.340197 0.325654 0.248216 llama-2-chat:7:ggufv2:Q5_K_M 1 0.0697591 0.6 nan 0.633333 0.293333 0.444444 0.288889 0.40056 0 0.0375 0 0.342529 0.317931 0.289372 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 0.333333 0.157514 0.6 nan 0.726667 0.48 0 0.733333 0.329599 0 0 0 0.305495 0.317547 0.269925 llama-2-chat:13:ggufv2:Q2_K 1 0.0649389 0.5 nan 0.433333 0.366667 0 0.288889 0.414566 0 0 0 0.278945 0.283917 0.285171 llama-2-chat:7:ggufv2:Q2_K 0.833333 0.0361865 1 nan 0.686667 0.1 0 0.688889 0.369748 0 0 0 0.337711 0.218856 0.359055 code-llama-instruct:13:ggufv2:Q2_K 0.0333333 nan 0.4 nan 0.82 0.566667 0 0.875 nan 0 0 0 0.299444 0.166389 0.336056 code-llama-instruct:13:ggufv2:Q3_K_M 0 nan 0 nan 0.833333 0.533333 0.45 0.85 nan 0 0 0 0.296296 0.148148 0.336707"},{"location":"benchmarking/","title":"Benchmarking","text":"<p>For trustworthy application of LLMs to real-world and biomedical problems, it is imperative to understand their performance and limitations. We need to constantly evaluate the multitude of combinations of individual models and versions, their parameters (e.g., temperature), prompt sets, databases and vector databases, and diverse application scenarios. To this end, we are maintaining a living benchmarking framework that allows us to continuously compare the performance of different models and configurations on a variety of tasks. The procedure of the living benchmark follows the idea of test-driven development, in that test cases are added with practical reference to a desired feature or behaviour. For cases where a model does not yield a perfect response out of the box, which will be most cases, modifications to the components of the framework, such as prompts or functions, are made to improve the model's performance. By tracking model performance on these tests over time, we can gain insights into the reliability of the framework and identify the gaps to be addressed.</p>"},{"location":"benchmarking/#running-the-benchmark","title":"Running the benchmark","text":"<p>The benchmark uses the pytest framework to orchestrate the evaluation of a number of models on a number of tasks. The benchmark is run on a regular basis, and the results are published in the benchmark section. The benchmarking suite can be found in the <code>benchmark</code> directory of the BioChatter repository. It can be executed using standard pytest syntax, e.g., <code>poetry run pytest benchmark</code>. As default behavior it checks, which test cases have already been executed and only executes the tests that have not been executed yet. To run all benchmarks again, use <code>poetry run pytest benchmark --run-all</code>. If you want to develop and debug the benchmark instead of just running it, please refer to the developer docs.</p> <p>To allow flexible extension of the benchmark, we have implemeted a modular test framework that uses pytest fixtures to allow easy addition of new models and tasks. All setup is done in the <code>conftest.py</code> file in the <code>benchmark</code> directory. The benchmarks are organised by module, task, and subtask. Module names (e.g., <code>biocypher_query_generation</code> or <code>rag_interpretation</code>) group tasks by use case and are used to create benchmark dataset objects for the tests. Tasks (e.g., <code>entity_selection</code> or <code>query_generation</code>) are the actual tests that are executed in each module. The name of the test (minus the <code>test_</code> prefix) is used to generate the result file name. Subtasks (e.g., <code>single_word</code> or <code>multi_word</code>) are used to parametrise the tests and track the results of different subtasks in the result files. In addition, we generate md5 hashes of the test data for each subtask and use them to skip tests that have already been executed. Exemplary tests are defined in a YAML file in the <code>benchmark/data</code> directory, while the actual test dataset is encrypted and only available to the benchmarking pipeline. The result files are simple CSVs whose file names are generated from the name of the test function; they can be found in <code>benchmark/results</code> and contain scores for all executed combination of parameters.</p> <p>To achieve modularity, we use pytest fixtures and parametrization. For instance, to add a new model, we can modify the <code>OPENAI_MODEL_NAMES</code> and <code>XINFERENCE_MODELS</code> dictionary in <code>conftest.py</code>. The latter carries model names, sizes, format, and quantisations. The environment that runs the benchmark needs to make available all prerequisites for the different modules. For instance, the tasks requiring connection to an LLM need to provide the necessary credentials and API keys, or a connection to a self-hosted model. Likewise, the benchmarks of retrieval-augmented generation (RAG) processes require a connection to the RAG agent, e.g., a vector database.</p>"},{"location":"benchmarking/#calibration","title":"Calibration","text":"<p>To ensure valid assessment of LLM performance, we need to ensure calibration and technical validity of the benchmarking framework. More recent LLMs in particular may be problematic when using publicly available benchmark datasets, since they could have been used for training the model. This is particularly relevant in closed-source (e.g., OpenAI) models. Thus, we need to consider strategies for avoiding contamination, such as hand-crafting datasets, carefully testing for contamination, or using perturbation strategies to generate new datasets from existing ones. Advanced scenarios could utilise LLMs as \"examiners,\" allowing more flexible test design and free-form answers. There is much research into these phenomena, all of which should be considered in the maintenance of this testing framework.</p>"},{"location":"benchmarking/#aspects-of-benchmarking","title":"Aspects of benchmarking","text":"<p>In the following, we will detail the different aspects of benchmarking that we are currently testing. This is a living document that will be updated as we add new tests and test modules.</p>"},{"location":"benchmarking/#models","title":"Models","text":"<p>Naturally the biggest impact on BioChatter performance comes with the model used. However, model versions can have a significant impact, which can be obfuscated by the fact that model names are often not unique. For instance, OpenAI's GPT models often have versions with significantly diverging capabilities and performance. Open-source models even come in a range of sizes and quantisations, which can have a significant impact on performance.</p>"},{"location":"benchmarking/#prompts","title":"Prompts","text":"<p>As has been recently studied extensively, prompt engineering can make or break the performance of a model on a given task. As such, it is important to test the default prompts we commonly use, as well as a range of variations to determine factors of prompt performance and robustness. As an added complexity, LLMs are often used to generate prompts, which theoretically allows for procedural generation of an infinite number of prompts, as long as time and resources allow.</p>"},{"location":"benchmarking/#model-parameters","title":"Model parameters","text":"<p>The parameters of the model can have a significant impact on the performance of the model. We often set model temperature to 0 to provide consistent results, but some applications may benefit from a higher temperature. In testing, we mostly rely on a temperature of 0 due to the complexity of testing highly variable results in most cases.</p>"},{"location":"benchmarking/#databases","title":"Databases","text":"<p>An important facet of BioChatter and BioCypher is their combination in querying databases. This helps to ameliorate the limitations of LLMs by providing structured and validated knowledge to counteract hallucinations. To ensure the seamless interaction of BioChatter and BioCypher, we need to test the performance of BioChatter on a variety of databases.</p>"},{"location":"benchmarking/#vector-databases","title":"Vector databases","text":"<p>Similarly to regular databases, vector databases are an important tool to provide validated knowledge to LLMs. Vector databases bring their own set of parameters and application scenarios, which likewise need to be tested. For instance, the length and overlap of fragments, the embedding algorithms, as well as the semantic search algorithms applied can have an impact on LLM conversation performance.</p>"},{"location":"benchmarking/#tasks","title":"Tasks","text":"<p>There is a wide range of tasks that are potentially useful to BioChatter users. To cover most scenarios of research and development use, as well as clinical applications, we test a variety of tasks and LLM personas.</p>"},{"location":"chat/","title":"Basic Usage: Chat","text":"<p>BioChatter provides access to chat functionality via the <code>Conversation</code> class, which is implemented in several child classes (in the <code>llm_connect.py</code> module) to account for differences in APIs of the LLMs.</p>"},{"location":"chat/#setting-up-the-conversation","title":"Setting up the conversation","text":"<p>To start a conversation, we can initialise the Conversation class (here exemplified by GPT):</p> <pre><code>from biochatter.llm_connect import GptConversation\n\nconversation = GptConversation(\n    model_name=\"gpt-3.5-turbo\",\n    prompts={},\n)\n</code></pre> <p>It is possible to supply a dictionary of prompts to the conversation from the outset, which is formatted in a way to correspond to the different roles of the conversation, i.e., primary and correcting models. Prompts with the <code>primary_model_prompts</code> key will be appended to the System Messages of the primary model, and <code>correcting_agent_prompts</code> will be appended to the System Messages of the correction model at setup. If we pass a dictionary without these keys (or an empty one), there will be no system messages appended to the models. They can however be introduced later by using the following method:</p> <pre><code>conversation.append_system_message(\"System Message\")\n</code></pre> <p>Similarly, the user queries (<code>HumanMessage</code>) are passed to the conversation using <code>conversation.append_user_message(\"User Message\")</code>. For purposes of keeping track of the conversation history, we can also append the model's responses as <code>AIMessage</code> using <code>conversation.append_ai_message</code>.</p>"},{"location":"chat/#querying-the-model","title":"Querying the model","text":"<p>After setting up the conversation in this way, for instance by establishing a flattery component (e.g. 'You are an assistant to a researcher ...'), the model can be queried using the <code>query</code> function.</p> <pre><code>msg, token_usage, correction = conversation.query('Question here')\n</code></pre> <p>Note that a query will automatically append a user message to the message history, so there is no need to call <code>append_user_message()</code> again. The query function returns the actual answer of the model (<code>msg</code>), the token usage statistics reported by the API (<code>token_usage</code>), and an optional <code>correction</code> that contains the opinion of the corrective agent.</p>"},{"location":"chat/#using-openai-models","title":"Using OpenAI models","text":"<p>Using an OpenAI model via the API is generally the easiest way to get started, but requires the provision of an API key to the OpenAI API. To do this, you can designate the <code>OPENAI_API_KEY</code> variable in your environment directly (<code>export OPENAI_API_KEY=sk-...</code>) by adding it to your shell configuration (e.g., the <code>zshrc</code>).</p>"},{"location":"chat/#multimodal-models-text-and-image","title":"Multimodal models - Text and image","text":"<p>We support multimodal queries in models that offer these capabilities after the blueprint of the OpenAI API. We can either add an image-containing message to the conversation using the <code>append_image_message</code> method, or we can pass an image URL directly to the <code>query</code> method:</p> <pre><code># Either: Append image message\nconversation.append_image_message(\n    message=\"Here is an attached image\",\n    image_url=\"https://example.com/image.jpg\"\n)\n\n# Or: Query with image included\nmsg, token_usage, correction = conversation.query(\n    \"What's in this image?\",\n    image_url=\"https://example.com/image.jpg\"\n)\n</code></pre>"},{"location":"chat/#using-local-images","title":"Using local images","text":"<p>Following the recommendations by OpenAI, we can pass local images as base64-encoded strings. We allow this by setting the <code>local</code> flag to <code>True</code> in the <code>append_image_message</code> method:</p> <pre><code>conversation.append_image_message(\n    message=\"Here is an attached image\",\n    image_url=\"my/local/image.jpg\",\n    local=True\n)\n</code></pre> <p>We also support the use of local images in the <code>query</code> method by detecting the netloc of the image URL. If the netloc is empty, we assume that the image is local and read it as a base64-encoded string:</p> <pre><code>msg, token_usage, correction = conversation.query(\n    \"What's in this image?\",\n    image_url=\"my/local/image.jpg\"\n)\n</code></pre>"},{"location":"chat/#open-source-multimodal-models","title":"Open-source multimodal models","text":"<p>While OpenAI models work seamlessly, open-source multimodal models can be buggy or incompatible with certain hardware. We have experienced mixed success with open models and, while they are technically supported by BioChatter, their outputs currently may be unreliable.</p>"},{"location":"kgs/","title":"Connecting Knowledge Graphs","text":"<p>To increase accessibility of databases, we can leverage the BioCypher integration of BioChatter.  In BioCypher, we use a YAML configuration (<code>schema_config.yaml</code>) to specify the contents of the knowledge graph and their ontological associations.  We also generate a more extensive, but essentially similar YAML file during the BioCypher creation of a knowledge graph (<code>schema_info.yaml</code>), which contains more information pertinent to LLM interaction with the database.  The current prototypical implementation of query generation through an LLM is implemented in the <code>prompts.py</code> module on the example of a Neo4j knowledge graph connection.</p>"},{"location":"kgs/#connecting","title":"Connecting","text":"<p>Currently, BioChatter does not handle database connectivity, but simply returns a query for a given language.  The application using BioChatter should establish connectivity and send the query to the database, as is implemented in BioChatter Light, for instance.  For a demonstration using a simple Docker compose setup, see the Pole Crime Dataset demo repository.</p>"},{"location":"kgs/#querying","title":"Querying","text":"<p>The generation of a query based on BioCypher configuration files is a multi-step process.  This is partly to account for the limited token input space of some models, and partly to better be able to test and compare the individual steps. We will implement a wrapper function that goes through the steps automatically soon, but for now the steps need to be run individually.</p>"},{"location":"kgs/#setup","title":"Setup","text":"<p>We use the <code>BioCypherPromptEngine</code> class to handle the LLM conversation.</p> <pre><code>from biochatter.prompts import BioCypherPromptEngine\nprompt_engine = BioCypherPromptEngine(\n    schema_config_or_info_path=\"test/schema_info.yaml\"\n)\n</code></pre> <p>This will load the <code>schema_config.yaml</code> or <code>schema_info.yaml</code> (preferred) file and set up the conversation.</p>"},{"location":"kgs/#query-generation","title":"Query generation","text":"<p>Using the <code>generate_query</code> wrapper, we can generate a query from a question and a database language.</p> <pre><code>query = prompt_engine.generate_query(\n    question=\"Which genes are associated with mucoviscidosis?\",\n    database_language=\"Cypher\",\n)\n</code></pre> <p>This will return a query that can be used in the database query language (e.g., Cypher).  This end to end process executes the steps detailed below, namely, entity selection, relationship selection, and property selection, as well as the generation of the final query using the selected components. You can run each of these steps individually, if you want.</p>"},{"location":"kgs/#entity-selection","title":"Entity selection","text":"<p>Starting from the <code>schema_config.yaml</code> or <code>schema_info.yaml</code> (preferred) file, we first have the model decide which entities in the database are relevant to the user's question.</p> <pre><code>success = prompt_engine._select_entities(\n    question=\"Which genes are associated with mucoviscidosis?\"\n)\n</code></pre> <p>This will select a number of entities from the database schema to be used subsequently, and return True or False to indicate success.</p>"},{"location":"kgs/#relationship-selection","title":"Relationship selection","text":"<p>Next, we will use the entities determined in the first step to select relationships between them.  The entities selected in the first step will be stored in the <code>selected_entities</code> attribute of the <code>BioCypherPromptEngine</code> instance, and the question is stored in the <code>question</code> attribute.  Both are automatically used to select relationships.</p> <pre><code>success = prompt_engine._select_relationships()\n</code></pre>"},{"location":"kgs/#property-selection","title":"Property selection","text":"<p>To not unnecessarily waste token input space, we are only interested in selecting properties of entities that are of interest given the question asked. We do so in the third step, which uses the entities and relationships determined in the first steps.  Again, <code>question</code>, <code>selected_entities</code>, and <code>selected_relationships</code> are automatically used to select properties.</p> <pre><code>success = prompt_engine._select_properties()\n</code></pre> <p>This will select a number of properties to be used in the query, and also return True or False to indicate success.</p>"},{"location":"kgs/#query-generation_1","title":"Query generation","text":"<p>Finally, we can use the entities and relationships, as well as the selected properties, to ask the LLM to generate a query in the desired language.</p> <pre><code>query = prompt_engine._generate_query(\n    question=\"Which genes are associated with mucoviscidosis?\",\n    entities=[\"Gene\", \"Disease\"],\n    relationships=[\"GeneToDiseaseAssociation\"],\n    properties={\"Disease\": [\"name\", \"ICD10\", \"DSM5\"]},\n    database_language=\"Cypher\",\n)\n</code></pre> <p>This will (hopefully) return a query that can be used in the database query language (e.g., Cypher).</p>"},{"location":"kgs/#query-interaction","title":"Query interaction","text":"<p>As an optional follow-up, you can interact with the returned query using the <code>BioCypherQueryHandler</code> class (<code>query_interaction.py</code>). It takes the query, the original question and the KG information so that the interaction is still aware of the KG.</p> <pre><code>from biochatter.query_interaction import BioCypherQueryHandler\nquery_handler = BioCypherQueryHandler(\n    query=query,\n    query_lang=\"Cypher\",\n    kg_selected={\n        entities: [\"Gene\", \"Disease\"],\n        relationships: [\"GeneToDiseaseAssociation\"],\n        properties: {\"Disease\": [\"name\", \"ICD10\", \"DSM5\"]}\n    },\n    question=\"Which genes are associated with mucoviscidosis?\"\n)\n</code></pre>"},{"location":"kgs/#explanation","title":"Explanation","text":"<p>You can retrieve an explanation of the returned query with:</p> <pre><code>explanation = query_handler.explain_query()\n</code></pre>"},{"location":"kgs/#updating","title":"Updating","text":"<p>Alternatively, you can ask the LLM for an update of the query with:</p> <pre><code>request = \"Only return 10 results and sort them alphabetically\"\nexplanation = query_handler.update_query(request)\n</code></pre> <p>NB: for updates, it might sometimes be relevant that all the KG enitites/relationships/properties are known to the LLM instead of only those that were selected to be relevant for the original question. For this, you can optionally pass them as input to the query handler with <code>kg</code> (similar to <code>kg_selected</code>).</p> <p>(Tip: the prompt_engine object contains both the selected and non-selected as attributes)</p>"},{"location":"llm_connect-reference/","title":"LLM connectivity module","text":"<p>Here we handle connections to various LLM services, proprietary and open source.</p>"},{"location":"llm_connect-reference/#biochatter.llm_connect.AzureGptConversation","title":"<code>AzureGptConversation</code>","text":"<p>               Bases: <code>GptConversation</code></p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>class AzureGptConversation(GptConversation):\n    def __init__(\n        self,\n        deployment_name: str,\n        model_name: str,\n        prompts: dict,\n        correct: bool = True,\n        split_correction: bool = False,\n        version: Optional[str] = None,\n        base_url: Optional[str] = None,\n    ):\n        \"\"\"\n        Connect to Azure's GPT API and set up a conversation with the user.\n        Extends GptConversation.\n\n        Args:\n            deployment_name (str): The name of the Azure deployment to use.\n\n            model_name (str): The name of the model to use. This is distinct\n                from the deployment name.\n\n            prompts (dict): A dictionary of prompts to use for the conversation.\n\n            correct (bool): Whether to correct the model output.\n\n            split_correction (bool): Whether to correct the model output by\n                splitting the output into sentences and correcting each\n                sentence individually.\n\n            version (str): The version of the Azure API to use.\n\n            base_url (str): The base URL of the Azure API to use.\n        \"\"\"\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            correct=correct,\n            split_correction=split_correction,\n        )\n\n        self.version = version\n        self.base_url = base_url\n        self.deployment_name = deployment_name\n\n    def set_api_key(self, api_key: str, user: Optional[str] = None):\n        \"\"\"\n        Set the API key for the Azure API. If the key is valid, initialise the\n        conversational agent. No user stats on Azure.\n\n        Args:\n            api_key (str): The API key for the Azure API.\n\n        Returns:\n            bool: True if the API key is valid, False otherwise.\n        \"\"\"\n\n        try:\n            self.chat = AzureChatOpenAI(\n                deployment_name=self.deployment_name,\n                model_name=self.model_name,\n                openai_api_version=self.version,\n                azure_endpoint=self.base_url,\n                openai_api_key=api_key,\n                temperature=0,\n            )\n            # TODO this is the same model as the primary one; refactor to be\n            # able to use any model for correction\n            self.ca_chat = AzureChatOpenAI(\n                deployment_name=self.deployment_name,\n                model_name=self.model_name,\n                openai_api_version=self.version,\n                azure_endpoint=self.base_url,\n                openai_api_key=api_key,\n                temperature=0,\n            )\n\n            test = self.chat.generate([[HumanMessage(content=\"Hello\")]])\n\n            return True\n\n        except openai._exceptions.AuthenticationError as e:\n            return False\n\n    def _update_usage_stats(self, model: str, token_usage: dict):\n        \"\"\"\n        We do not track usage stats for Azure.\n        \"\"\"\n        return\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.AzureGptConversation.__init__","title":"<code>__init__(deployment_name, model_name, prompts, correct=True, split_correction=False, version=None, base_url=None)</code>","text":"<p>Connect to Azure's GPT API and set up a conversation with the user. Extends GptConversation.</p> <p>Parameters:</p> Name Type Description Default <code>deployment_name</code> <code>str</code> <p>The name of the Azure deployment to use.</p> required <code>model_name</code> <code>str</code> <p>The name of the model to use. This is distinct from the deployment name.</p> required <code>prompts</code> <code>dict</code> <p>A dictionary of prompts to use for the conversation.</p> required <code>correct</code> <code>bool</code> <p>Whether to correct the model output.</p> <code>True</code> <code>split_correction</code> <code>bool</code> <p>Whether to correct the model output by splitting the output into sentences and correcting each sentence individually.</p> <code>False</code> <code>version</code> <code>str</code> <p>The version of the Azure API to use.</p> <code>None</code> <code>base_url</code> <code>str</code> <p>The base URL of the Azure API to use.</p> <code>None</code> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def __init__(\n    self,\n    deployment_name: str,\n    model_name: str,\n    prompts: dict,\n    correct: bool = True,\n    split_correction: bool = False,\n    version: Optional[str] = None,\n    base_url: Optional[str] = None,\n):\n    \"\"\"\n    Connect to Azure's GPT API and set up a conversation with the user.\n    Extends GptConversation.\n\n    Args:\n        deployment_name (str): The name of the Azure deployment to use.\n\n        model_name (str): The name of the model to use. This is distinct\n            from the deployment name.\n\n        prompts (dict): A dictionary of prompts to use for the conversation.\n\n        correct (bool): Whether to correct the model output.\n\n        split_correction (bool): Whether to correct the model output by\n            splitting the output into sentences and correcting each\n            sentence individually.\n\n        version (str): The version of the Azure API to use.\n\n        base_url (str): The base URL of the Azure API to use.\n    \"\"\"\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        correct=correct,\n        split_correction=split_correction,\n    )\n\n    self.version = version\n    self.base_url = base_url\n    self.deployment_name = deployment_name\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.AzureGptConversation.set_api_key","title":"<code>set_api_key(api_key, user=None)</code>","text":"<p>Set the API key for the Azure API. If the key is valid, initialise the conversational agent. No user stats on Azure.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The API key for the Azure API.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the API key is valid, False otherwise.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def set_api_key(self, api_key: str, user: Optional[str] = None):\n    \"\"\"\n    Set the API key for the Azure API. If the key is valid, initialise the\n    conversational agent. No user stats on Azure.\n\n    Args:\n        api_key (str): The API key for the Azure API.\n\n    Returns:\n        bool: True if the API key is valid, False otherwise.\n    \"\"\"\n\n    try:\n        self.chat = AzureChatOpenAI(\n            deployment_name=self.deployment_name,\n            model_name=self.model_name,\n            openai_api_version=self.version,\n            azure_endpoint=self.base_url,\n            openai_api_key=api_key,\n            temperature=0,\n        )\n        # TODO this is the same model as the primary one; refactor to be\n        # able to use any model for correction\n        self.ca_chat = AzureChatOpenAI(\n            deployment_name=self.deployment_name,\n            model_name=self.model_name,\n            openai_api_version=self.version,\n            azure_endpoint=self.base_url,\n            openai_api_key=api_key,\n            temperature=0,\n        )\n\n        test = self.chat.generate([[HumanMessage(content=\"Hello\")]])\n\n        return True\n\n    except openai._exceptions.AuthenticationError as e:\n        return False\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.BloomConversation","title":"<code>BloomConversation</code>","text":"<p>               Bases: <code>Conversation</code></p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>class BloomConversation(Conversation):\n    def __init__(\n        self,\n        model_name: str,\n        prompts: dict,\n        split_correction: bool,\n    ):\n        \"\"\"\n        DEPRECATED: Superceded by XinferenceConversation.\n        \"\"\"\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            split_correction=split_correction,\n        )\n\n        self.messages = []\n\n    def set_api_key(self, api_key: str, user: Optional[str] = None):\n        self.chat = HuggingFaceHub(\n            repo_id=self.model_name,\n            model_kwargs={\"temperature\": 1.0},  # \"regular sampling\"\n            # as per https://huggingface.co/docs/api-inference/detailed_parameters\n            huggingfacehub_api_token=api_key,\n        )\n\n        try:\n            self.chat.generate([\"Hello, I am a biomedical researcher.\"])\n            return True\n        except ValueError as e:\n            return False\n\n    def _cast_messages(self, messages):\n        \"\"\"\n        Render the different roles of the chat-based conversation as plain text.\n        \"\"\"\n        cast = \"\"\n        for m in messages:\n            if isinstance(m, SystemMessage):\n                cast += f\"System: {m.content}\\n\"\n            elif isinstance(m, HumanMessage):\n                cast += f\"Human: {m.content}\\n\"\n            elif isinstance(m, AIMessage):\n                cast += f\"AI: {m.content}\\n\"\n            else:\n                raise ValueError(f\"Unknown message type: {type(m)}\")\n\n        return cast\n\n    def _primary_query(self):\n        response = self.chat.generate([self._cast_messages(self.messages)])\n\n        msg = response.generations[0][0].text\n        token_usage = {\n            \"prompt_tokens\": 0,\n            \"completion_tokens\": 0,\n            \"total_tokens\": 0,\n        }\n\n        self.append_ai_message(msg)\n\n        return msg, token_usage\n\n    def _correct_response(self, msg: str):\n        return \"ok\"\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.BloomConversation.__init__","title":"<code>__init__(model_name, prompts, split_correction)</code>","text":"<p>DEPRECATED: Superceded by XinferenceConversation.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    prompts: dict,\n    split_correction: bool,\n):\n    \"\"\"\n    DEPRECATED: Superceded by XinferenceConversation.\n    \"\"\"\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        split_correction=split_correction,\n    )\n\n    self.messages = []\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.Conversation","title":"<code>Conversation</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Use this class to set up a connection to an LLM API. Can be used to set the user name and API key, append specific messages for system, user, and AI roles (if available), set up the general context as well as manual and tool-based data inputs, and finally to query the API with prompts made by the user.</p> <p>The conversation class is expected to have a <code>messages</code> attribute to store the conversation, and a <code>history</code> attribute, which is a list of messages in a specific format for logging / printing.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>class Conversation(ABC):\n    \"\"\"\n\n    Use this class to set up a connection to an LLM API. Can be used to set the\n    user name and API key, append specific messages for system, user, and AI\n    roles (if available), set up the general context as well as manual and\n    tool-based data inputs, and finally to query the API with prompts made by\n    the user.\n\n    The conversation class is expected to have a `messages` attribute to store\n    the conversation, and a `history` attribute, which is a list of messages in\n    a specific format for logging / printing.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        prompts: dict,\n        correct: bool = True,\n        split_correction: bool = False,\n    ):\n        super().__init__()\n        self.model_name = model_name\n        self.prompts = prompts\n        self.correct = correct\n        self.split_correction = split_correction\n        self.rag_agents: list[RagAgent] = []\n        self.history = []\n        self.messages = []\n        self.ca_messages = []\n        self.current_statements = []\n\n    def set_user_name(self, user_name: str):\n        self.user_name = user_name\n\n    def set_rag_agent(self, agent: RagAgent):\n        \"\"\"\n        Update or insert rag_agent: if the rag_agent with the same mode already\n        exists, it will be updated. Otherwise, the new rag_agent will be inserted.\n        \"\"\"\n        i, _ = self._find_rag_agent(agent.mode)\n        if i &lt; 0:\n            # insert\n            self.rag_agents.append(agent)\n        else:\n            # update\n            self.rag_agents[i] = agent\n\n    def _find_rag_agent(self, mode: str) -&gt; tuple[int, RagAgent]:\n        for i, val in enumerate(self.rag_agents):\n            if val.mode == mode:\n                return i, val\n        return -1, None\n\n    @abstractmethod\n    def set_api_key(self, api_key: str, user: Optional[str] = None):\n        pass\n\n    def get_prompts(self):\n        return self.prompts\n\n    def set_prompts(self, prompts: dict):\n        self.prompts = prompts\n\n    def append_ai_message(self, message: str) -&gt; None:\n        \"\"\"\n        Add a message from the AI to the conversation.\n\n        Args:\n            message (str): The message from the AI.\n        \"\"\"\n        self.messages.append(\n            AIMessage(\n                content=message,\n            ),\n        )\n\n    def append_system_message(self, message: str) -&gt; None:\n        \"\"\"\n        Add a system message to the conversation.\n\n        Args:\n            message (str): The system message.\n        \"\"\"\n        self.messages.append(\n            SystemMessage(\n                content=message,\n            ),\n        )\n\n    def append_ca_message(self, message: str) -&gt; None:\n        \"\"\"\n        Add a message to the correcting agent conversation.\n\n        Args:\n            message (str): The message to the correcting agent.\n        \"\"\"\n        self.ca_messages.append(\n            SystemMessage(\n                content=message,\n            ),\n        )\n\n    def append_user_message(self, message: str) -&gt; None:\n        \"\"\"\n        Add a message from the user to the conversation.\n\n        Args:\n            message (str): The message from the user.\n        \"\"\"\n        self.messages.append(\n            HumanMessage(\n                content=message,\n            ),\n        )\n\n    def append_image_message(\n        self, message: str, image_url: str, local: bool = False\n    ) -&gt; None:\n        \"\"\"\n        Add a user message with an image to the conversation. Also checks, in\n        addition to the `local` flag, if the image URL is a local file path.\n        If it is local, the image will be encoded as a base64 string to be\n        passed to the LLM.\n\n        Args:\n            message (str): The message from the user.\n            image_url (str): The URL of the image.\n            local (bool): Whether the image is local or not. If local, it will\n                be encoded as a base64 string to be passed to the LLM.\n        \"\"\"\n        parsed_url = urllib.parse.urlparse(image_url)\n        if local or not parsed_url.netloc:\n            image_url = f\"data:image/jpeg;base64,{encode_image(image_url)}\"\n        else:\n            image_url = (\n                f\"data:image/jpeg;base64,{encode_image_from_url(image_url)}\"\n            )\n\n        self.messages.append(\n            HumanMessage(\n                content=[\n                    {\"type\": \"text\", \"text\": message},\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n                ],\n            ),\n        )\n\n    def setup(self, context: str):\n        \"\"\"\n        Set up the conversation with general prompts and a context.\n        \"\"\"\n        for msg in self.prompts[\"primary_model_prompts\"]:\n            if msg:\n                self.append_system_message(msg)\n\n        for msg in self.prompts[\"correcting_agent_prompts\"]:\n            if msg:\n                self.append_ca_message(msg)\n\n        self.context = context\n        msg = f\"The topic of the research is {context}.\"\n        self.append_system_message(msg)\n\n    def setup_data_input_manual(self, data_input: str):\n        self.data_input = data_input\n        msg = f\"The user has given information on the data input: {data_input}.\"\n        self.append_system_message(msg)\n\n    def setup_data_input_tool(self, df, input_file_name: str):\n        self.data_input_tool = df\n\n        for tool_name in self.prompts[\"tool_prompts\"]:\n            if tool_name in input_file_name:\n                msg = self.prompts[\"tool_prompts\"][tool_name].format(df=df)\n                self.append_system_message(msg)\n\n    def query(self, text: str, image_url: str = None) -&gt; tuple[str, dict, str]:\n        \"\"\"\n        The main workflow for querying the LLM API. Appends the most recent\n        query to the conversation, optionally injects context from the RAG\n        agent, and runs the primary query method of the child class.\n\n        Args:\n            text (str): The user query.\n\n            image_url (str): The URL of an image to include in the conversation.\n                Optional and only supported for models with vision capabilities.\n\n        Returns:\n            tuple: A tuple containing the response from the API, the token usage\n                information, and the correction if necessary/desired.\n        \"\"\"\n\n        if not image_url:\n            self.append_user_message(text)\n        else:\n            self.append_image_message(text, image_url)\n\n        self._inject_context(text)\n\n        msg, token_usage = self._primary_query()\n\n        if not token_usage:\n            # indicates error\n            return (msg, token_usage, None)\n\n        if not self.correct:\n            return (msg, token_usage, None)\n\n        cor_msg = (\n            \"Correcting (using single sentences) ...\"\n            if self.split_correction\n            else \"Correcting ...\"\n        )\n\n        if st:\n            with st.spinner(cor_msg):\n                corrections = self._correct_query(text)\n        else:\n            corrections = self._correct_query(text)\n\n        if not corrections:\n            return (msg, token_usage, None)\n\n        correction = \"\\n\".join(corrections)\n        return (msg, token_usage, correction)\n\n    def _correct_query(self, msg: str):\n        corrections = []\n        if self.split_correction:\n            nltk.download(\"punkt\")\n            tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n            sentences = tokenizer.tokenize(msg)\n            for sentence in sentences:\n                correction = self._correct_response(sentence)\n\n                if not str(correction).lower() in [\"ok\", \"ok.\"]:\n                    corrections.append(correction)\n        else:\n            correction = self._correct_response(msg)\n\n            if not str(correction).lower() in [\"ok\", \"ok.\"]:\n                corrections.append(correction)\n\n        return corrections\n\n    @abstractmethod\n    def _primary_query(self, text: str):\n        pass\n\n    @abstractmethod\n    def _correct_response(self, msg: str):\n        pass\n\n    def _inject_context(self, text: str):\n        \"\"\"\n\n        Inject the context received from the RAG agent into the prompt. The RAG\n        agent will find the most similar n text fragments and add them to the\n        message history object for usage in the next prompt. Uses the document\n        summarisation prompt set to inject the context. The ultimate prompt\n        should include the placeholder for the statements, `{statements}` (used\n        for formatting the string).\n\n        Args:\n            text (str): The user query to be used for similarity search.\n        \"\"\"\n\n        sim_msg = f\"Performing similarity search to inject fragments ...\"\n\n        if st:\n            with st.spinner(sim_msg):\n                statements = []\n                for agent in self.rag_agents:\n                    try:\n                        docs = agent.generate_responses(text)\n                        statements = statements + [doc[0] for doc in docs]\n                    except ValueError as e:\n                        logger.warning(e)\n\n        else:\n            statements = []\n            for agent in self.rag_agents:\n                try:\n                    docs = agent.generate_responses(text)\n                    statements = statements + [doc[0] for doc in docs]\n                except ValueError as e:\n                    logger.warning(e)\n\n        if statements and len(statements) &gt; 0:\n            prompts = self.prompts[\"rag_agent_prompts\"]\n            self.current_statements = statements\n            for i, prompt in enumerate(prompts):\n                # if last prompt, format the statements into the prompt\n                if i == len(prompts) - 1:\n                    self.append_system_message(\n                        prompt.format(statements=statements)\n                    )\n                else:\n                    self.append_system_message(prompt)\n\n    def get_last_injected_context(self) -&gt; list[dict]:\n        \"\"\"\n        Get a formatted list of the last context injected into the\n        conversation. Contains one dictionary for each RAG mode.\n\n        Returns:\n            List[dict]: A list of dictionaries containing the mode and context\n            for each RAG agent.\n        \"\"\"\n        last_context = []\n        for agent in self.rag_agents:\n            last_context.append(\n                {\"mode\": agent.mode, \"context\": agent.last_response}\n            )\n        return last_context\n\n    def get_msg_json(self):\n        \"\"\"\n        Return a JSON representation (of a list of dicts) of the messages in\n        the conversation. The keys of the dicts are the roles, the values are\n        the messages.\n\n        Returns:\n            str: A JSON representation of the messages in the conversation.\n        \"\"\"\n        d = []\n        for msg in self.messages:\n            if isinstance(msg, SystemMessage):\n                role = \"system\"\n            elif isinstance(msg, HumanMessage):\n                role = \"user\"\n            elif isinstance(msg, AIMessage):\n                role = \"ai\"\n            else:\n                raise ValueError(f\"Unknown message type: {type(msg)}\")\n\n            d.append({role: msg.content})\n\n        return json.dumps(d)\n\n    def reset(self):\n        \"\"\"\n        Resets the conversation to the initial state.\n        \"\"\"\n\n        self.history = []\n        self.messages = []\n        self.ca_messages = []\n        self.current_statements = []\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.Conversation.append_ai_message","title":"<code>append_ai_message(message)</code>","text":"<p>Add a message from the AI to the conversation.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message from the AI.</p> required Source code in <code>biochatter/llm_connect.py</code> <pre><code>def append_ai_message(self, message: str) -&gt; None:\n    \"\"\"\n    Add a message from the AI to the conversation.\n\n    Args:\n        message (str): The message from the AI.\n    \"\"\"\n    self.messages.append(\n        AIMessage(\n            content=message,\n        ),\n    )\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.Conversation.append_ca_message","title":"<code>append_ca_message(message)</code>","text":"<p>Add a message to the correcting agent conversation.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to the correcting agent.</p> required Source code in <code>biochatter/llm_connect.py</code> <pre><code>def append_ca_message(self, message: str) -&gt; None:\n    \"\"\"\n    Add a message to the correcting agent conversation.\n\n    Args:\n        message (str): The message to the correcting agent.\n    \"\"\"\n    self.ca_messages.append(\n        SystemMessage(\n            content=message,\n        ),\n    )\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.Conversation.append_image_message","title":"<code>append_image_message(message, image_url, local=False)</code>","text":"<p>Add a user message with an image to the conversation. Also checks, in addition to the <code>local</code> flag, if the image URL is a local file path. If it is local, the image will be encoded as a base64 string to be passed to the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message from the user.</p> required <code>image_url</code> <code>str</code> <p>The URL of the image.</p> required <code>local</code> <code>bool</code> <p>Whether the image is local or not. If local, it will be encoded as a base64 string to be passed to the LLM.</p> <code>False</code> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def append_image_message(\n    self, message: str, image_url: str, local: bool = False\n) -&gt; None:\n    \"\"\"\n    Add a user message with an image to the conversation. Also checks, in\n    addition to the `local` flag, if the image URL is a local file path.\n    If it is local, the image will be encoded as a base64 string to be\n    passed to the LLM.\n\n    Args:\n        message (str): The message from the user.\n        image_url (str): The URL of the image.\n        local (bool): Whether the image is local or not. If local, it will\n            be encoded as a base64 string to be passed to the LLM.\n    \"\"\"\n    parsed_url = urllib.parse.urlparse(image_url)\n    if local or not parsed_url.netloc:\n        image_url = f\"data:image/jpeg;base64,{encode_image(image_url)}\"\n    else:\n        image_url = (\n            f\"data:image/jpeg;base64,{encode_image_from_url(image_url)}\"\n        )\n\n    self.messages.append(\n        HumanMessage(\n            content=[\n                {\"type\": \"text\", \"text\": message},\n                {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n            ],\n        ),\n    )\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.Conversation.append_system_message","title":"<code>append_system_message(message)</code>","text":"<p>Add a system message to the conversation.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The system message.</p> required Source code in <code>biochatter/llm_connect.py</code> <pre><code>def append_system_message(self, message: str) -&gt; None:\n    \"\"\"\n    Add a system message to the conversation.\n\n    Args:\n        message (str): The system message.\n    \"\"\"\n    self.messages.append(\n        SystemMessage(\n            content=message,\n        ),\n    )\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.Conversation.append_user_message","title":"<code>append_user_message(message)</code>","text":"<p>Add a message from the user to the conversation.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message from the user.</p> required Source code in <code>biochatter/llm_connect.py</code> <pre><code>def append_user_message(self, message: str) -&gt; None:\n    \"\"\"\n    Add a message from the user to the conversation.\n\n    Args:\n        message (str): The message from the user.\n    \"\"\"\n    self.messages.append(\n        HumanMessage(\n            content=message,\n        ),\n    )\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.Conversation.get_last_injected_context","title":"<code>get_last_injected_context()</code>","text":"<p>Get a formatted list of the last context injected into the conversation. Contains one dictionary for each RAG mode.</p> <p>Returns:</p> Type Description <code>list[dict]</code> <p>List[dict]: A list of dictionaries containing the mode and context</p> <code>list[dict]</code> <p>for each RAG agent.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def get_last_injected_context(self) -&gt; list[dict]:\n    \"\"\"\n    Get a formatted list of the last context injected into the\n    conversation. Contains one dictionary for each RAG mode.\n\n    Returns:\n        List[dict]: A list of dictionaries containing the mode and context\n        for each RAG agent.\n    \"\"\"\n    last_context = []\n    for agent in self.rag_agents:\n        last_context.append(\n            {\"mode\": agent.mode, \"context\": agent.last_response}\n        )\n    return last_context\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.Conversation.get_msg_json","title":"<code>get_msg_json()</code>","text":"<p>Return a JSON representation (of a list of dicts) of the messages in the conversation. The keys of the dicts are the roles, the values are the messages.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>A JSON representation of the messages in the conversation.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def get_msg_json(self):\n    \"\"\"\n    Return a JSON representation (of a list of dicts) of the messages in\n    the conversation. The keys of the dicts are the roles, the values are\n    the messages.\n\n    Returns:\n        str: A JSON representation of the messages in the conversation.\n    \"\"\"\n    d = []\n    for msg in self.messages:\n        if isinstance(msg, SystemMessage):\n            role = \"system\"\n        elif isinstance(msg, HumanMessage):\n            role = \"user\"\n        elif isinstance(msg, AIMessage):\n            role = \"ai\"\n        else:\n            raise ValueError(f\"Unknown message type: {type(msg)}\")\n\n        d.append({role: msg.content})\n\n    return json.dumps(d)\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.Conversation.query","title":"<code>query(text, image_url=None)</code>","text":"<p>The main workflow for querying the LLM API. Appends the most recent query to the conversation, optionally injects context from the RAG agent, and runs the primary query method of the child class.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The user query.</p> required <code>image_url</code> <code>str</code> <p>The URL of an image to include in the conversation. Optional and only supported for models with vision capabilities.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[str, dict, str]</code> <p>A tuple containing the response from the API, the token usage information, and the correction if necessary/desired.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def query(self, text: str, image_url: str = None) -&gt; tuple[str, dict, str]:\n    \"\"\"\n    The main workflow for querying the LLM API. Appends the most recent\n    query to the conversation, optionally injects context from the RAG\n    agent, and runs the primary query method of the child class.\n\n    Args:\n        text (str): The user query.\n\n        image_url (str): The URL of an image to include in the conversation.\n            Optional and only supported for models with vision capabilities.\n\n    Returns:\n        tuple: A tuple containing the response from the API, the token usage\n            information, and the correction if necessary/desired.\n    \"\"\"\n\n    if not image_url:\n        self.append_user_message(text)\n    else:\n        self.append_image_message(text, image_url)\n\n    self._inject_context(text)\n\n    msg, token_usage = self._primary_query()\n\n    if not token_usage:\n        # indicates error\n        return (msg, token_usage, None)\n\n    if not self.correct:\n        return (msg, token_usage, None)\n\n    cor_msg = (\n        \"Correcting (using single sentences) ...\"\n        if self.split_correction\n        else \"Correcting ...\"\n    )\n\n    if st:\n        with st.spinner(cor_msg):\n            corrections = self._correct_query(text)\n    else:\n        corrections = self._correct_query(text)\n\n    if not corrections:\n        return (msg, token_usage, None)\n\n    correction = \"\\n\".join(corrections)\n    return (msg, token_usage, correction)\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.Conversation.reset","title":"<code>reset()</code>","text":"<p>Resets the conversation to the initial state.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def reset(self):\n    \"\"\"\n    Resets the conversation to the initial state.\n    \"\"\"\n\n    self.history = []\n    self.messages = []\n    self.ca_messages = []\n    self.current_statements = []\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.Conversation.set_rag_agent","title":"<code>set_rag_agent(agent)</code>","text":"<p>Update or insert rag_agent: if the rag_agent with the same mode already exists, it will be updated. Otherwise, the new rag_agent will be inserted.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def set_rag_agent(self, agent: RagAgent):\n    \"\"\"\n    Update or insert rag_agent: if the rag_agent with the same mode already\n    exists, it will be updated. Otherwise, the new rag_agent will be inserted.\n    \"\"\"\n    i, _ = self._find_rag_agent(agent.mode)\n    if i &lt; 0:\n        # insert\n        self.rag_agents.append(agent)\n    else:\n        # update\n        self.rag_agents[i] = agent\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.Conversation.setup","title":"<code>setup(context)</code>","text":"<p>Set up the conversation with general prompts and a context.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def setup(self, context: str):\n    \"\"\"\n    Set up the conversation with general prompts and a context.\n    \"\"\"\n    for msg in self.prompts[\"primary_model_prompts\"]:\n        if msg:\n            self.append_system_message(msg)\n\n    for msg in self.prompts[\"correcting_agent_prompts\"]:\n        if msg:\n            self.append_ca_message(msg)\n\n    self.context = context\n    msg = f\"The topic of the research is {context}.\"\n    self.append_system_message(msg)\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.GptConversation","title":"<code>GptConversation</code>","text":"<p>               Bases: <code>Conversation</code></p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>class GptConversation(Conversation):\n    def __init__(\n        self,\n        model_name: str,\n        prompts: dict,\n        correct: bool = True,\n        split_correction: bool = False,\n    ):\n        \"\"\"\n        Connect to OpenAI's GPT API and set up a conversation with the user.\n        Also initialise a second conversational agent to provide corrections to\n        the model output, if necessary.\n\n        Args:\n            model_name (str): The name of the model to use.\n\n            prompts (dict): A dictionary of prompts to use for the conversation.\n\n            split_correction (bool): Whether to correct the model output by\n                splitting the output into sentences and correcting each\n                sentence individually.\n        \"\"\"\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            correct=correct,\n            split_correction=split_correction,\n        )\n\n        self.ca_model_name = \"gpt-3.5-turbo\"\n        # TODO make accessible by drop-down\n\n    def set_api_key(self, api_key: str, user: str):\n        \"\"\"\n        Set the API key for the OpenAI API. If the key is valid, initialise the\n        conversational agent. Set the user for usage statistics.\n\n        Args:\n            api_key (str): The API key for the OpenAI API.\n\n            user (str): The user for usage statistics.\n\n        Returns:\n            bool: True if the API key is valid, False otherwise.\n        \"\"\"\n        client = openai.OpenAI(\n            api_key=api_key,\n        )\n        self.user = user\n\n        try:\n            client.models.list()\n            self.chat = ChatOpenAI(\n                model_name=self.model_name,\n                temperature=0,\n                openai_api_key=api_key,\n            )\n            self.ca_chat = ChatOpenAI(\n                model_name=self.ca_model_name,\n                temperature=0,\n                openai_api_key=api_key,\n            )\n            if user == \"community\":\n                self.usage_stats = get_stats(user=user)\n\n            return True\n\n        except openai._exceptions.AuthenticationError as e:\n            return False\n\n    def _primary_query(self):\n        \"\"\"\n        Query the OpenAI API with the user's message and return the response\n        using the message history (flattery system messages, prior conversation)\n        as context. Correct the response if necessary.\n\n        Returns:\n            tuple: A tuple containing the response from the OpenAI API and the\n                token usage.\n        \"\"\"\n        try:\n            response = self.chat.generate([self.messages])\n        except (\n            openai._exceptions.APIError,\n            openai._exceptions.OpenAIError,\n            openai._exceptions.ConflictError,\n            openai._exceptions.NotFoundError,\n            openai._exceptions.APIStatusError,\n            openai._exceptions.RateLimitError,\n            openai._exceptions.APITimeoutError,\n            openai._exceptions.BadRequestError,\n            openai._exceptions.APIConnectionError,\n            openai._exceptions.AuthenticationError,\n            openai._exceptions.InternalServerError,\n            openai._exceptions.PermissionDeniedError,\n            openai._exceptions.UnprocessableEntityError,\n            openai._exceptions.APIResponseValidationError,\n        ) as e:\n            return str(e), None\n\n        msg = response.generations[0][0].text\n        token_usage = response.llm_output.get(\"token_usage\")\n\n        self._update_usage_stats(self.model_name, token_usage)\n\n        self.append_ai_message(msg)\n\n        return msg, token_usage\n\n    def _correct_response(self, msg: str):\n        \"\"\"\n        Correct the response from the OpenAI API by sending it to a secondary\n        language model. Optionally split the response into single sentences and\n        correct each sentence individually. Update usage stats.\n\n        Args:\n            msg (str): The response from the OpenAI API.\n\n        Returns:\n            str: The corrected response (or OK if no correction necessary).\n        \"\"\"\n        ca_messages = self.ca_messages.copy()\n        ca_messages.append(\n            HumanMessage(\n                content=msg,\n            ),\n        )\n        ca_messages.append(\n            SystemMessage(\n                content=\"If there is nothing to correct, please respond \"\n                \"with just 'OK', and nothing else!\",\n            ),\n        )\n\n        response = self.ca_chat.generate([ca_messages])\n\n        correction = response.generations[0][0].text\n        token_usage = response.llm_output.get(\"token_usage\")\n\n        self._update_usage_stats(self.ca_model_name, token_usage)\n\n        return correction\n\n    def _update_usage_stats(self, model: str, token_usage: dict):\n        \"\"\"\n        Update redis database with token usage statistics using the usage_stats\n        object with the increment method.\n\n        Args:\n            model (str): The model name.\n\n            token_usage (dict): The token usage statistics.\n        \"\"\"\n        if self.user == \"community\":\n            self.usage_stats.increment(\n                f\"usage:[date]:[user]\",\n                {f\"{k}:{model}\": v for k, v in token_usage.items()},\n            )\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.GptConversation.__init__","title":"<code>__init__(model_name, prompts, correct=True, split_correction=False)</code>","text":"<p>Connect to OpenAI's GPT API and set up a conversation with the user. Also initialise a second conversational agent to provide corrections to the model output, if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to use.</p> required <code>prompts</code> <code>dict</code> <p>A dictionary of prompts to use for the conversation.</p> required <code>split_correction</code> <code>bool</code> <p>Whether to correct the model output by splitting the output into sentences and correcting each sentence individually.</p> <code>False</code> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    prompts: dict,\n    correct: bool = True,\n    split_correction: bool = False,\n):\n    \"\"\"\n    Connect to OpenAI's GPT API and set up a conversation with the user.\n    Also initialise a second conversational agent to provide corrections to\n    the model output, if necessary.\n\n    Args:\n        model_name (str): The name of the model to use.\n\n        prompts (dict): A dictionary of prompts to use for the conversation.\n\n        split_correction (bool): Whether to correct the model output by\n            splitting the output into sentences and correcting each\n            sentence individually.\n    \"\"\"\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        correct=correct,\n        split_correction=split_correction,\n    )\n\n    self.ca_model_name = \"gpt-3.5-turbo\"\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.GptConversation.set_api_key","title":"<code>set_api_key(api_key, user)</code>","text":"<p>Set the API key for the OpenAI API. If the key is valid, initialise the conversational agent. Set the user for usage statistics.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The API key for the OpenAI API.</p> required <code>user</code> <code>str</code> <p>The user for usage statistics.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the API key is valid, False otherwise.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def set_api_key(self, api_key: str, user: str):\n    \"\"\"\n    Set the API key for the OpenAI API. If the key is valid, initialise the\n    conversational agent. Set the user for usage statistics.\n\n    Args:\n        api_key (str): The API key for the OpenAI API.\n\n        user (str): The user for usage statistics.\n\n    Returns:\n        bool: True if the API key is valid, False otherwise.\n    \"\"\"\n    client = openai.OpenAI(\n        api_key=api_key,\n    )\n    self.user = user\n\n    try:\n        client.models.list()\n        self.chat = ChatOpenAI(\n            model_name=self.model_name,\n            temperature=0,\n            openai_api_key=api_key,\n        )\n        self.ca_chat = ChatOpenAI(\n            model_name=self.ca_model_name,\n            temperature=0,\n            openai_api_key=api_key,\n        )\n        if user == \"community\":\n            self.usage_stats = get_stats(user=user)\n\n        return True\n\n    except openai._exceptions.AuthenticationError as e:\n        return False\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.OllamaConversation","title":"<code>OllamaConversation</code>","text":"<p>               Bases: <code>Conversation</code></p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>class OllamaConversation(Conversation):\n    def set_api_key(self, api_key: str, user: Optional[str] = None):\n        pass\n\n    def __init__(\n        self,\n        base_url: str,\n        prompts: dict,\n        model_name: str = \"llama3\",\n        correct: bool = True,\n        split_correction: bool = False,\n    ):\n        \"\"\"\n\n        Connect to an Ollama LLM via the Ollama/Langchain library and set\n        up a conversation with the user. Also initialise a second\n        conversational agent to provide corrections to the model output, if\n        necessary.\n\n        Args:\n\n            base_url (str): The base URL of the Ollama instance.\n\n            prompts (dict): A dictionary of prompts to use for the conversation.\n\n            model_name (str): The name of the model to use. Can be any model name available in your Ollama instance.\n\n            correct (bool): Whether to correct the model output.\n\n            split_correction (bool): Whether to correct the model output by\n            splitting the output into sentences and correcting each sentence\n            individually.\n\n        \"\"\"\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            correct=correct,\n            split_correction=split_correction,\n        )\n        self.model_name = model_name\n        self.model = ChatOllama(\n            base_url=base_url, model=self.model_name, temperature=0.0\n        )\n\n        self.ca_model_name = \"mixtral:latest\"\n\n        self.ca_model = ChatOllama(\n            base_url=base_url, model_name=self.ca_model_name, temperature=0.0\n        )\n\n    def append_system_message(self, message: str):\n        \"\"\"\n        We override the system message addition because Ollama does not\n        accept multiple system messages. We concatenate them if there are\n        multiple.\n\n        Args:\n            message (str): The message to append.\n        \"\"\"\n        # if there is not already a system message in self.messages\n        if not any(isinstance(m, SystemMessage) for m in self.messages):\n            self.messages.append(\n                SystemMessage(\n                    content=message,\n                ),\n            )\n        else:\n            # if there is a system message, append to the last one\n            for i, msg in enumerate(self.messages):\n                if isinstance(msg, SystemMessage):\n                    self.messages[i].content += f\"\\n{message}\"\n                    break\n\n    def append_ca_message(self, message: str):\n        \"\"\"\n\n        We also override the system message addition for the correcting agent,\n        likewise because Ollama does not accept multiple system messages. We\n        concatenate them if there are multiple.\n\n        TODO this currently assumes that the correcting agent is the same model\n        as the primary one.\n\n        Args:\n            message (str): The message to append.\n        \"\"\"\n        # if there is not already a system message in self.messages\n        if not any(isinstance(m, SystemMessage) for m in self.ca_messages):\n            self.ca_messages.append(\n                SystemMessage(\n                    content=message,\n                ),\n            )\n        else:\n            # if there is a system message, append to the last one\n            for i, msg in enumerate(self.ca_messages):\n                if isinstance(msg, SystemMessage):\n                    self.ca_messages[i].content += f\"\\n{message}\"\n                    break\n\n    def _primary_query(self):\n        \"\"\"\n\n        Query the Ollama client API with the user's message and return the\n        response using the message history (flattery system messages, prior\n        conversation) as context. Correct the response if necessary.\n\n        Returns:\n\n            tuple: A tuple containing the response from the Ollama API\n            (formatted similarly to responses from the OpenAI API) and the token\n            usage.\n\n        \"\"\"\n        try:\n            messages = self._create_history(self.messages)\n            response = self.model.invoke(\n                messages\n                # ,generate_config={\"max_tokens\": 2048, \"temperature\": 0},\n            )\n        except (\n            openai._exceptions.APIError,\n            openai._exceptions.OpenAIError,\n            openai._exceptions.ConflictError,\n            openai._exceptions.NotFoundError,\n            openai._exceptions.APIStatusError,\n            openai._exceptions.RateLimitError,\n            openai._exceptions.APITimeoutError,\n            openai._exceptions.BadRequestError,\n            openai._exceptions.APIConnectionError,\n            openai._exceptions.AuthenticationError,\n            openai._exceptions.InternalServerError,\n            openai._exceptions.PermissionDeniedError,\n            openai._exceptions.UnprocessableEntityError,\n            openai._exceptions.APIResponseValidationError,\n        ) as e:\n            return str(e), None\n        response_dict = response.dict()\n        msg = response_dict[\"content\"]\n        token_usage = response_dict[\"response_metadata\"][\"eval_count\"]\n\n        self._update_usage_stats(self.model_name, token_usage)\n\n        self.append_ai_message(msg)\n\n        return msg, token_usage\n\n    def _create_history(self, messages):\n        history = []\n        for i, m in enumerate(messages):\n            if isinstance(m, AIMessage):\n                history.append(AIMessage(content=m.content))\n            elif isinstance(m, HumanMessage):\n                history.append(HumanMessage(content=m.content))\n            elif isinstance(m, SystemMessage):\n                history.append(SystemMessage(content=m.content))\n\n        return history\n\n    def _correct_response(self, msg: str):\n        \"\"\"\n\n        Correct the response from the Ollama API by sending it to a\n        secondary language model. Optionally split the response into single\n        sentences and correct each sentence individually. Update usage stats.\n\n        Args:\n            msg (str): The response from the model.\n\n        Returns:\n            str: The corrected response (or OK if no correction necessary).\n        \"\"\"\n        ca_messages = self.ca_messages.copy()\n        ca_messages.append(\n            HumanMessage(\n                content=msg,\n            ),\n        )\n        ca_messages.append(\n            SystemMessage(\n                content=\"If there is nothing to correct, please respond \"\n                \"with just 'OK', and nothing else!\",\n            ),\n        )\n        response = self.ca_model.invoke(\n            chat_history=self._create_history(self.messages)\n        ).dict()\n        correction = response[\"content\"]\n        token_usage = response[\"eval_count\"]\n\n        self._update_usage_stats(self.ca_model_name, token_usage)\n\n        return correction\n\n    def _update_usage_stats(self, model: str, token_usage: dict):\n        \"\"\"\n        Update redis database with token usage statistics using the usage_stats\n        object with the increment method.\n\n        Args:\n            model (str): The model name.\n\n            token_usage (dict): The token usage statistics.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.OllamaConversation.__init__","title":"<code>__init__(base_url, prompts, model_name='llama3', correct=True, split_correction=False)</code>","text":"<p>Connect to an Ollama LLM via the Ollama/Langchain library and set up a conversation with the user. Also initialise a second conversational agent to provide corrections to the model output, if necessary.</p> <p>Args:</p> <pre><code>base_url (str): The base URL of the Ollama instance.\n\nprompts (dict): A dictionary of prompts to use for the conversation.\n\nmodel_name (str): The name of the model to use. Can be any model name available in your Ollama instance.\n\ncorrect (bool): Whether to correct the model output.\n\nsplit_correction (bool): Whether to correct the model output by\nsplitting the output into sentences and correcting each sentence\nindividually.\n</code></pre> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def __init__(\n    self,\n    base_url: str,\n    prompts: dict,\n    model_name: str = \"llama3\",\n    correct: bool = True,\n    split_correction: bool = False,\n):\n    \"\"\"\n\n    Connect to an Ollama LLM via the Ollama/Langchain library and set\n    up a conversation with the user. Also initialise a second\n    conversational agent to provide corrections to the model output, if\n    necessary.\n\n    Args:\n\n        base_url (str): The base URL of the Ollama instance.\n\n        prompts (dict): A dictionary of prompts to use for the conversation.\n\n        model_name (str): The name of the model to use. Can be any model name available in your Ollama instance.\n\n        correct (bool): Whether to correct the model output.\n\n        split_correction (bool): Whether to correct the model output by\n        splitting the output into sentences and correcting each sentence\n        individually.\n\n    \"\"\"\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        correct=correct,\n        split_correction=split_correction,\n    )\n    self.model_name = model_name\n    self.model = ChatOllama(\n        base_url=base_url, model=self.model_name, temperature=0.0\n    )\n\n    self.ca_model_name = \"mixtral:latest\"\n\n    self.ca_model = ChatOllama(\n        base_url=base_url, model_name=self.ca_model_name, temperature=0.0\n    )\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.OllamaConversation.append_ca_message","title":"<code>append_ca_message(message)</code>","text":"<p>We also override the system message addition for the correcting agent, likewise because Ollama does not accept multiple system messages. We concatenate them if there are multiple.</p> <p>TODO this currently assumes that the correcting agent is the same model as the primary one.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to append.</p> required Source code in <code>biochatter/llm_connect.py</code> <pre><code>def append_ca_message(self, message: str):\n    \"\"\"\n\n    We also override the system message addition for the correcting agent,\n    likewise because Ollama does not accept multiple system messages. We\n    concatenate them if there are multiple.\n\n    TODO this currently assumes that the correcting agent is the same model\n    as the primary one.\n\n    Args:\n        message (str): The message to append.\n    \"\"\"\n    # if there is not already a system message in self.messages\n    if not any(isinstance(m, SystemMessage) for m in self.ca_messages):\n        self.ca_messages.append(\n            SystemMessage(\n                content=message,\n            ),\n        )\n    else:\n        # if there is a system message, append to the last one\n        for i, msg in enumerate(self.ca_messages):\n            if isinstance(msg, SystemMessage):\n                self.ca_messages[i].content += f\"\\n{message}\"\n                break\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.OllamaConversation.append_system_message","title":"<code>append_system_message(message)</code>","text":"<p>We override the system message addition because Ollama does not accept multiple system messages. We concatenate them if there are multiple.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to append.</p> required Source code in <code>biochatter/llm_connect.py</code> <pre><code>def append_system_message(self, message: str):\n    \"\"\"\n    We override the system message addition because Ollama does not\n    accept multiple system messages. We concatenate them if there are\n    multiple.\n\n    Args:\n        message (str): The message to append.\n    \"\"\"\n    # if there is not already a system message in self.messages\n    if not any(isinstance(m, SystemMessage) for m in self.messages):\n        self.messages.append(\n            SystemMessage(\n                content=message,\n            ),\n        )\n    else:\n        # if there is a system message, append to the last one\n        for i, msg in enumerate(self.messages):\n            if isinstance(msg, SystemMessage):\n                self.messages[i].content += f\"\\n{message}\"\n                break\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.WasmConversation","title":"<code>WasmConversation</code>","text":"<p>               Bases: <code>Conversation</code></p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>class WasmConversation(Conversation):\n    def __init__(\n        self,\n        model_name: str,\n        prompts: dict,\n        correct: bool = True,\n        split_correction: bool = False,\n    ):\n        \"\"\"\n\n        This class is used to return the complete query as a string to be used\n        in the frontend running the wasm model. It does not call the API itself,\n        but updates the message history similarly to the other conversation\n        classes. It overrides the `query` method from the `Conversation` class\n        to return a plain string that contains the entire message for the model\n        as the first element of the tuple. The second and third elements are\n        `None` as there is no token usage or correction for the wasm model.\n\n        \"\"\"\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            correct=correct,\n            split_correction=split_correction,\n        )\n\n    def query(self, text: str):\n        \"\"\"\n        Return the entire message history as a single string. This is the\n        message that is sent to the wasm model.\n\n        Args:\n            text (str): The user query.\n\n            collection_name (str): The name of the collection to use for\n                retrieval-augmented generation.\n\n        Returns:\n            tuple: A tuple containing the message history as a single string,\n                and `None` for the second and third elements of the tuple.\n        \"\"\"\n        self.append_user_message(text)\n\n        self._inject_context(text)\n\n        return (self._primary_query(), None, None)\n\n    def _primary_query(self):\n        \"\"\"\n        Concatenate all messages in the conversation into a single string and\n        return it. Currently discards information about roles (system, user).\n        \"\"\"\n        return \"\\n\".join([m.content for m in self.messages])\n\n    def _correct_response(self, msg: str):\n        \"\"\"\n        This method is not used for the wasm model.\n        \"\"\"\n        return \"ok\"\n\n    def set_api_key(self, api_key: str, user: str | None = None):\n        \"\"\"\n        This method is not used for the wasm model.\n        \"\"\"\n        return True\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.WasmConversation.__init__","title":"<code>__init__(model_name, prompts, correct=True, split_correction=False)</code>","text":"<p>This class is used to return the complete query as a string to be used in the frontend running the wasm model. It does not call the API itself, but updates the message history similarly to the other conversation classes. It overrides the <code>query</code> method from the <code>Conversation</code> class to return a plain string that contains the entire message for the model as the first element of the tuple. The second and third elements are <code>None</code> as there is no token usage or correction for the wasm model.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    prompts: dict,\n    correct: bool = True,\n    split_correction: bool = False,\n):\n    \"\"\"\n\n    This class is used to return the complete query as a string to be used\n    in the frontend running the wasm model. It does not call the API itself,\n    but updates the message history similarly to the other conversation\n    classes. It overrides the `query` method from the `Conversation` class\n    to return a plain string that contains the entire message for the model\n    as the first element of the tuple. The second and third elements are\n    `None` as there is no token usage or correction for the wasm model.\n\n    \"\"\"\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        correct=correct,\n        split_correction=split_correction,\n    )\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.WasmConversation.query","title":"<code>query(text)</code>","text":"<p>Return the entire message history as a single string. This is the message that is sent to the wasm model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The user query.</p> required <code>collection_name</code> <code>str</code> <p>The name of the collection to use for retrieval-augmented generation.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing the message history as a single string, and <code>None</code> for the second and third elements of the tuple.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def query(self, text: str):\n    \"\"\"\n    Return the entire message history as a single string. This is the\n    message that is sent to the wasm model.\n\n    Args:\n        text (str): The user query.\n\n        collection_name (str): The name of the collection to use for\n            retrieval-augmented generation.\n\n    Returns:\n        tuple: A tuple containing the message history as a single string,\n            and `None` for the second and third elements of the tuple.\n    \"\"\"\n    self.append_user_message(text)\n\n    self._inject_context(text)\n\n    return (self._primary_query(), None, None)\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.WasmConversation.set_api_key","title":"<code>set_api_key(api_key, user=None)</code>","text":"<p>This method is not used for the wasm model.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def set_api_key(self, api_key: str, user: str | None = None):\n    \"\"\"\n    This method is not used for the wasm model.\n    \"\"\"\n    return True\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.XinferenceConversation","title":"<code>XinferenceConversation</code>","text":"<p>               Bases: <code>Conversation</code></p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>class XinferenceConversation(Conversation):\n    def __init__(\n        self,\n        base_url: str,\n        prompts: dict,\n        model_name: str = \"auto\",\n        correct: bool = True,\n        split_correction: bool = False,\n    ):\n        \"\"\"\n\n        Connect to an open-source LLM via the Xinference client library and set\n        up a conversation with the user.  Also initialise a second\n        conversational agent to provide corrections to the model output, if\n        necessary.\n\n        Args:\n\n            base_url (str): The base URL of the Xinference instance (should not\n            include the /v1 part).\n\n            prompts (dict): A dictionary of prompts to use for the conversation.\n\n            model_name (str): The name of the model to use. Will be mapped to\n            the according uid from the list of available models. Can be set to\n            \"auto\" to use the first available model.\n\n            correct (bool): Whether to correct the model output.\n\n            split_correction (bool): Whether to correct the model output by\n            splitting the output into sentences and correcting each sentence\n            individually.\n\n        \"\"\"\n        # Shaohong: Please keep this xinference importing code here, so that,\n        # we don't need to depend on xinference if we dont need it (xinference\n        # is expensive to install)\n        from xinference.client import Client\n\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            correct=correct,\n            split_correction=split_correction,\n        )\n        self.client = Client(base_url=base_url)\n\n        self.models = {}\n        self.load_models()\n\n        self.ca_model_name = model_name\n\n        self.set_api_key()\n\n        # TODO make accessible by drop-down\n\n    def load_models(self):\n        for id, model in self.client.list_models().items():\n            model[\"id\"] = id\n            self.models[model[\"model_name\"]] = model\n\n    # def list_models_by_type(self, type: str):\n    #     names = []\n    #     if type == 'embed' or type == 'embedding':\n    #         for name, model in self.models.items():\n    #             if \"model_ability\" in model:\n    #                 if \"embed\" in model[\"model_ability\"]:\n    #                     names.append(name)\n    #             elif model[\"model_type\"] == \"embedding\":\n    #                 names.append(name)\n    #         return names\n    #     for name, model in self.models.items():\n    #         if \"model_ability\" in model:\n    #             if type in model[\"model_ability\"]:\n    #                 names.append(name)\n    #         elif model[\"model_type\"] == type:\n    #             names.append(name)\n    #     return names\n\n    def append_system_message(self, message: str):\n        \"\"\"\n        We override the system message addition because Xinference does not\n        accept multiple system messages. We concatenate them if there are\n        multiple.\n\n        Args:\n            message (str): The message to append.\n        \"\"\"\n        # if there is not already a system message in self.messages\n        if not any(isinstance(m, SystemMessage) for m in self.messages):\n            self.messages.append(\n                SystemMessage(\n                    content=message,\n                ),\n            )\n        else:\n            # if there is a system message, append to the last one\n            for i, msg in enumerate(self.messages):\n                if isinstance(msg, SystemMessage):\n                    self.messages[i].content += f\"\\n{message}\"\n                    break\n\n    def append_ca_message(self, message: str):\n        \"\"\"\n\n        We also override the system message addition for the correcting agent,\n        likewise because Xinference does not accept multiple system messages. We\n        concatenate them if there are multiple.\n\n        TODO this currently assumes that the correcting agent is the same model\n        as the primary one.\n\n        Args:\n            message (str): The message to append.\n        \"\"\"\n        # if there is not already a system message in self.messages\n        if not any(isinstance(m, SystemMessage) for m in self.ca_messages):\n            self.ca_messages.append(\n                SystemMessage(\n                    content=message,\n                ),\n            )\n        else:\n            # if there is a system message, append to the last one\n            for i, msg in enumerate(self.ca_messages):\n                if isinstance(msg, SystemMessage):\n                    self.ca_messages[i].content += f\"\\n{message}\"\n                    break\n\n    def _primary_query(self):\n        \"\"\"\n\n        Query the Xinference client API with the user's message and return the\n        response using the message history (flattery system messages, prior\n        conversation) as context. Correct the response if necessary.\n\n        LLaMA2 architecture does not accept separate system messages, so we\n        concatenate the system message with the user message to form the prompt.\n        'LLaMA enforces a strict rule that chats should alternate\n        user/assistant/user/assistant, and the system message, if present,\n        should be embedded into the first user message.' (from\n        https://discuss.huggingface.co/t/issue-with-llama-2-chat-template-and-out-of-date-documentation/61645/3)\n\n        Returns:\n\n            tuple: A tuple containing the response from the Xinference API\n            (formatted similarly to responses from the OpenAI API) and the token\n            usage.\n\n        \"\"\"\n        try:\n            history = self._create_history()\n            # TODO this is for LLaMA2 arch, may be different for newer models\n            prompt = history.pop()\n            response = self.model.chat(\n                prompt=prompt[\"content\"],\n                chat_history=history,\n                generate_config={\"max_tokens\": 2048, \"temperature\": 0},\n            )\n        except (\n            openai._exceptions.APIError,\n            openai._exceptions.OpenAIError,\n            openai._exceptions.ConflictError,\n            openai._exceptions.NotFoundError,\n            openai._exceptions.APIStatusError,\n            openai._exceptions.RateLimitError,\n            openai._exceptions.APITimeoutError,\n            openai._exceptions.BadRequestError,\n            openai._exceptions.APIConnectionError,\n            openai._exceptions.AuthenticationError,\n            openai._exceptions.InternalServerError,\n            openai._exceptions.PermissionDeniedError,\n            openai._exceptions.UnprocessableEntityError,\n            openai._exceptions.APIResponseValidationError,\n        ) as e:\n            return str(e), None\n\n        msg = response[\"choices\"][0][\"message\"][\"content\"]\n        token_usage = response[\"usage\"]\n\n        self._update_usage_stats(self.model_name, token_usage)\n\n        self.append_ai_message(msg)\n\n        return msg, token_usage\n\n    def _create_history(self):\n        history = []\n        # extract text components from message contents\n        msg_texts = [\n            m.content[0][\"text\"] if isinstance(m.content, list) else m.content\n            for m in self.messages\n        ]\n\n        # check if last message is an image message\n        is_image_message = False\n        if isinstance(self.messages[-1].content, list):\n            is_image_message = (\n                self.messages[-1].content[1][\"type\"] == \"image_url\"\n            )\n\n        # find location of last AI message (if any)\n        last_ai_message = None\n        for i, m in enumerate(self.messages):\n            if isinstance(m, AIMessage):\n                last_ai_message = i\n\n        # concatenate all messages before the last AI message into one message\n        if last_ai_message:\n            history.append(\n                {\n                    \"role\": \"user\",\n                    \"content\": \"\\n\".join(\n                        [m for m in msg_texts[:last_ai_message]]\n                    ),\n                }\n            )\n            # then append the last AI message\n            history.append(\n                {\n                    \"role\": \"assistant\",\n                    \"content\": msg_texts[last_ai_message],\n                }\n            )\n\n            # then concatenate all messages after that\n            # into one HumanMessage\n            history.append(\n                {\n                    \"role\": \"user\",\n                    \"content\": \"\\n\".join(\n                        [m for m in msg_texts[last_ai_message + 1 :]]\n                    ),\n                }\n            )\n\n        # if there is no AI message, concatenate all messages into one user\n        # message\n        else:\n            history.append(\n                {\n                    \"role\": \"user\",\n                    \"content\": \"\\n\".join([m for m in msg_texts[:]]),\n                }\n            )\n\n        # if the last message is an image message, add the image to the history\n        if is_image_message:\n            history[-1][\"content\"] = [\n                {\"type\": \"text\", \"text\": history[-1][\"content\"]},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": self.messages[-1].content[1][\"image_url\"][\"url\"]\n                    },\n                },\n            ]\n        return history\n\n    def _correct_response(self, msg: str):\n        \"\"\"\n\n        Correct the response from the Xinference API by sending it to a\n        secondary language model. Optionally split the response into single\n        sentences and correct each sentence individually. Update usage stats.\n\n        Args:\n            msg (str): The response from the model.\n\n        Returns:\n            str: The corrected response (or OK if no correction necessary).\n        \"\"\"\n        ca_messages = self.ca_messages.copy()\n        ca_messages.append(\n            HumanMessage(\n                content=msg,\n            ),\n        )\n        ca_messages.append(\n            SystemMessage(\n                content=\"If there is nothing to correct, please respond \"\n                \"with just 'OK', and nothing else!\",\n            ),\n        )\n        history = []\n        for m in self.messages:\n            if isinstance(m, SystemMessage):\n                history.append({\"role\": \"system\", \"content\": m.content})\n            elif isinstance(m, HumanMessage):\n                history.append({\"role\": \"user\", \"content\": m.content})\n            elif isinstance(m, AIMessage):\n                history.append({\"role\": \"assistant\", \"content\": m.content})\n        prompt = history.pop()\n        response = self.ca_model.chat(\n            prompt=prompt[\"content\"],\n            chat_history=history,\n            generate_config={\"max_tokens\": 2048, \"temperature\": 0},\n        )\n\n        correction = response[\"choices\"][0][\"message\"][\"content\"]\n        token_usage = response[\"usage\"]\n\n        self._update_usage_stats(self.ca_model_name, token_usage)\n\n        return correction\n\n    def _update_usage_stats(self, model: str, token_usage: dict):\n        \"\"\"\n        Update redis database with token usage statistics using the usage_stats\n        object with the increment method.\n\n        Args:\n            model (str): The model name.\n\n            token_usage (dict): The token usage statistics.\n        \"\"\"\n        pass\n\n    def set_api_key(self):\n        \"\"\"\n        Try to get the Xinference model from the client API. If the model is\n        found, initialise the conversational agent. If the model is not found,\n        `get_model` will raise a RuntimeError.\n\n        Returns:\n            bool: True if the model is found, False otherwise.\n        \"\"\"\n\n        try:\n            if self.model_name is None or self.model_name == \"auto\":\n                self.model_name = self.list_models_by_type(\"chat\")[0]\n            self.model = self.client.get_model(\n                self.models[self.model_name][\"id\"]\n            )\n\n            if self.ca_model_name is None or self.ca_model_name == \"auto\":\n                self.ca_model_name = self.list_models_by_type(\"chat\")[0]\n            self.ca_model = self.client.get_model(\n                self.models[self.ca_model_name][\"id\"]\n            )\n            return True\n\n        except RuntimeError as e:\n            # TODO handle error, log?\n            return False\n\n    def list_models_by_type(self, type: str):\n        names = []\n        if type == \"embed\" or type == \"embedding\":\n            for name, model in self.models.items():\n                if \"model_ability\" in model:\n                    if \"embed\" in model[\"model_ability\"]:\n                        names.append(name)\n                elif model[\"model_type\"] == \"embedding\":\n                    names.append(name)\n            return names\n        for name, model in self.models.items():\n            if \"model_ability\" in model:\n                if type in model[\"model_ability\"]:\n                    names.append(name)\n            elif model[\"model_type\"] == type:\n                names.append(name)\n        return names\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.XinferenceConversation.__init__","title":"<code>__init__(base_url, prompts, model_name='auto', correct=True, split_correction=False)</code>","text":"<p>Connect to an open-source LLM via the Xinference client library and set up a conversation with the user.  Also initialise a second conversational agent to provide corrections to the model output, if necessary.</p> <p>Args:</p> <pre><code>base_url (str): The base URL of the Xinference instance (should not\ninclude the /v1 part).\n\nprompts (dict): A dictionary of prompts to use for the conversation.\n\nmodel_name (str): The name of the model to use. Will be mapped to\nthe according uid from the list of available models. Can be set to\n\"auto\" to use the first available model.\n\ncorrect (bool): Whether to correct the model output.\n\nsplit_correction (bool): Whether to correct the model output by\nsplitting the output into sentences and correcting each sentence\nindividually.\n</code></pre> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def __init__(\n    self,\n    base_url: str,\n    prompts: dict,\n    model_name: str = \"auto\",\n    correct: bool = True,\n    split_correction: bool = False,\n):\n    \"\"\"\n\n    Connect to an open-source LLM via the Xinference client library and set\n    up a conversation with the user.  Also initialise a second\n    conversational agent to provide corrections to the model output, if\n    necessary.\n\n    Args:\n\n        base_url (str): The base URL of the Xinference instance (should not\n        include the /v1 part).\n\n        prompts (dict): A dictionary of prompts to use for the conversation.\n\n        model_name (str): The name of the model to use. Will be mapped to\n        the according uid from the list of available models. Can be set to\n        \"auto\" to use the first available model.\n\n        correct (bool): Whether to correct the model output.\n\n        split_correction (bool): Whether to correct the model output by\n        splitting the output into sentences and correcting each sentence\n        individually.\n\n    \"\"\"\n    # Shaohong: Please keep this xinference importing code here, so that,\n    # we don't need to depend on xinference if we dont need it (xinference\n    # is expensive to install)\n    from xinference.client import Client\n\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        correct=correct,\n        split_correction=split_correction,\n    )\n    self.client = Client(base_url=base_url)\n\n    self.models = {}\n    self.load_models()\n\n    self.ca_model_name = model_name\n\n    self.set_api_key()\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.XinferenceConversation.append_ca_message","title":"<code>append_ca_message(message)</code>","text":"<p>We also override the system message addition for the correcting agent, likewise because Xinference does not accept multiple system messages. We concatenate them if there are multiple.</p> <p>TODO this currently assumes that the correcting agent is the same model as the primary one.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to append.</p> required Source code in <code>biochatter/llm_connect.py</code> <pre><code>def append_ca_message(self, message: str):\n    \"\"\"\n\n    We also override the system message addition for the correcting agent,\n    likewise because Xinference does not accept multiple system messages. We\n    concatenate them if there are multiple.\n\n    TODO this currently assumes that the correcting agent is the same model\n    as the primary one.\n\n    Args:\n        message (str): The message to append.\n    \"\"\"\n    # if there is not already a system message in self.messages\n    if not any(isinstance(m, SystemMessage) for m in self.ca_messages):\n        self.ca_messages.append(\n            SystemMessage(\n                content=message,\n            ),\n        )\n    else:\n        # if there is a system message, append to the last one\n        for i, msg in enumerate(self.ca_messages):\n            if isinstance(msg, SystemMessage):\n                self.ca_messages[i].content += f\"\\n{message}\"\n                break\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.XinferenceConversation.append_system_message","title":"<code>append_system_message(message)</code>","text":"<p>We override the system message addition because Xinference does not accept multiple system messages. We concatenate them if there are multiple.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to append.</p> required Source code in <code>biochatter/llm_connect.py</code> <pre><code>def append_system_message(self, message: str):\n    \"\"\"\n    We override the system message addition because Xinference does not\n    accept multiple system messages. We concatenate them if there are\n    multiple.\n\n    Args:\n        message (str): The message to append.\n    \"\"\"\n    # if there is not already a system message in self.messages\n    if not any(isinstance(m, SystemMessage) for m in self.messages):\n        self.messages.append(\n            SystemMessage(\n                content=message,\n            ),\n        )\n    else:\n        # if there is a system message, append to the last one\n        for i, msg in enumerate(self.messages):\n            if isinstance(msg, SystemMessage):\n                self.messages[i].content += f\"\\n{message}\"\n                break\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.XinferenceConversation.set_api_key","title":"<code>set_api_key()</code>","text":"<p>Try to get the Xinference model from the client API. If the model is found, initialise the conversational agent. If the model is not found, <code>get_model</code> will raise a RuntimeError.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the model is found, False otherwise.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def set_api_key(self):\n    \"\"\"\n    Try to get the Xinference model from the client API. If the model is\n    found, initialise the conversational agent. If the model is not found,\n    `get_model` will raise a RuntimeError.\n\n    Returns:\n        bool: True if the model is found, False otherwise.\n    \"\"\"\n\n    try:\n        if self.model_name is None or self.model_name == \"auto\":\n            self.model_name = self.list_models_by_type(\"chat\")[0]\n        self.model = self.client.get_model(\n            self.models[self.model_name][\"id\"]\n        )\n\n        if self.ca_model_name is None or self.ca_model_name == \"auto\":\n            self.ca_model_name = self.list_models_by_type(\"chat\")[0]\n        self.ca_model = self.client.get_model(\n            self.models[self.ca_model_name][\"id\"]\n        )\n        return True\n\n    except RuntimeError as e:\n        # TODO handle error, log?\n        return False\n</code></pre>"},{"location":"open-llm/","title":"Open-source and Local LLMs","text":"<p>There are two self-hosted/local LLM solutions that BioChatter currently supports out-of-the-box.</p> <ul> <li>Xorbits Inference</li> <li>Ollama</li> </ul> <p>Below, we provide installation and usage instructions for both of them.</p>"},{"location":"open-llm/#xorbits-inference-xinference","title":"Xorbits Inference (Xinference)","text":"<p>Xorbits Inference is an open-source toolkit for running open-source models, particularly language models. To support BioChatter applications in local and protected contexts, we provide API access via BioChatter classes in a unified way. Briefly, this module allows to connect to any open-source model supported by Xinference via the state-of-the-art and easy-to-use OpenAI API. This allows local and remote access to essentially all relevant open-source models, including these builtin models, at very little setup cost.</p>"},{"location":"open-llm/#usage","title":"Usage","text":"<p>Usage is essentially the same as when calling the official OpenAI API, but uses the <code>XinferenceConversation</code> class under the hood. Interaction with the class is possible in the exact same way as with the standard class.</p>"},{"location":"open-llm/#connecting-to-the-model-from-biochatter","title":"Connecting to the model from BioChatter","text":"<p>All that remains once Xinference has started your model is to tell BioChatter the API endpoint of your deployed model via the <code>base_url</code> parameter of the <code>XinferenceConversation</code> class. For instance:</p> <pre><code>from biochatter.llm_connect import XinferenceConversation\n\nconversation = XinferenceConversation(\n    base_url=\"http://localhost:9997\",\n    prompts={},\n    correct=False,\n)\nresponse, token_usage, correction = conversation.query(\"Hello world!\")\n</code></pre>"},{"location":"open-llm/#deploying-locally-via-docker","title":"Deploying locally via Docker","text":"<p>We have created a Docker workflow that allows the deployment of builtin Xinference models, here. It will soon be available via Dockerhub. There is another workflow that allows mounting (potentially) any compatible model from HuggingFace, here. Note that, due to graphics driver limitations, this currently only works for Linux machines with dedicated Nvidia graphics cards. If you have a different setup, please check below for deploying Xinference without the Docker workflow.</p>"},{"location":"open-llm/#deploying-locally-without-docker","title":"Deploying locally without Docker","text":""},{"location":"open-llm/#installation","title":"Installation","text":"<p>To run Xinference locally on your computer or a workstation available on your network, follow the official instructions for your type of hardware. Briefly, this includes installing the <code>xinference</code> and <code>ctransformers</code> Python libraries into an environment of your choice, as well as a hardware-specific installation of the <code>llama-ccp-python</code> library.</p>"},{"location":"open-llm/#deploying-your-model","title":"Deploying your model","text":"<p>After installation, you can run the model (locally using <code>xinference</code> or in a distributed fashion. After startup, you can visit the local server address in your browser (standard is <code>http://localhost:9997</code>) and select and start your desired model. There is a large selection of predefined models to choose from, as well as the possibility to add your own favourite models to the framework. You will see your running models in the <code>Running Models</code> tab, once they have started.</p> <p>Alternatively, you can deploy (and query) your model via the Xinference Python client:</p> <pre><code>from xinference.client import Client\n\nclient = Client(\"http://localhost:9997\")\nmodel_uid = client.launch_model(model_name=\"chatglm2\")  # download model from HuggingFace and deploy\nmodel = client.get_model(model_uid)\n\nchat_history = []\nprompt = \"What is the largest animal?\"\nmodel.chat(\n    prompt,\n    chat_history,\n    generate_config={\"max_tokens\": 1024}\n)\n</code></pre>"},{"location":"open-llm/#ollama","title":"Ollama","text":"<p>Ollama is arguably the biggest open-source project for local LLM hosting right now. In comparison to Xinference it lacks the complete freedom of running any HuggingFace model in a simple fashion, but has the benefit of higher stability for the supported models. The list of supported models is updated diligently by the Ollama community. BioChatter support was added by implementing the LangChain ChatOllama and LangChain OllamaEmbeddings classes, connecting to Ollama APIs.</p>"},{"location":"open-llm/#usage_1","title":"Usage","text":"<p>Usage is essentially the same as when calling the official OpenAI API, but uses the <code>OllamaConversation</code> class under the hood. Interaction with the class is possible in the exact same way as with the standard class.</p>"},{"location":"open-llm/#connecting-to-the-model-from-biochatter_1","title":"Connecting to the model from BioChatter","text":"<p>Once Ollama has been set up (see below), you can directly use BioChatter to connect to the API endpoint and start any available model. It will be downloaded and launched on-demand. You can now configure the <code>OllamaConversation</code> instance setting the <code>base_url</code> and <code>model_name</code> parameters. For example:</p> <pre><code>from biochatter.llm_connect import OllamaConversation\n\nconversation = OllamaConversation(\n    base_url=\"http://localhost:11434\",\n    prompts={},\n    model_name='llama3',\n    correct=False,\n)\nresponse, token_usage, correction = conversation.query(\"Hello world!\")\n</code></pre>"},{"location":"open-llm/#deploying-locally-via-docker_1","title":"Deploying locally via Docker","text":"<p>To deploy Ollama with Docker is extremely easy and well documented. You can follow the official Ollama Docker blog post for that or check the Ollama DockerHub page that will also help you with the installation of the required <code>nvidia-container-toolkit</code> library if you want to use GPUs from Docker containers.</p>"},{"location":"open-llm/#deploying-locally-without-docker_1","title":"Deploying locally without Docker","text":""},{"location":"open-llm/#installation_1","title":"Installation","text":"<p>You can download and run Ollama also directly on your computer. For this you can just visit the official website that provides you with an installer for any OS. More info on the setup and startup process can be found in the GitHub README.</p>"},{"location":"podcast-reference/","title":"Podcast module","text":"<p>Here we handle generation of podcasts from texts.</p>"},{"location":"podcast-reference/#biochatter.podcast.Podcaster","title":"<code>Podcaster</code>","text":"Source code in <code>biochatter/podcast.py</code> <pre><code>class Podcaster:\n    def __init__(\n        self,\n        document: Document,\n        model_name: str = \"gpt-3.5-turbo\",\n    ) -&gt; None:\n        \"\"\"\n        Orchestrates the podcasting of a document.\n        \"\"\"\n        self.document = document\n        self.model_name = model_name\n\n    def generate_podcast(self, characters_per_paragraph: int) -&gt; None:\n        \"\"\"\n        Podcasts the document.\n\n        TODO:\n        - chain of density prompting for variable summary length\n        \"\"\"\n        full_text = self.document[0].page_content\n\n        # split text by sentence\n        sentences = self._split_text(full_text)\n\n        # could embed sentences and cluster on cosine similarity to identify\n        # paragraphs here\n\n        # preprocess text\n        for i, sentence in enumerate(sentences):\n            # special cases i.e. and e.g. - if sentence ends with one of these,\n            # append next sentence\n            special_cases = [\"i.e.\", \"e.g.\"]\n            if sentence.endswith(tuple(special_cases)):\n                sentences[i] = sentence + \" \" + sentences[i + 1]\n                del sentences[i + 1]\n\n        # concatenate first 5 sentences for title and author extraction\n        first_5 = \"\\n\".join(sentences[:5])\n        self.podcast_info = self._title_and_authors(first_5)\n\n        # LLM to determine section breaks?\n\n        # go through sections and summarise each\n        self.processed_sections = self._process_sections(\n            sentences,\n            characters_per_paragraph,\n        )\n\n        # summarise the summaries\n\n    def _split_text(self, text: str) -&gt; list[str]:\n        \"\"\"\n        Splits consecutive text into sentences.\n        \"\"\"\n        nltk.download(\"punkt\")\n        tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n        return tokenizer.tokenize(text)\n\n    def _title_and_authors(self, text: str) -&gt; str:\n        \"\"\"\n        Extracts title and authors from document.\n\n        Args:\n            text (str): text to extract title and authors from\n\n        Returns:\n            str: title and authors\n        \"\"\"\n        # first sentence - extract title, authors\n        c_first = GptConversation(\n            model_name=self.model_name,\n            prompts={},\n            correct=False,\n        )\n        c_first.set_api_key(api_key=os.getenv(\"OPENAI_API_KEY\"), user=\"podcast\")\n        c_first.append_system_message(FIRST_PROMPT)\n        msg, token_usage, correction = c_first.query(text)\n        # split at authors ('Authors:' or '\\nAuthors:')\n        if \"Authors:\" in msg:\n            title = msg.split(\"Title:\")[1].split(\"Authors:\")[0].strip()\n            authors = msg.split(\"Authors:\")[1].strip()\n            return f\"{title}, by {authors}, podcasted by biochatter.\"\n        else:\n            return \"A podcast by biochatter.\"\n\n    def _process_section(self, text: str, summarise: bool = False) -&gt; str:\n        \"\"\"\n        Processes a section of the document. Summarises if summarise is True,\n        otherwise just makes the text more listenable.\n\n        Args:\n            text (str): text to summarise\n\n            summarise (bool): whether to summarise the text\n\n        Returns:\n            str: summarised text\n        \"\"\"\n        # summarise section\n        c = GptConversation(\n            model_name=self.model_name,\n            prompts={},\n            correct=False,\n        )\n        c.set_api_key(api_key=os.getenv(\"OPENAI_API_KEY\"), user=\"podcast\")\n        if summarise:\n            c.append_system_message(SUMMARISE_PROMPT)\n        else:\n            c.append_system_message(PROCESS_PROMPT)\n        msg, token_usage, correction = c.query(text)\n        return msg\n\n    def _process_sections(\n        self, sentences: list, characters_per_paragraph: int\n    ) -&gt; list:\n        \"\"\"\n\n        Processes sections of the document. Concatenates sentences until\n        characters_per_paragraph is reached, removing each sentence from the\n        list as it is added to the section to be processed.\n\n        Args:\n            sentences (list): list of sentences to summarise\n\n            characters_per_paragraph (int): number of characters per paragraph\n\n        Returns:\n            list: list of processed sections\n        \"\"\"\n        summarised_sections = []\n        section = \"\"\n        while sentences:\n            sentence = sentences.pop(0)\n            tmp = section + sentence\n            if len(tmp) &lt; characters_per_paragraph and sentences:\n                section += sentence\n            else:\n                if sentences:\n                    sentences.insert(0, sentence)\n                summarised_section = self._process_section(section)\n                # filter \"no content\" sections\n                if not (\n                    \"no content\" in summarised_section.lower()\n                    and len(summarised_section) &lt; 30\n                ):\n                    summarised_sections.append(summarised_section)\n                section = \"\"\n\n        return summarised_sections\n\n    def podcast_to_file(\n        self,\n        path: str,\n        model: str = \"gtts\",\n        voice: str = \"alloy\",\n    ) -&gt; None:\n        \"\"\"\n        Uses text-to-speech to generate audio for the summarised paper podcast.\n\n        Args:\n            path (str): path to save audio file to\n\n            model (str): model to use for text-to-speech. Currently supported:\n                'gtts' (Google Text-to-Speech, free),\n                'tts-1' (OpenAI API, paid, prioritises speed),\n                'tts-1-hd' (OpenAI API, paid, prioritises quality)\n\n            voice (str): voice to use for text-to-speech. See OpenAI API\n                documentation for available voices.\n        \"\"\"\n\n        full_text = self.podcast_to_text()\n\n        if model == \"gtts\":\n            audio = gTTS(text=full_text)\n            audio.save(path)\n        else:\n            client = OpenAI()\n\n            # Save the intro to the original file\n            response = client.audio.speech.create(\n                model=model,\n                voice=voice,\n                input=(\n                    \"You are listening to: \\n\\n\"\n                    + self.podcast_info\n                    + \"\\n\\n\"\n                    + \" Text-to-speech generated by OpenAI.\"\n                ),\n            )\n            first_path = path.rsplit(\".\", 1)[0] + \"_0.mp3\"\n            response.stream_to_file(first_path)\n\n            # Concatenate the sections\n            full_text = \"\"\n            for i, section in enumerate(self.processed_sections):\n                full_text += section + \"\\n\\n\"\n\n            # Make sections of 4000 characters max (at sentence boundaries)\n            nltk.download(\"punkt\")\n            tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n            sentences = deque(\n                tokenizer.tokenize(full_text)\n            )  # Use a deque instead of a list\n\n            # Split the text into sections by filling one section until it\n            # exceeds 4000 characters, then starting a new section (not adding\n            # the sentence that would exceed the limit)\n            sections = []\n            section = \"\"\n            while sentences:\n                sentence = sentences[0]\n                tmp = section + sentence\n                if len(tmp) &lt; 4000:\n                    section += sentences.popleft()\n                else:\n                    sections.append(section)\n                    section = \"\"\n\n            sections.append(section)  # Add the penultimate section\n\n            # Last section: conclude the podcast\n            sections.append(\n                f\"This was {self.podcast_info}. Thank you for listening.\"\n            )\n\n            # Save each section to a separate file with an integer suffix\n            for i, section in enumerate(sections):\n                response = client.audio.speech.create(\n                    model=model,\n                    voice=voice,\n                    input=section,\n                )\n                # Insert the integer suffix just before the .mp3 extension\n                section_path = path.rsplit(\".\", 1)[0] + f\"_{i+1}.mp3\"\n                response.stream_to_file(section_path)\n\n    def podcast_to_text(self):\n        \"\"\"\n        Returns the summarised paper podcast as text.\n        \"\"\"\n        full_text = \"You are listening to: \" + self.podcast_info + \"\\n\\n\"\n        for section in self.processed_sections:\n            full_text += section + \"\\n\\n\"\n        return full_text\n</code></pre>"},{"location":"podcast-reference/#biochatter.podcast.Podcaster.__init__","title":"<code>__init__(document, model_name='gpt-3.5-turbo')</code>","text":"<p>Orchestrates the podcasting of a document.</p> Source code in <code>biochatter/podcast.py</code> <pre><code>def __init__(\n    self,\n    document: Document,\n    model_name: str = \"gpt-3.5-turbo\",\n) -&gt; None:\n    \"\"\"\n    Orchestrates the podcasting of a document.\n    \"\"\"\n    self.document = document\n    self.model_name = model_name\n</code></pre>"},{"location":"podcast-reference/#biochatter.podcast.Podcaster.generate_podcast","title":"<code>generate_podcast(characters_per_paragraph)</code>","text":"<p>Podcasts the document.</p> <p>TODO: - chain of density prompting for variable summary length</p> Source code in <code>biochatter/podcast.py</code> <pre><code>def generate_podcast(self, characters_per_paragraph: int) -&gt; None:\n    \"\"\"\n    Podcasts the document.\n\n    TODO:\n    - chain of density prompting for variable summary length\n    \"\"\"\n    full_text = self.document[0].page_content\n\n    # split text by sentence\n    sentences = self._split_text(full_text)\n\n    # could embed sentences and cluster on cosine similarity to identify\n    # paragraphs here\n\n    # preprocess text\n    for i, sentence in enumerate(sentences):\n        # special cases i.e. and e.g. - if sentence ends with one of these,\n        # append next sentence\n        special_cases = [\"i.e.\", \"e.g.\"]\n        if sentence.endswith(tuple(special_cases)):\n            sentences[i] = sentence + \" \" + sentences[i + 1]\n            del sentences[i + 1]\n\n    # concatenate first 5 sentences for title and author extraction\n    first_5 = \"\\n\".join(sentences[:5])\n    self.podcast_info = self._title_and_authors(first_5)\n\n    # LLM to determine section breaks?\n\n    # go through sections and summarise each\n    self.processed_sections = self._process_sections(\n        sentences,\n        characters_per_paragraph,\n    )\n</code></pre>"},{"location":"podcast-reference/#biochatter.podcast.Podcaster.podcast_to_file","title":"<code>podcast_to_file(path, model='gtts', voice='alloy')</code>","text":"<p>Uses text-to-speech to generate audio for the summarised paper podcast.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path to save audio file to</p> required <code>model</code> <code>str</code> <p>model to use for text-to-speech. Currently supported: 'gtts' (Google Text-to-Speech, free), 'tts-1' (OpenAI API, paid, prioritises speed), 'tts-1-hd' (OpenAI API, paid, prioritises quality)</p> <code>'gtts'</code> <code>voice</code> <code>str</code> <p>voice to use for text-to-speech. See OpenAI API documentation for available voices.</p> <code>'alloy'</code> Source code in <code>biochatter/podcast.py</code> <pre><code>def podcast_to_file(\n    self,\n    path: str,\n    model: str = \"gtts\",\n    voice: str = \"alloy\",\n) -&gt; None:\n    \"\"\"\n    Uses text-to-speech to generate audio for the summarised paper podcast.\n\n    Args:\n        path (str): path to save audio file to\n\n        model (str): model to use for text-to-speech. Currently supported:\n            'gtts' (Google Text-to-Speech, free),\n            'tts-1' (OpenAI API, paid, prioritises speed),\n            'tts-1-hd' (OpenAI API, paid, prioritises quality)\n\n        voice (str): voice to use for text-to-speech. See OpenAI API\n            documentation for available voices.\n    \"\"\"\n\n    full_text = self.podcast_to_text()\n\n    if model == \"gtts\":\n        audio = gTTS(text=full_text)\n        audio.save(path)\n    else:\n        client = OpenAI()\n\n        # Save the intro to the original file\n        response = client.audio.speech.create(\n            model=model,\n            voice=voice,\n            input=(\n                \"You are listening to: \\n\\n\"\n                + self.podcast_info\n                + \"\\n\\n\"\n                + \" Text-to-speech generated by OpenAI.\"\n            ),\n        )\n        first_path = path.rsplit(\".\", 1)[0] + \"_0.mp3\"\n        response.stream_to_file(first_path)\n\n        # Concatenate the sections\n        full_text = \"\"\n        for i, section in enumerate(self.processed_sections):\n            full_text += section + \"\\n\\n\"\n\n        # Make sections of 4000 characters max (at sentence boundaries)\n        nltk.download(\"punkt\")\n        tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n        sentences = deque(\n            tokenizer.tokenize(full_text)\n        )  # Use a deque instead of a list\n\n        # Split the text into sections by filling one section until it\n        # exceeds 4000 characters, then starting a new section (not adding\n        # the sentence that would exceed the limit)\n        sections = []\n        section = \"\"\n        while sentences:\n            sentence = sentences[0]\n            tmp = section + sentence\n            if len(tmp) &lt; 4000:\n                section += sentences.popleft()\n            else:\n                sections.append(section)\n                section = \"\"\n\n        sections.append(section)  # Add the penultimate section\n\n        # Last section: conclude the podcast\n        sections.append(\n            f\"This was {self.podcast_info}. Thank you for listening.\"\n        )\n\n        # Save each section to a separate file with an integer suffix\n        for i, section in enumerate(sections):\n            response = client.audio.speech.create(\n                model=model,\n                voice=voice,\n                input=section,\n            )\n            # Insert the integer suffix just before the .mp3 extension\n            section_path = path.rsplit(\".\", 1)[0] + f\"_{i+1}.mp3\"\n            response.stream_to_file(section_path)\n</code></pre>"},{"location":"podcast-reference/#biochatter.podcast.Podcaster.podcast_to_text","title":"<code>podcast_to_text()</code>","text":"<p>Returns the summarised paper podcast as text.</p> Source code in <code>biochatter/podcast.py</code> <pre><code>def podcast_to_text(self):\n    \"\"\"\n    Returns the summarised paper podcast as text.\n    \"\"\"\n    full_text = \"You are listening to: \" + self.podcast_info + \"\\n\\n\"\n    for section in self.processed_sections:\n        full_text += section + \"\\n\\n\"\n    return full_text\n</code></pre>"},{"location":"podcast/","title":"Podcast my Paper","text":"<p>We provide a module to perform document processing and text-to-speech to enable listening to any document in podcast style.  The functionality can be accessed through the podcast API or by running the script <code>scripts/podcast_single_document.py</code>.</p>"},{"location":"podcast/#api-access","title":"API access","text":"<p>The podcast API is available through the <code>podcast</code> module. An end-to-end workflow looks like this (modified from the test module):</p> <pre><code>from biochatter.podcast import Podcaster\nfrom biochatter.vectorstore import DocumentReader\n\n# Load document\nreader = DocumentReader()\ndocument = reader.load_document(\"test/dcn.pdf\")\n\n# Initialise podcaster\npodcaster = Podcaster(document)\n\n# Generate podcast (LLM task)\npodcaster.generate_podcast(characters_per_paragraph=5000)\n\n# Employ text-to-speech to generate audio file (optional)\npodcaster.podcast_to_file(\"test/test.mp3\", model=\"tts-1-hd\", voice=\"alloy\")\n</code></pre> <p>If you do not want audio output, you can simply access the generated text though the function <code>podcaster.podcast_to_text()</code>.</p> <p>This example uses the paid OpenAI text-to-speech API to generate the audio file. The default of the <code>podcast_to_file</code> function is to use the free Google text-to-speech API.  When using OpenAI, due to the input length limit of 4096 characters, the podcast is split into multiple parts indicated by integer suffixes.</p>"},{"location":"podcast/#command-line-access","title":"Command line access","text":"<p>To generate a podcast from a single document more quickly, you can use the <code>scripts/podcast_single_document.py</code> script.  It accepts two arguments, the path to the document and the path to the desired output file.  If the output file ends in <code>.mp3</code>, the OpenAI text-to-speech API will be used to generate an audio file.  Otherwise, the script will generate a text file and skip the text-to-speech step.  If using the OpenAI text-to-speech API, multiple files will be generated with integer suffixes.  If you installed BioChatter with poetry, you can run the script like this (from the root directory of the repository):</p> <pre><code>poetry run python scripts/podcast_single_document.py test/dcn.pdf test/test.mp3\n</code></pre>"},{"location":"prompts-reference/","title":"Prompts module","text":"<p>Here we handle generation of use case-specific prompts.</p>"},{"location":"prompts-reference/#biochatter.prompts.BioCypherPromptEngine","title":"<code>BioCypherPromptEngine</code>","text":"Source code in <code>biochatter/prompts.py</code> <pre><code>class BioCypherPromptEngine:\n    def __init__(\n        self,\n        schema_config_or_info_path: Optional[str] = None,\n        schema_config_or_info_dict: Optional[dict] = None,\n        model_name: str = \"gpt-3.5-turbo\",\n        conversation_factory: Optional[Callable] = None,\n    ) -&gt; None:\n        \"\"\"\n\n        Given a biocypher schema configuration, extract the entities and\n        relationships, and for each extract their mode of representation (node\n        or edge), properties, and identifier namespace. Using these data, allow\n        the generation of prompts for a large language model, informing it of\n        the schema constituents and their properties, to enable the\n        parameterisation of function calls to a knowledge graph.\n\n        Args:\n            schema_config_or_info_path: Path to a biocypher schema configuration\n                file or the extended schema information output generated by\n                BioCypher's `write_schema_info` function (preferred).\n\n            schema_config_or_info_dict: A dictionary containing the schema\n                configuration file or the extended schema information output\n                generated by BioCypher's `write_schema_info` function\n                (preferred).\n\n            model_name: The name of the model to use for the conversation.\n                DEPRECATED: This should now be set in the conversation factory.\n\n            conversation_factory: A function used to create a conversation for\n                creating the KG query. If not provided, a default function is\n                used (creating an OpenAI conversation with the specified model,\n                see `_get_conversation`).\n        \"\"\"\n\n        if not schema_config_or_info_path and not schema_config_or_info_dict:\n            raise ValueError(\n                \"Please provide the schema configuration or schema info as a \"\n                \"path to a file or as a dictionary.\"\n            )\n\n        if schema_config_or_info_path and schema_config_or_info_dict:\n            raise ValueError(\n                \"Please provide the schema configuration or schema info as a \"\n                \"path to a file or as a dictionary, not both.\"\n            )\n\n        # set conversation factory or use default\n        self.conversation_factory = (\n            conversation_factory\n            if conversation_factory is not None\n            else self._get_conversation\n        )\n\n        if schema_config_or_info_path:\n            # read the schema configuration\n            with open(schema_config_or_info_path, \"r\") as f:\n                schema_config = yaml.safe_load(f)\n        elif schema_config_or_info_dict:\n            schema_config = schema_config_or_info_dict\n\n        # check whether it is the original schema config or the output of\n        # biocypher info\n        is_schema_info = schema_config.get(\"is_schema_info\", False)\n\n        # extract the entities and relationships: each top level key that has\n        # a 'represented_as' key\n        self.entities = {}\n        self.relationships = {}\n        if not is_schema_info:\n            for key, value in schema_config.items():\n                # hacky, better with biocypher output\n                name_indicates_relationship = (\n                    \"interaction\" in key.lower() or \"association\" in key.lower()\n                )\n                if \"represented_as\" in value:\n                    if (\n                        value[\"represented_as\"] == \"node\"\n                        and not name_indicates_relationship\n                    ):\n                        self.entities[sentencecase_to_pascalcase(key)] = value\n                    elif (\n                        value[\"represented_as\"] == \"node\"\n                        and name_indicates_relationship\n                    ):\n                        self.relationships[sentencecase_to_pascalcase(key)] = (\n                            value\n                        )\n                    elif value[\"represented_as\"] == \"edge\":\n                        self.relationships[sentencecase_to_pascalcase(key)] = (\n                            value\n                        )\n        else:\n            for key, value in schema_config.items():\n                if not isinstance(value, dict):\n                    continue\n                if value.get(\"present_in_knowledge_graph\", None) == False:\n                    continue\n                if value.get(\"is_relationship\", None) == False:\n                    self.entities[sentencecase_to_pascalcase(key)] = value\n                elif value.get(\"is_relationship\", None) == True:\n                    value = self._capitalise_source_and_target(value)\n                    self.relationships[sentencecase_to_pascalcase(key)] = value\n\n        self.question = \"\"\n        self.selected_entities = []\n        self.selected_relationships = []  # used in property selection\n        self.selected_relationship_labels = {}  # copy to deal with labels that\n        # are not the same as the relationship name, used in query generation\n        # dictionary to also include source and target types\n        self.rel_directions = {}\n        self.model_name = model_name\n\n    def _capitalise_source_and_target(self, relationship: dict) -&gt; dict:\n        \"\"\"\n        Make sources and targets PascalCase to match the entities. Sources and\n        targets can be strings or lists of strings.\n        \"\"\"\n        if \"source\" in relationship:\n            if isinstance(relationship[\"source\"], str):\n                relationship[\"source\"] = sentencecase_to_pascalcase(\n                    relationship[\"source\"]\n                )\n            elif isinstance(relationship[\"source\"], list):\n                relationship[\"source\"] = [\n                    sentencecase_to_pascalcase(s)\n                    for s in relationship[\"source\"]\n                ]\n        if \"target\" in relationship:\n            if isinstance(relationship[\"target\"], str):\n                relationship[\"target\"] = sentencecase_to_pascalcase(\n                    relationship[\"target\"]\n                )\n            elif isinstance(relationship[\"target\"], list):\n                relationship[\"target\"] = [\n                    sentencecase_to_pascalcase(t)\n                    for t in relationship[\"target\"]\n                ]\n        return relationship\n\n    def _select_graph_entities_from_question(\n        self, question: str, conversation: Conversation\n    ) -&gt; str:\n        conversation.reset()\n        success1 = self._select_entities(\n            question=question, conversation=conversation\n        )\n        if not success1:\n            raise ValueError(\n                \"Entity selection failed. Please try again with a different \"\n                \"question.\"\n            )\n        conversation.reset()\n        success2 = self._select_relationships(conversation=conversation)\n        if not success2:\n            raise ValueError(\n                \"Relationship selection failed. Please try again with a \"\n                \"different question.\"\n            )\n        conversation.reset()\n        success3 = self._select_properties(conversation=conversation)\n        if not success3:\n            raise ValueError(\n                \"Property selection failed. Please try again with a different \"\n                \"question.\"\n            )\n\n    def _generate_query_prompt(\n        self,\n        entities: list,\n        relationships: dict,\n        properties: dict,\n        query_language: Optional[str] = \"Cypher\",\n    ) -&gt; str:\n        \"\"\"\n        Generate a prompt for a large language model to generate a database\n        query based on the selected entities, relationships, and properties.\n\n        Args:\n            entities: A list of entities that are relevant to the question.\n\n            relationships: A list of relationships that are relevant to the\n                question.\n\n            properties: A dictionary of properties that are relevant to the\n                question.\n\n            query_language: The language of the query to generate.\n\n        Returns:\n            A prompt for a large language model to generate a database query.\n        \"\"\"\n        msg = (\n            f\"Generate a database query in {query_language} that answers \"\n            f\"the user's question. \"\n            f\"You can use the following entities: {entities}, \"\n            f\"relationships: {list(relationships.keys())}, and \"\n            f\"properties: {properties}. \"\n        )\n\n        for relationship, values in relationships.items():\n            self._expand_pairs(relationship, values)\n\n        if self.rel_directions:\n            msg += \"Given the following valid combinations of source, relationship, and target: \"\n            for key, value in self.rel_directions.items():\n                for pair in value:\n                    msg += f\"'(:{pair[0]})-(:{key})-&gt;(:{pair[1]})', \"\n            msg += f\"generate a {query_language} query using one of these combinations. \"\n\n        msg += \"Only return the query, without any additional text.\"\n        return msg\n\n    def generate_query_prompt(\n        self, question: str, query_language: Optional[str] = \"Cypher\"\n    ) -&gt; str:\n        \"\"\"\n        Generate a prompt for a large language model to generate a database\n        query based on the user's question and class attributes informing about\n        the schema.\n\n        Args:\n            question: A user's question.\n\n            query_language: The language of the query to generate.\n\n        Returns:\n            A prompt for a large language model to generate a database query.\n        \"\"\"\n        self._select_graph_entities_from_question(\n            question, self.conversation_factory()\n        )\n        msg = self._generate_query_prompt(\n            self.selected_entities,\n            self.selected_relationship_labels,\n            self.selected_properties,\n            query_language,\n        )\n        return msg\n\n    def generate_query(\n        self, question: str, query_language: Optional[str] = \"Cypher\"\n    ) -&gt; str:\n        \"\"\"\n        Wrap entity and property selection and query generation; return the\n        generated query.\n\n        Args:\n            question: A user's question.\n\n            query_language: The language of the query to generate.\n\n        Returns:\n            A database query that could answer the user's question.\n        \"\"\"\n\n        self._select_graph_entities_from_question(\n            question, self.conversation_factory()\n        )\n\n        return self._generate_query(\n            question=question,\n            entities=self.selected_entities,\n            relationships=self.selected_relationship_labels,\n            properties=self.selected_properties,\n            query_language=query_language,\n            conversation=self.conversation_factory(),\n        )\n\n    def _get_conversation(\n        self, model_name: Optional[str] = None\n    ) -&gt; \"Conversation\":\n        \"\"\"\n        Create a conversation object given a model name.\n\n        Args:\n            model_name: The name of the model to use for the conversation.\n\n        Returns:\n            A BioChatter Conversation object for connecting to the LLM.\n\n        Todo:\n            Genericise to models outside of OpenAI.\n        \"\"\"\n\n        conversation = GptConversation(\n            model_name=model_name or self.model_name,\n            prompts={},\n            correct=False,\n        )\n        conversation.set_api_key(\n            api_key=os.getenv(\"OPENAI_API_KEY\"), user=\"test_user\"\n        )\n        return conversation\n\n    def _select_entities(\n        self, question: str, conversation: \"Conversation\"\n    ) -&gt; bool:\n        \"\"\"\n\n        Given a question, select the entities that are relevant to the question\n        and store them in `selected_entities` and `selected_relationships`. Use\n        LLM conversation to do this.\n\n        Args:\n            question: A user's question.\n\n            conversation: A BioChatter Conversation object for connecting to the\n                LLM.\n\n        Returns:\n            True if at least one entity was selected, False otherwise.\n\n        \"\"\"\n\n        self.question = question\n\n        conversation.append_system_message(\n            (\n                \"You have access to a knowledge graph that contains \"\n                f\"these entity types: {', '.join(self.entities)}. Your task is \"\n                \"to select the entity types that are relevant to the user's question \"\n                \"for subsequent use in a query. Only return the entity types, \"\n                \"comma-separated, without any additional text. Do not return \"\n                \"entity names, relationships, or properties.\"\n            )\n        )\n\n        msg, token_usage, correction = conversation.query(question)\n\n        result = msg.split(\",\") if msg else []\n        # TODO: do we go back and retry if no entities were selected? or ask for\n        # a reason? offer visual selection of entities and relationships by the\n        # user?\n\n        if result:\n            for entity in result:\n                entity = entity.strip()\n                if entity in self.entities:\n                    self.selected_entities.append(entity)\n\n        return bool(result)\n\n    def _select_relationships(self, conversation: \"Conversation\") -&gt; bool:\n        \"\"\"\n        Given a question and the preselected entities, select relationships for\n        the query.\n\n        Args:\n            conversation: A BioChatter Conversation object for connecting to the\n                LLM.\n\n        Returns:\n            True if at least one relationship was selected, False otherwise.\n\n        Todo:\n            Now we have the problem that we discard all relationships that do\n            not have a source and target, if at least one relationship has a\n            source and target. At least communicate this all-or-nothing\n            behaviour to the user.\n        \"\"\"\n\n        if not self.question:\n            raise ValueError(\n                \"No question found. Please make sure to run entity selection \"\n                \"first.\"\n            )\n\n        if not self.selected_entities:\n            raise ValueError(\n                \"No entities found. Please run the entity selection step first.\"\n            )\n\n        rels = {}\n        source_and_target_present = False\n        for key, value in self.relationships.items():\n            if \"source\" in value and \"target\" in value:\n                # if source or target is a list, expand to single pairs\n                source = ensure_iterable(value[\"source\"])\n                target = ensure_iterable(value[\"target\"])\n                pairs = []\n                for s in source:\n                    for t in target:\n                        pairs.append((s, t))\n                rels[key] = pairs\n                source_and_target_present = True\n            else:\n                rels[key] = {}\n\n        # prioritise relationships that have source and target, and discard\n        # relationships that do not have both source and target, if at least one\n        # relationship has both source and target. keep relationships that have\n        # either source or target, if none of the relationships have both source\n        # and target.\n\n        if source_and_target_present:\n            # First, separate the relationships into two groups: those with both\n            # source and target in the selected entities, and those with either\n            # source or target but not both.\n\n            rels_with_both = {}\n            rels_with_either = {}\n            for key, value in rels.items():\n                for pair in value:\n                    if pair[0] in self.selected_entities:\n                        if pair[1] in self.selected_entities:\n                            rels_with_both[key] = value\n                        else:\n                            rels_with_either[key] = value\n                    elif pair[1] in self.selected_entities:\n                        rels_with_either[key] = value\n\n            # If there are any relationships with both source and target,\n            # discard the others.\n\n            if rels_with_both:\n                rels = rels_with_both\n            else:\n                rels = rels_with_either\n\n            selected_rels = []\n            for key, value in rels.items():\n                if not value:\n                    continue\n\n                for pair in value:\n                    if (\n                        pair[0] in self.selected_entities\n                        or pair[1] in self.selected_entities\n                    ):\n                        selected_rels.append((key, pair))\n\n            rels = json.dumps(selected_rels)\n        else:\n            rels = json.dumps(self.relationships)\n\n        msg = (\n            \"You have access to a knowledge graph that contains \"\n            f\"these entities: {', '.join(self.selected_entities)}. \"\n            \"Your task is to select the relationships that are relevant \"\n            \"to the user's question for subsequent use in a query. Only \"\n            \"return the relationships without their sources or targets, \"\n            \"comma-separated, and without any additional text. Here are the \"\n            \"possible relationships and their source and target entities: \"\n            f\"{rels}.\"\n        )\n\n        conversation.append_system_message(msg)\n\n        res, token_usage, correction = conversation.query(self.question)\n\n        result = res.split(\",\") if msg else []\n\n        if result:\n            for relationship in result:\n                relationship = relationship.strip()\n                if relationship in self.relationships:\n                    self.selected_relationships.append(relationship)\n                    rel_dict = self.relationships[relationship]\n                    label = rel_dict.get(\"label_as_edge\", relationship)\n                    if \"source\" in rel_dict and \"target\" in rel_dict:\n                        self.selected_relationship_labels[label] = {\n                            \"source\": rel_dict[\"source\"],\n                            \"target\": rel_dict[\"target\"],\n                        }\n                    else:\n                        self.selected_relationship_labels[label] = {\n                            \"source\": None,\n                            \"target\": None,\n                        }\n\n        # if we selected relationships that have either source or target which\n        # is not in the selected entities, we add those entities to the selected\n        # entities.\n\n        if self.selected_relationship_labels:\n            for key, value in self.selected_relationship_labels.items():\n                sources = ensure_iterable(value[\"source\"])\n                targets = ensure_iterable(value[\"target\"])\n                for source in sources:\n                    if source is None:\n                        continue\n                    if source not in self.selected_entities:\n                        self.selected_entities.append(source)\n                for target in targets:\n                    if target is None:\n                        continue\n                    if target not in self.selected_entities:\n                        self.selected_entities.append(target)\n\n        return bool(result)\n\n    def _select_properties(self, conversation: \"Conversation\") -&gt; bool:\n        \"\"\"\n\n        Given a question (optionally provided, but in the standard use case\n        reused from the entity selection step) and the selected entities, select\n        the properties that are relevant to the question and store them in\n        the dictionary `selected_properties`.\n\n        Returns:\n            True if at least one property was selected, False otherwise.\n\n        \"\"\"\n\n        if not self.question:\n            raise ValueError(\n                \"No question found. Please make sure to run entity and \"\n                \"relationship selection first.\"\n            )\n\n        if not self.selected_entities and not self.selected_relationships:\n            raise ValueError(\n                \"No entities or relationships provided, and none available \"\n                \"from entity selection step. Please provide \"\n                \"entities/relationships or run the entity selection \"\n                \"(`select_entities()`) step first.\"\n            )\n\n        e_props = {}\n        for entity in self.selected_entities:\n            if self.entities[entity].get(\"properties\"):\n                e_props[entity] = list(\n                    self.entities[entity][\"properties\"].keys()\n                )\n\n        r_props = {}\n        for relationship in self.selected_relationships:\n            if self.relationships[relationship].get(\"properties\"):\n                r_props[relationship] = list(\n                    self.relationships[relationship][\"properties\"].keys()\n                )\n\n        msg = (\n            \"You have access to a knowledge graph that contains entities and \"\n            \"relationships. They have the following properties. Entities:\"\n            f\"{e_props}, Relationships: {r_props}. \"\n            \"Your task is to select the properties that are relevant to the \"\n            \"user's question for subsequent use in a query. Only return the \"\n            \"entities and relationships with their relevant properties in JSON \"\n            \"format, without any additional text. Return the \"\n            \"entities/relationships as top-level dictionary keys, and their \"\n            \"properties as dictionary values. \"\n            \"Do not return properties that are not relevant to the question.\"\n        )\n\n        conversation.append_system_message(msg)\n\n        msg, token_usage, correction = conversation.query(self.question)\n\n        try:\n            self.selected_properties = json.loads(msg) if msg else {}\n        except json.decoder.JSONDecodeError:\n            self.selected_properties = {}\n\n        return bool(self.selected_properties)\n\n    def _generate_query(\n        self,\n        question: str,\n        entities: list,\n        relationships: dict,\n        properties: dict,\n        query_language: str,\n        conversation: \"Conversation\",\n    ) -&gt; str:\n        \"\"\"\n        Generate a query in the specified query language that answers the user's\n        question.\n\n        Args:\n            question: A user's question.\n\n            entities: A list of entities that are relevant to the question.\n\n            relationships: A list of relationships that are relevant to the\n                question.\n\n            properties: A dictionary of properties that are relevant to the\n                question.\n\n            query_language: The language of the query to generate.\n\n            conversation: A BioChatter Conversation object for connecting to the\n                LLM.\n\n        Returns:\n            A database query that could answer the user's question.\n        \"\"\"\n        msg = self._generate_query_prompt(\n            entities,\n            relationships,\n            properties,\n            query_language,\n        )\n\n        conversation.append_system_message(msg)\n\n        out_msg, token_usage, correction = conversation.query(question)\n\n        return out_msg.strip()\n\n    def _expand_pairs(self, relationship, values) -&gt; None:\n        if not self.rel_directions.get(relationship):\n            self.rel_directions[relationship] = []\n        if isinstance(values[\"source\"], list):\n            for source in values[\"source\"]:\n                if isinstance(values[\"target\"], list):\n                    for target in values[\"target\"]:\n                        self.rel_directions[relationship].append(\n                            (source, target)\n                        )\n                else:\n                    self.rel_directions[relationship].append(\n                        (source, values[\"target\"])\n                    )\n        elif isinstance(values[\"target\"], list):\n            for target in values[\"target\"]:\n                self.rel_directions[relationship].append(\n                    (values[\"source\"], target)\n                )\n        else:\n            self.rel_directions[relationship].append(\n                (values[\"source\"], values[\"target\"])\n            )\n</code></pre>"},{"location":"prompts-reference/#biochatter.prompts.BioCypherPromptEngine.__init__","title":"<code>__init__(schema_config_or_info_path=None, schema_config_or_info_dict=None, model_name='gpt-3.5-turbo', conversation_factory=None)</code>","text":"<p>Given a biocypher schema configuration, extract the entities and relationships, and for each extract their mode of representation (node or edge), properties, and identifier namespace. Using these data, allow the generation of prompts for a large language model, informing it of the schema constituents and their properties, to enable the parameterisation of function calls to a knowledge graph.</p> <p>Parameters:</p> Name Type Description Default <code>schema_config_or_info_path</code> <code>Optional[str]</code> <p>Path to a biocypher schema configuration file or the extended schema information output generated by BioCypher's <code>write_schema_info</code> function (preferred).</p> <code>None</code> <code>schema_config_or_info_dict</code> <code>Optional[dict]</code> <p>A dictionary containing the schema configuration file or the extended schema information output generated by BioCypher's <code>write_schema_info</code> function (preferred).</p> <code>None</code> <code>model_name</code> <code>str</code> <p>The name of the model to use for the conversation. DEPRECATED: This should now be set in the conversation factory.</p> <code>'gpt-3.5-turbo'</code> <code>conversation_factory</code> <code>Optional[Callable]</code> <p>A function used to create a conversation for creating the KG query. If not provided, a default function is used (creating an OpenAI conversation with the specified model, see <code>_get_conversation</code>).</p> <code>None</code> Source code in <code>biochatter/prompts.py</code> <pre><code>def __init__(\n    self,\n    schema_config_or_info_path: Optional[str] = None,\n    schema_config_or_info_dict: Optional[dict] = None,\n    model_name: str = \"gpt-3.5-turbo\",\n    conversation_factory: Optional[Callable] = None,\n) -&gt; None:\n    \"\"\"\n\n    Given a biocypher schema configuration, extract the entities and\n    relationships, and for each extract their mode of representation (node\n    or edge), properties, and identifier namespace. Using these data, allow\n    the generation of prompts for a large language model, informing it of\n    the schema constituents and their properties, to enable the\n    parameterisation of function calls to a knowledge graph.\n\n    Args:\n        schema_config_or_info_path: Path to a biocypher schema configuration\n            file or the extended schema information output generated by\n            BioCypher's `write_schema_info` function (preferred).\n\n        schema_config_or_info_dict: A dictionary containing the schema\n            configuration file or the extended schema information output\n            generated by BioCypher's `write_schema_info` function\n            (preferred).\n\n        model_name: The name of the model to use for the conversation.\n            DEPRECATED: This should now be set in the conversation factory.\n\n        conversation_factory: A function used to create a conversation for\n            creating the KG query. If not provided, a default function is\n            used (creating an OpenAI conversation with the specified model,\n            see `_get_conversation`).\n    \"\"\"\n\n    if not schema_config_or_info_path and not schema_config_or_info_dict:\n        raise ValueError(\n            \"Please provide the schema configuration or schema info as a \"\n            \"path to a file or as a dictionary.\"\n        )\n\n    if schema_config_or_info_path and schema_config_or_info_dict:\n        raise ValueError(\n            \"Please provide the schema configuration or schema info as a \"\n            \"path to a file or as a dictionary, not both.\"\n        )\n\n    # set conversation factory or use default\n    self.conversation_factory = (\n        conversation_factory\n        if conversation_factory is not None\n        else self._get_conversation\n    )\n\n    if schema_config_or_info_path:\n        # read the schema configuration\n        with open(schema_config_or_info_path, \"r\") as f:\n            schema_config = yaml.safe_load(f)\n    elif schema_config_or_info_dict:\n        schema_config = schema_config_or_info_dict\n\n    # check whether it is the original schema config or the output of\n    # biocypher info\n    is_schema_info = schema_config.get(\"is_schema_info\", False)\n\n    # extract the entities and relationships: each top level key that has\n    # a 'represented_as' key\n    self.entities = {}\n    self.relationships = {}\n    if not is_schema_info:\n        for key, value in schema_config.items():\n            # hacky, better with biocypher output\n            name_indicates_relationship = (\n                \"interaction\" in key.lower() or \"association\" in key.lower()\n            )\n            if \"represented_as\" in value:\n                if (\n                    value[\"represented_as\"] == \"node\"\n                    and not name_indicates_relationship\n                ):\n                    self.entities[sentencecase_to_pascalcase(key)] = value\n                elif (\n                    value[\"represented_as\"] == \"node\"\n                    and name_indicates_relationship\n                ):\n                    self.relationships[sentencecase_to_pascalcase(key)] = (\n                        value\n                    )\n                elif value[\"represented_as\"] == \"edge\":\n                    self.relationships[sentencecase_to_pascalcase(key)] = (\n                        value\n                    )\n    else:\n        for key, value in schema_config.items():\n            if not isinstance(value, dict):\n                continue\n            if value.get(\"present_in_knowledge_graph\", None) == False:\n                continue\n            if value.get(\"is_relationship\", None) == False:\n                self.entities[sentencecase_to_pascalcase(key)] = value\n            elif value.get(\"is_relationship\", None) == True:\n                value = self._capitalise_source_and_target(value)\n                self.relationships[sentencecase_to_pascalcase(key)] = value\n\n    self.question = \"\"\n    self.selected_entities = []\n    self.selected_relationships = []  # used in property selection\n    self.selected_relationship_labels = {}  # copy to deal with labels that\n    # are not the same as the relationship name, used in query generation\n    # dictionary to also include source and target types\n    self.rel_directions = {}\n    self.model_name = model_name\n</code></pre>"},{"location":"prompts-reference/#biochatter.prompts.BioCypherPromptEngine.generate_query","title":"<code>generate_query(question, query_language='Cypher')</code>","text":"<p>Wrap entity and property selection and query generation; return the generated query.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>A user's question.</p> required <code>query_language</code> <code>Optional[str]</code> <p>The language of the query to generate.</p> <code>'Cypher'</code> <p>Returns:</p> Type Description <code>str</code> <p>A database query that could answer the user's question.</p> Source code in <code>biochatter/prompts.py</code> <pre><code>def generate_query(\n    self, question: str, query_language: Optional[str] = \"Cypher\"\n) -&gt; str:\n    \"\"\"\n    Wrap entity and property selection and query generation; return the\n    generated query.\n\n    Args:\n        question: A user's question.\n\n        query_language: The language of the query to generate.\n\n    Returns:\n        A database query that could answer the user's question.\n    \"\"\"\n\n    self._select_graph_entities_from_question(\n        question, self.conversation_factory()\n    )\n\n    return self._generate_query(\n        question=question,\n        entities=self.selected_entities,\n        relationships=self.selected_relationship_labels,\n        properties=self.selected_properties,\n        query_language=query_language,\n        conversation=self.conversation_factory(),\n    )\n</code></pre>"},{"location":"prompts-reference/#biochatter.prompts.BioCypherPromptEngine.generate_query_prompt","title":"<code>generate_query_prompt(question, query_language='Cypher')</code>","text":"<p>Generate a prompt for a large language model to generate a database query based on the user's question and class attributes informing about the schema.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>A user's question.</p> required <code>query_language</code> <code>Optional[str]</code> <p>The language of the query to generate.</p> <code>'Cypher'</code> <p>Returns:</p> Type Description <code>str</code> <p>A prompt for a large language model to generate a database query.</p> Source code in <code>biochatter/prompts.py</code> <pre><code>def generate_query_prompt(\n    self, question: str, query_language: Optional[str] = \"Cypher\"\n) -&gt; str:\n    \"\"\"\n    Generate a prompt for a large language model to generate a database\n    query based on the user's question and class attributes informing about\n    the schema.\n\n    Args:\n        question: A user's question.\n\n        query_language: The language of the query to generate.\n\n    Returns:\n        A prompt for a large language model to generate a database query.\n    \"\"\"\n    self._select_graph_entities_from_question(\n        question, self.conversation_factory()\n    )\n    msg = self._generate_query_prompt(\n        self.selected_entities,\n        self.selected_relationship_labels,\n        self.selected_properties,\n        query_language,\n    )\n    return msg\n</code></pre>"},{"location":"rag/","title":"Retrieval-Augmented Generation","text":"<p>To connect to a vector database for using semantic similarity search and retrieval-augmented generation (RAG), we provide an implementation that connects to a Milvus instance (local or remote).  These functions are provided by the modules <code>vectorstore.py</code> (for performing embeddings) and <code>vectorstore_agent.py</code> (for maintaining the connection and search).</p> <p>This is implemented in the BioChatter Light Docker workflow and the BioChatter Docker compose found in this repository.  To start Milvus on its own in these repositories, you can call <code>docker compose up -d standalone</code> (<code>standalone</code> being the Milvus endpoint, which starts two other services alongside it).</p>"},{"location":"rag/#connecting","title":"Connecting","text":"<p>To connect to a vector DB host, we can use the corresponding class:</p> <pre><code>from biochatter.vectorstore_agent import VectorDatabaseAgentMilvus\n\ndbHost = VectorDatabaseAgentMilvus(\n    embedding_func=OpenAIEmbeddings(),\n    connection_args={\"host\": _HOST, \"port\": _PORT},\n    embedding_collection_name=EMBEDDING_NAME,\n    metadata_collection_name=METADATA_NAME\n)\n</code></pre> <p>This establishes a connection with the vector database (using a host IP and port) and uses two collections, one for the embeddings and one for the metadata of embedded text (e.g. the title and authors of the paper that was embedded).</p>"},{"location":"rag/#embedding-documents","title":"Embedding documents","text":"<p>To embed text from documents, we use the LangChain and BioChatter functionalities for processing and passing the text to the vector database.</p> <pre><code>from biochatter.vectorstore import DocumentReader()\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# read and split document at `pdf_path`\nreader = DocumentReader()\ndocs = reader.load_document(pdf_path)\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=chunk_size,\n    chunk_overlap=chunk_overlap,\n    separators=[\" \", \",\", \"\\n\"],\n)\nsplit_text = text_splitter.split_documents(docs)\n\n# embed and store embeddings in the connected vector DB\ndoc_id = dbHost.store_embeddings(splitted_docs)\n</code></pre> <p>The dbHost class takes care of calling an embedding model, storing the embedding in the database, and returning a document ID that can be used to refer to the stored document.</p>"},{"location":"rag/#semantic-search","title":"Semantic search","text":"<p>To perform a semantic similarity search, all that is left to do is pass a question or statement to the <code>dbHost</code>, which will be embedded and compared to the present embeddings, returning a number <code>k</code> most similar text fragments.</p> <pre><code>results = dbHost.similarity_search(\n    query=\"Semantic similarity search query\",\n    k=3,\n)\n</code></pre>"},{"location":"rag/#vectorstore-management","title":"Vectorstore management","text":"<p>Using the collections we created at setup, we can delete entries in the vector database using their IDs. We can also return a list of all collected docs to determine which we want to delete.</p> <pre><code>docs = dbHost.get_all_documents()\nres = dbHost.remove_document(docs[0][\"id\"])\n</code></pre>"},{"location":"reflexion-agent/","title":"Reflexion Agent","text":"<p>While current LLMs have many capabilities, their outputs can be unstable at  times. To stabilise responses and allow more complex agent workflows, we have introduced a Reflexion Agent, allowing agents to reflect on their experiences, score their output, and self-improve.</p>"},{"location":"reflexion-agent/#workflow","title":"Workflow","text":"<p>The workflow of a Reflexion Agent is composed of individual nodes that can either generate or consume data as follows:</p> <p></p> <p>draft: in this node, an LLM is initially prompted to generate a specific text and action. (mem &lt;- AIMessage(...)).  </p> <p>execute tool: this node executes a tool function based on an action/text generated in the previous node. (mem &lt;- ToolMessage(...)).  </p> <p>revise: this node scores the output of the tool call and generates a self-reflection to provide feedback aimed at improving the results. (mem &lt;- AIMessage(...))  </p> <p>evaluate: this node assesses the quality of the generated outputs  </p> <p>memory: a list of BaseMessage</p>"},{"location":"reflexion-agent/#usage","title":"Usage","text":"<p>The <code>ReflexionAgent</code> class can be used to implement a reflexion workflow. Here, we demonstrate the ability to generate Cypher queries based on user's question using the <code>KGQueryReflexionAgent</code> class, which is derived from the abstract <code>ReflexionAgent</code> base class.</p> <p>To use the <code>KGQueryReflexionAgent</code>:</p> <ol> <li>We pass in connection arguments that enable connection to the target graph database and a conversation factory, which can create an instance of GptConversation (see Basic Usage: Chat).</li> </ol> <pre><code>import os\nfrom biochatter.llm_connect import GptConversation\nfrom biochatter.kg_langgraph_agent import KGQueryReflexionAgent\ndef create_conversation():\n    conversation = GptConversation(model_name=\"gpt-3.5-turbo\", prompts={})\n    conversation.set_api_key(os.getenv(\"OPENAI_API_KEY\"))\n    return conversation\n\nconnection_args = {\n    \"host\": \"127.0.0.1\",\n    \"port\": \"7687\",\n}\n\nagent = KGQueryReflexionAgent(\n    connection_args=connection_args,\n    conversation_factory=create_conversation,\n)\n</code></pre> <ol> <li>We generate the basic Knowledge Graph prompt for the LLM based on a user's question with the <code>BioCypherPromptEngine</code>, which provides node info, edge info, and node and edge properties based on the user's question.</li> </ol> <pre><code>from biochatter.prompts import BioCypherPromptEngine\nprompt_engine = BioCypherPromptEngine(\n    model_name=\"gpt-3.5-turbo\",\n    schema_config_or_info_dict=schema_dict,  # the schema definition of our graph\n    conversation_factory=create_conversation,\n)\nkg_prompt = prompt_engine.generate_query_prompt(question)\n</code></pre> <ol> <li>We can now use the agent to generate and reflect on the Cypher query and optimise it.</li> </ol> <pre><code>cypher_query = agent.execute(question, kg_prompt)\n</code></pre>"},{"location":"reflexion-agent/#implementation","title":"Implementation","text":"<p>To use the <code>ReflexionAgent</code> class, we need to implement the following abstract methods:</p> <ol> <li> <p>_tool_function(self, state: List[BaseMessage]):  execute tool function based on previous action/text and return ToolMessage</p> </li> <li> <p>_create_initial_responder(self, prompt: Option[str]): create draft responder, which is used to generate the initial answer</p> </li> <li> <p>_create_revise_responder(self, prompt: Optional[str]): create revise responder, which is used to score outputs and revise the answers</p> </li> <li> <p>_log_step_message(self, step: int, node: str, output: BaseMessage): parse step message and generate logs</p> </li> <li> <p>_log_final_result(self, output: BaseMessage): parse final result and generate logs</p> </li> <li> <p>_parse_final_result(self, output: BaseMessage): parse final result</p> </li> </ol> <p>As an example, we use the <code>kg_langgraph_agent.py</code> implementation that can reflect on the task of generating a knowledge graph query. The <code>KGQueryReflexionAgent</code> derived from <code>ReflexionAgent</code> is the main class to perform this task. In the <code>KGQueryReflexionAgent</code>, we have implemented the abstract methods described above:</p> <ol> <li> <p>_tool_function(self, state: List[BaseMessage]): connect to kg database and query KG in draft/revise node</p> </li> <li> <p>_create_initial_responder(self, prompt: Option[str]): create initial responder, which prompts LLM to generate the query</p> </li> </ol> <p>initial prompts: <pre><code>(\n    \"system\",\n    (\n        \"As a senior biomedical researcher and graph database expert, \"\n        f\"your task is to generate '{query_lang}' queries to extract data from our graph database based on the user's question. \"\n        \"\"\"Current time {time}. {instruction}\"\"\"\n    ),\n),\n(\n    \"system\",\n    \"Only generate query according to the user's question above.\",\n),\n</code></pre></p> <p>Initial answer schema:  </p> <pre><code>class GenerateQuery(BaseModel):\n    \"\"\"Generate the query.\"\"\"\n\n    answer: str = Field(\n        description=\"Cypher query for graph database according to user's question.\"\n    )\n    reflection: str = Field(\n        description=\"Your reflection on the initial answer, critique of what to improve\"\n    )\n    search_queries: List[str] = Field(description=\"query for graph database\")\n</code></pre> <ol> <li>_create_revise_responder(self, prompt: Optional[str]): create revise responder, which prompts LLM to score the outputs, reflects on the outputs, and revises the current query</li> </ol> <p>Revise prompts:  </p> <pre><code>\"\"\"\nRevise your previous query using the query result and follow the guidelines:\n1. If you consistently obtain empty results, please consider removing constraints such as relationship constraints to try to obtain a result.\n2. You should use previous critique to improve your query.\n3. Only generate a query without returning any other text.\n\"\"\"\n</code></pre> <p>Revise answer schema:  </p> <pre><code>class ReviseQuery(GenerateQuery):\n    \"\"\"Revise your previous query according to your question.\"\"\"\n\n    revised_query: str = Field(description=REVISED_QUERY_\"Revised query\"DESCRIPTION)\n    score: str = Field(description=(\n    \"the score for the query based on its query result\"\n    \" and relevance to the user's question,\"\n    \" with 0 representing the lowest score and 10 representing the highest score.\"))\n</code></pre> <ol> <li> <p>_log_step_message(self, step: int, node: str, output: BaseMessage): parse message from current step and generate logs</p> </li> <li> <p>_log_final_result(self, output: BaseMessage): parse final result and generate logs</p> </li> <li> <p>_parse_final_result(self, output: BaseMessage): parse final result</p> </li> <li> <p>_should_continue(self, state: List[BaseMessage]): assess output and determine if we can exit loop based on the following rules:   1). if loop steps are greater than limit (30 or user defined), exit   2). if score in previous revise node is greater than 7, exit   3). if query result in execute_tool node is not empty, exit  </p> </li> </ol>"},{"location":"vectorstore-reference/","title":"Vectorstore module","text":"<p>Here we handle the application of vectorstore services to retrieval-augmented generation tasks by embedding documents.</p>"},{"location":"vectorstore-reference/#biochatter.vectorstore.DocumentEmbedder","title":"<code>DocumentEmbedder</code>","text":"Source code in <code>biochatter/vectorstore.py</code> <pre><code>class DocumentEmbedder:\n    def __init__(\n        self,\n        used: bool = False,\n        online: bool = False,\n        chunk_size: int = 1000,\n        chunk_overlap: int = 0,\n        split_by_characters: bool = True,\n        separators: Optional[list] = None,\n        n_results: int = 3,\n        model: Optional[str] = \"text-embedding-ada-002\",\n        vector_db_vendor: Optional[str] = None,\n        connection_args: Optional[dict] = None,\n        embedding_collection_name: Optional[str] = None,\n        metadata_collection_name: Optional[str] = None,\n        api_key: Optional[str] = None,\n        is_azure: Optional[bool] = False,\n        azure_deployment: Optional[str] = None,\n        azure_endpoint: Optional[str] = None,\n        base_url: Optional[str] = None,\n        embeddings: Optional[\n            OpenAIEmbeddings | XinferenceEmbeddings | OllamaEmbeddings\n        ] = None,\n        documentids_workspace: Optional[list[str]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Class that handles the retrieval-augmented generation (RAG) functionality\n        of BioChatter. It splits text into chunks, embeds them, and stores them in\n        a vector database. It can then be used to do similarity search on the\n        database.\n\n        Args:\n\n            used (bool, optional): whether RAG has been used (frontend setting).\n                Defaults to False.\n\n            online (bool, optional): whether we are running the frontend online.\n                Defaults to False.\n\n            chunk_size (int, optional): size of chunks to split text into.\n                Defaults to 1000.\n\n            chunk_overlap (int, optional): overlap between chunks. Defaults to 0.\n\n            split_by_characters (bool, optional): whether to split by characters\n                or tokens. Defaults to True.\n\n            separators (Optional[list], optional): list of separators to use when\n                splitting by characters. Defaults to [\" \", \",\", \"\\n\"].\n\n            n_results (int, optional): number of results to return from\n                similarity search. Defaults to 3.\n\n            model (Optional[str], optional): name of model to use for embeddings.\n                Defaults to 'text-embedding-ada-002'.\n\n            vector_db_vendor (Optional[str], optional): name of vector database\n                to use. Defaults to Milvus.\n\n            connection_args (Optional[dict], optional): arguments to pass to\n                vector database connection. Defaults to None.\n\n            api_key (Optional[str], optional): OpenAI API key. Defaults to None.\n\n            base_url (Optional[str], optional): base url of OpenAI API.\n\n            embeddings (Optional[OpenAIEmbeddings | XinferenceEmbeddings],\n                optional): Embeddings object to use. Defaults to OpenAI.\n\n            documentids_workspace (Optional[List[str]], optional): a list of document IDs\n                that defines the scope within which rag operations (remove, similarity search,\n                and get all) occur. Defaults to None, which means the operations will be\n                performed across all documents in the database.\n\n            is_azure (Optional[bool], optional): if we are using Azure\n            azure_deployment (Optional[str], optional): Azure embeddings model deployment,\n                should work with azure_endpoint when is_azure is True\n            azure_endpoint (Optional[str], optional): Azure endpoint, should work with\n                azure_deployment when is_azure is True\n\n        \"\"\"\n        self.used = used\n        self.online = online\n        self.chunk_size = chunk_size\n        self.chunk_overlap = chunk_overlap\n        self.separators = separators or [\" \", \",\", \"\\n\"]\n        self.n_results = n_results\n        self.split_by_characters = split_by_characters\n        self.model_name = model\n\n        # TODO API Key handling to central config?\n        if base_url:\n            openai.api_base = base_url\n\n        if embeddings:\n            self.embeddings = embeddings\n        else:\n            if not self.online:\n                self.embeddings = (\n                    OpenAIEmbeddings(openai_api_key=api_key, model=model)\n                    if not is_azure\n                    else AzureOpenAIEmbeddings(\n                        api_key=api_key,\n                        azure_deployment=azure_deployment,\n                        azure_endpoint=azure_endpoint,\n                        model=model,\n                    )\n                )\n            else:\n                self.embeddings = None\n\n        # connection arguments\n        self.connection_args = connection_args or {\n            \"host\": \"127.0.0.1\",\n            \"port\": \"19530\",\n        }\n        self.embedding_collection_name = embedding_collection_name\n        self.metadata_collection_name = metadata_collection_name\n        self.documentids_workspace = documentids_workspace\n\n        # TODO: vector db selection\n        self.vector_db_vendor = vector_db_vendor or \"milvus\"\n        # instantiate VectorDatabaseHost\n        self.database_host = None\n        self._init_database_host()\n\n    def _set_embeddings(self, embeddings):\n        print(\"setting embedder\")\n        self.embeddings = embeddings\n\n    def _init_database_host(self):\n        if self.vector_db_vendor == \"milvus\":\n            self.database_host = VectorDatabaseAgentMilvus(\n                embedding_func=self.embeddings,\n                connection_args=self.connection_args,\n                embedding_collection_name=self.embedding_collection_name,\n                metadata_collection_name=self.metadata_collection_name,\n            )\n        else:\n            raise NotImplementedError(self.vector_db_vendor)\n\n    def set_chunk_siue(self, chunk_size: int) -&gt; None:\n        self.chunk_size = chunk_size\n\n    def set_chunk_overlap(self, chunk_overlap: int) -&gt; None:\n        self.chunk_overlap = chunk_overlap\n\n    def set_separators(self, separators: list) -&gt; None:\n        self.separators = separators\n\n    def _characters_splitter(self) -&gt; RecursiveCharacterTextSplitter:\n        return RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=self.separators,\n        )\n\n    def _tokens_splitter(self) -&gt; RecursiveCharacterTextSplitter:\n        DEFAULT_OPENAI_MODEL = \"gpt-3.5-turbo\"\n        HUGGINGFACE_MODELS = [\"bigscience/bloom\"]\n        if self.model_name and self.model_name in HUGGINGFACE_MODELS:\n            tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n            return RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n                tokenizer,\n                chunk_size=self.chunk_size,\n                chunk_overlap=self.chunk_overlap,\n                separators=self.separators,\n            )\n        else:\n            return RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n                encoding_name=\"\",\n                model_name=(\n                    DEFAULT_OPENAI_MODEL\n                    if not self.model_name\n                    else self.model_name\n                ),\n                chunk_size=self.chunk_size,\n                chunk_overlap=self.chunk_overlap,\n                separators=self.separators,\n            )\n\n    def _text_splitter(self) -&gt; RecursiveCharacterTextSplitter:\n        return (\n            self._characters_splitter()\n            if self.split_by_characters\n            else self._tokens_splitter()\n        )\n\n    def save_document(self, doc: list[Document]) -&gt; str:\n        \"\"\"\n        This function saves document to the vector database\n        Args:\n            doc List[Document]: document content, read with DocumentReader load_document(),\n                or document_from_pdf(), document_from_txt()\n        Returns:\n            str: document id, which can be used to remove an uploaded document with remove_document()\n        \"\"\"\n        splitted = self._split_document(doc)\n        return self._store_embeddings(splitted)\n\n    def _split_document(self, document: list[Document]) -&gt; list[Document]:\n        text_splitter = self._text_splitter()\n        return text_splitter.split_documents(document)\n\n    def _store_embeddings(self, doc: list[Document]) -&gt; str:\n        return self.database_host.store_embeddings(documents=doc)\n\n    def connect(self) -&gt; None:\n        self.database_host.connect()\n\n    def get_all_documents(self) -&gt; list[dict]:\n        return self.database_host.get_all_documents(\n            doc_ids=self.documentids_workspace\n        )\n\n    def remove_document(self, doc_id: str) -&gt; None:\n        return self.database_host.remove_document(\n            doc_id, self.documentids_workspace\n        )\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.DocumentEmbedder.__init__","title":"<code>__init__(used=False, online=False, chunk_size=1000, chunk_overlap=0, split_by_characters=True, separators=None, n_results=3, model='text-embedding-ada-002', vector_db_vendor=None, connection_args=None, embedding_collection_name=None, metadata_collection_name=None, api_key=None, is_azure=False, azure_deployment=None, azure_endpoint=None, base_url=None, embeddings=None, documentids_workspace=None)</code>","text":"<pre><code>    Class that handles the retrieval-augmented generation (RAG) functionality\n    of BioChatter. It splits text into chunks, embeds them, and stores them in\n    a vector database. It can then be used to do similarity search on the\n    database.\n\n    Args:\n\n        used (bool, optional): whether RAG has been used (frontend setting).\n            Defaults to False.\n\n        online (bool, optional): whether we are running the frontend online.\n            Defaults to False.\n\n        chunk_size (int, optional): size of chunks to split text into.\n            Defaults to 1000.\n\n        chunk_overlap (int, optional): overlap between chunks. Defaults to 0.\n\n        split_by_characters (bool, optional): whether to split by characters\n            or tokens. Defaults to True.\n\n        separators (Optional[list], optional): list of separators to use when\n            splitting by characters. Defaults to [\" \", \",\", \"\n</code></pre> <p>\"].</p> <pre><code>        n_results (int, optional): number of results to return from\n            similarity search. Defaults to 3.\n\n        model (Optional[str], optional): name of model to use for embeddings.\n            Defaults to 'text-embedding-ada-002'.\n\n        vector_db_vendor (Optional[str], optional): name of vector database\n            to use. Defaults to Milvus.\n\n        connection_args (Optional[dict], optional): arguments to pass to\n            vector database connection. Defaults to None.\n\n        api_key (Optional[str], optional): OpenAI API key. Defaults to None.\n\n        base_url (Optional[str], optional): base url of OpenAI API.\n\n        embeddings (Optional[OpenAIEmbeddings | XinferenceEmbeddings],\n            optional): Embeddings object to use. Defaults to OpenAI.\n\n        documentids_workspace (Optional[List[str]], optional): a list of document IDs\n            that defines the scope within which rag operations (remove, similarity search,\n            and get all) occur. Defaults to None, which means the operations will be\n            performed across all documents in the database.\n\n        is_azure (Optional[bool], optional): if we are using Azure\n        azure_deployment (Optional[str], optional): Azure embeddings model deployment,\n            should work with azure_endpoint when is_azure is True\n        azure_endpoint (Optional[str], optional): Azure endpoint, should work with\n            azure_deployment when is_azure is True\n</code></pre> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def __init__(\n    self,\n    used: bool = False,\n    online: bool = False,\n    chunk_size: int = 1000,\n    chunk_overlap: int = 0,\n    split_by_characters: bool = True,\n    separators: Optional[list] = None,\n    n_results: int = 3,\n    model: Optional[str] = \"text-embedding-ada-002\",\n    vector_db_vendor: Optional[str] = None,\n    connection_args: Optional[dict] = None,\n    embedding_collection_name: Optional[str] = None,\n    metadata_collection_name: Optional[str] = None,\n    api_key: Optional[str] = None,\n    is_azure: Optional[bool] = False,\n    azure_deployment: Optional[str] = None,\n    azure_endpoint: Optional[str] = None,\n    base_url: Optional[str] = None,\n    embeddings: Optional[\n        OpenAIEmbeddings | XinferenceEmbeddings | OllamaEmbeddings\n    ] = None,\n    documentids_workspace: Optional[list[str]] = None,\n) -&gt; None:\n    \"\"\"\n    Class that handles the retrieval-augmented generation (RAG) functionality\n    of BioChatter. It splits text into chunks, embeds them, and stores them in\n    a vector database. It can then be used to do similarity search on the\n    database.\n\n    Args:\n\n        used (bool, optional): whether RAG has been used (frontend setting).\n            Defaults to False.\n\n        online (bool, optional): whether we are running the frontend online.\n            Defaults to False.\n\n        chunk_size (int, optional): size of chunks to split text into.\n            Defaults to 1000.\n\n        chunk_overlap (int, optional): overlap between chunks. Defaults to 0.\n\n        split_by_characters (bool, optional): whether to split by characters\n            or tokens. Defaults to True.\n\n        separators (Optional[list], optional): list of separators to use when\n            splitting by characters. Defaults to [\" \", \",\", \"\\n\"].\n\n        n_results (int, optional): number of results to return from\n            similarity search. Defaults to 3.\n\n        model (Optional[str], optional): name of model to use for embeddings.\n            Defaults to 'text-embedding-ada-002'.\n\n        vector_db_vendor (Optional[str], optional): name of vector database\n            to use. Defaults to Milvus.\n\n        connection_args (Optional[dict], optional): arguments to pass to\n            vector database connection. Defaults to None.\n\n        api_key (Optional[str], optional): OpenAI API key. Defaults to None.\n\n        base_url (Optional[str], optional): base url of OpenAI API.\n\n        embeddings (Optional[OpenAIEmbeddings | XinferenceEmbeddings],\n            optional): Embeddings object to use. Defaults to OpenAI.\n\n        documentids_workspace (Optional[List[str]], optional): a list of document IDs\n            that defines the scope within which rag operations (remove, similarity search,\n            and get all) occur. Defaults to None, which means the operations will be\n            performed across all documents in the database.\n\n        is_azure (Optional[bool], optional): if we are using Azure\n        azure_deployment (Optional[str], optional): Azure embeddings model deployment,\n            should work with azure_endpoint when is_azure is True\n        azure_endpoint (Optional[str], optional): Azure endpoint, should work with\n            azure_deployment when is_azure is True\n\n    \"\"\"\n    self.used = used\n    self.online = online\n    self.chunk_size = chunk_size\n    self.chunk_overlap = chunk_overlap\n    self.separators = separators or [\" \", \",\", \"\\n\"]\n    self.n_results = n_results\n    self.split_by_characters = split_by_characters\n    self.model_name = model\n\n    # TODO API Key handling to central config?\n    if base_url:\n        openai.api_base = base_url\n\n    if embeddings:\n        self.embeddings = embeddings\n    else:\n        if not self.online:\n            self.embeddings = (\n                OpenAIEmbeddings(openai_api_key=api_key, model=model)\n                if not is_azure\n                else AzureOpenAIEmbeddings(\n                    api_key=api_key,\n                    azure_deployment=azure_deployment,\n                    azure_endpoint=azure_endpoint,\n                    model=model,\n                )\n            )\n        else:\n            self.embeddings = None\n\n    # connection arguments\n    self.connection_args = connection_args or {\n        \"host\": \"127.0.0.1\",\n        \"port\": \"19530\",\n    }\n    self.embedding_collection_name = embedding_collection_name\n    self.metadata_collection_name = metadata_collection_name\n    self.documentids_workspace = documentids_workspace\n\n    # TODO: vector db selection\n    self.vector_db_vendor = vector_db_vendor or \"milvus\"\n    # instantiate VectorDatabaseHost\n    self.database_host = None\n    self._init_database_host()\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.DocumentEmbedder.save_document","title":"<code>save_document(doc)</code>","text":"<p>This function saves document to the vector database Args:     doc List[Document]: document content, read with DocumentReader load_document(),         or document_from_pdf(), document_from_txt() Returns:     str: document id, which can be used to remove an uploaded document with remove_document()</p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def save_document(self, doc: list[Document]) -&gt; str:\n    \"\"\"\n    This function saves document to the vector database\n    Args:\n        doc List[Document]: document content, read with DocumentReader load_document(),\n            or document_from_pdf(), document_from_txt()\n    Returns:\n        str: document id, which can be used to remove an uploaded document with remove_document()\n    \"\"\"\n    splitted = self._split_document(doc)\n    return self._store_embeddings(splitted)\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.DocumentReader","title":"<code>DocumentReader</code>","text":"Source code in <code>biochatter/vectorstore.py</code> <pre><code>class DocumentReader:\n    def load_document(self, path: str) -&gt; list[Document]:\n        \"\"\"\n        Loads a document from a path; accepts txt and pdf files. Txt files are\n        loaded as-is, pdf files are converted to text using fitz.\n\n        Args:\n            path (str): path to document\n\n        Returns:\n            List[Document]: list of documents\n        \"\"\"\n        if path.endswith(\".txt\"):\n            loader = TextLoader(path)\n            return loader.load()\n\n        elif path.endswith(\".pdf\"):\n            doc = fitz.open(path)\n            text = \"\"\n            for page in doc:\n                text += page.get_text()\n\n            meta = {k: v for k, v in doc.metadata.items() if v}\n            meta.update({\"source\": path})\n\n            return [\n                Document(\n                    page_content=text,\n                    metadata=meta,\n                )\n            ]\n\n    def document_from_pdf(self, pdf: bytes) -&gt; list[Document]:\n        \"\"\"\n        Receive a byte representation of a pdf file and return a list of Documents\n        with metadata.\n\n        Args:\n            pdf (bytes): byte representation of pdf file\n\n        Returns:\n            List[Document]: list of documents\n        \"\"\"\n        doc = fitz.open(stream=pdf, filetype=\"pdf\")\n        text = \"\"\n        for page in doc:\n            text += page.get_text()\n\n        meta = {k: v for k, v in doc.metadata.items() if v}\n        meta.update({\"source\": \"pdf\"})\n\n        return [\n            Document(\n                page_content=text,\n                metadata=meta,\n            )\n        ]\n\n    def document_from_txt(self, txt: bytes) -&gt; list[Document]:\n        \"\"\"\n        Receive a byte representation of a txt file and return a list of Documents\n        with metadata.\n\n        Args:\n            txt (bytes): byte representation of txt file\n\n        Returns:\n            List[Document]: list of documents\n        \"\"\"\n        meta = {\"source\": \"txt\"}\n        return [\n            Document(\n                page_content=txt,\n                metadata=meta,\n            )\n        ]\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.DocumentReader.document_from_pdf","title":"<code>document_from_pdf(pdf)</code>","text":"<p>Receive a byte representation of a pdf file and return a list of Documents with metadata.</p> <p>Parameters:</p> Name Type Description Default <code>pdf</code> <code>bytes</code> <p>byte representation of pdf file</p> required <p>Returns:</p> Type Description <code>list[Document]</code> <p>List[Document]: list of documents</p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def document_from_pdf(self, pdf: bytes) -&gt; list[Document]:\n    \"\"\"\n    Receive a byte representation of a pdf file and return a list of Documents\n    with metadata.\n\n    Args:\n        pdf (bytes): byte representation of pdf file\n\n    Returns:\n        List[Document]: list of documents\n    \"\"\"\n    doc = fitz.open(stream=pdf, filetype=\"pdf\")\n    text = \"\"\n    for page in doc:\n        text += page.get_text()\n\n    meta = {k: v for k, v in doc.metadata.items() if v}\n    meta.update({\"source\": \"pdf\"})\n\n    return [\n        Document(\n            page_content=text,\n            metadata=meta,\n        )\n    ]\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.DocumentReader.document_from_txt","title":"<code>document_from_txt(txt)</code>","text":"<p>Receive a byte representation of a txt file and return a list of Documents with metadata.</p> <p>Parameters:</p> Name Type Description Default <code>txt</code> <code>bytes</code> <p>byte representation of txt file</p> required <p>Returns:</p> Type Description <code>list[Document]</code> <p>List[Document]: list of documents</p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def document_from_txt(self, txt: bytes) -&gt; list[Document]:\n    \"\"\"\n    Receive a byte representation of a txt file and return a list of Documents\n    with metadata.\n\n    Args:\n        txt (bytes): byte representation of txt file\n\n    Returns:\n        List[Document]: list of documents\n    \"\"\"\n    meta = {\"source\": \"txt\"}\n    return [\n        Document(\n            page_content=txt,\n            metadata=meta,\n        )\n    ]\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.DocumentReader.load_document","title":"<code>load_document(path)</code>","text":"<p>Loads a document from a path; accepts txt and pdf files. Txt files are loaded as-is, pdf files are converted to text using fitz.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path to document</p> required <p>Returns:</p> Type Description <code>list[Document]</code> <p>List[Document]: list of documents</p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def load_document(self, path: str) -&gt; list[Document]:\n    \"\"\"\n    Loads a document from a path; accepts txt and pdf files. Txt files are\n    loaded as-is, pdf files are converted to text using fitz.\n\n    Args:\n        path (str): path to document\n\n    Returns:\n        List[Document]: list of documents\n    \"\"\"\n    if path.endswith(\".txt\"):\n        loader = TextLoader(path)\n        return loader.load()\n\n    elif path.endswith(\".pdf\"):\n        doc = fitz.open(path)\n        text = \"\"\n        for page in doc:\n            text += page.get_text()\n\n        meta = {k: v for k, v in doc.metadata.items() if v}\n        meta.update({\"source\": path})\n\n        return [\n            Document(\n                page_content=text,\n                metadata=meta,\n            )\n        ]\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.OllamaDocumentEmbedder","title":"<code>OllamaDocumentEmbedder</code>","text":"<p>               Bases: <code>DocumentEmbedder</code></p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>class OllamaDocumentEmbedder(DocumentEmbedder):\n    def __init__(\n        self,\n        used: bool = False,\n        chunk_size: int = 1000,\n        chunk_overlap: int = 0,\n        split_by_characters: bool = True,\n        separators: Optional[list] = None,\n        n_results: int = 3,\n        model: Optional[str] = \"nomic-embed-text\",\n        vector_db_vendor: Optional[str] = None,\n        connection_args: Optional[dict] = None,\n        embedding_collection_name: Optional[str] = None,\n        metadata_collection_name: Optional[str] = None,\n        api_key: Optional[str] = \"none\",\n        base_url: Optional[str] = None,\n        documentids_workspace: Optional[list[str]] = None,\n    ):\n        \"\"\"\n        Extension of the DocumentEmbedder class that uses Ollama for\n        embeddings.\n\n        Args:\n\n            used (bool, optional): whether RAG has been used (frontend setting).\n\n            chunk_size (int, optional): size of chunks to split text into.\n\n            chunk_overlap (int, optional): overlap between chunks.\n\n            split_by_characters (bool, optional): whether to split by characters\n            or tokens.\n\n            separators (Optional[list], optional): list of separators to use when\n            splitting by characters.\n\n            n_results (int, optional): number of results to return from\n            similarity search.\n\n            model (Optional[str], optional): name of model to use for embeddings.\n            Can be \"auto\" to use the first available model.\n\n            vector_db_vendor (Optional[str], optional): name of vector database\n            to use.\n\n            connection_args (Optional[dict], optional): arguments to pass to\n            vector database connection.\n\n            embedding_collection_name (Optional[str], optional): name of\n            collection to store embeddings in.\n\n            metadata_collection_name (Optional[str], optional): name of\n            collection to store metadata in.\n\n            api_key (Optional[str], optional): Xinference API key.\n\n            base_url (Optional[str], optional): base url of Xinference API.\n\n            documentids_workspace (Optional[List[str]], optional): a list of document IDs\n            that defines the scope within which rag operations (remove, similarity search,\n            and get all) occur. Defaults to None, which means the operations will be\n            performed across all documents in the database.\n\n        \"\"\"\n        from langchain_community.embeddings import OllamaEmbeddings\n\n        self.model_name = model\n\n        super().__init__(\n            used=used,\n            online=True,\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap,\n            split_by_characters=split_by_characters,\n            separators=separators,\n            n_results=n_results,\n            model=model,\n            vector_db_vendor=vector_db_vendor,\n            connection_args=connection_args,\n            embedding_collection_name=embedding_collection_name,\n            metadata_collection_name=metadata_collection_name,\n            api_key=api_key,\n            base_url=base_url,\n            embeddings=OllamaEmbeddings(\n                base_url=base_url, model=self.model_name\n            ),\n            documentids_workspace=documentids_workspace,\n        )\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.OllamaDocumentEmbedder.__init__","title":"<code>__init__(used=False, chunk_size=1000, chunk_overlap=0, split_by_characters=True, separators=None, n_results=3, model='nomic-embed-text', vector_db_vendor=None, connection_args=None, embedding_collection_name=None, metadata_collection_name=None, api_key='none', base_url=None, documentids_workspace=None)</code>","text":"<p>Extension of the DocumentEmbedder class that uses Ollama for embeddings.</p> <p>Args:</p> <pre><code>used (bool, optional): whether RAG has been used (frontend setting).\n\nchunk_size (int, optional): size of chunks to split text into.\n\nchunk_overlap (int, optional): overlap between chunks.\n\nsplit_by_characters (bool, optional): whether to split by characters\nor tokens.\n\nseparators (Optional[list], optional): list of separators to use when\nsplitting by characters.\n\nn_results (int, optional): number of results to return from\nsimilarity search.\n\nmodel (Optional[str], optional): name of model to use for embeddings.\nCan be \"auto\" to use the first available model.\n\nvector_db_vendor (Optional[str], optional): name of vector database\nto use.\n\nconnection_args (Optional[dict], optional): arguments to pass to\nvector database connection.\n\nembedding_collection_name (Optional[str], optional): name of\ncollection to store embeddings in.\n\nmetadata_collection_name (Optional[str], optional): name of\ncollection to store metadata in.\n\napi_key (Optional[str], optional): Xinference API key.\n\nbase_url (Optional[str], optional): base url of Xinference API.\n\ndocumentids_workspace (Optional[List[str]], optional): a list of document IDs\nthat defines the scope within which rag operations (remove, similarity search,\nand get all) occur. Defaults to None, which means the operations will be\nperformed across all documents in the database.\n</code></pre> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def __init__(\n    self,\n    used: bool = False,\n    chunk_size: int = 1000,\n    chunk_overlap: int = 0,\n    split_by_characters: bool = True,\n    separators: Optional[list] = None,\n    n_results: int = 3,\n    model: Optional[str] = \"nomic-embed-text\",\n    vector_db_vendor: Optional[str] = None,\n    connection_args: Optional[dict] = None,\n    embedding_collection_name: Optional[str] = None,\n    metadata_collection_name: Optional[str] = None,\n    api_key: Optional[str] = \"none\",\n    base_url: Optional[str] = None,\n    documentids_workspace: Optional[list[str]] = None,\n):\n    \"\"\"\n    Extension of the DocumentEmbedder class that uses Ollama for\n    embeddings.\n\n    Args:\n\n        used (bool, optional): whether RAG has been used (frontend setting).\n\n        chunk_size (int, optional): size of chunks to split text into.\n\n        chunk_overlap (int, optional): overlap between chunks.\n\n        split_by_characters (bool, optional): whether to split by characters\n        or tokens.\n\n        separators (Optional[list], optional): list of separators to use when\n        splitting by characters.\n\n        n_results (int, optional): number of results to return from\n        similarity search.\n\n        model (Optional[str], optional): name of model to use for embeddings.\n        Can be \"auto\" to use the first available model.\n\n        vector_db_vendor (Optional[str], optional): name of vector database\n        to use.\n\n        connection_args (Optional[dict], optional): arguments to pass to\n        vector database connection.\n\n        embedding_collection_name (Optional[str], optional): name of\n        collection to store embeddings in.\n\n        metadata_collection_name (Optional[str], optional): name of\n        collection to store metadata in.\n\n        api_key (Optional[str], optional): Xinference API key.\n\n        base_url (Optional[str], optional): base url of Xinference API.\n\n        documentids_workspace (Optional[List[str]], optional): a list of document IDs\n        that defines the scope within which rag operations (remove, similarity search,\n        and get all) occur. Defaults to None, which means the operations will be\n        performed across all documents in the database.\n\n    \"\"\"\n    from langchain_community.embeddings import OllamaEmbeddings\n\n    self.model_name = model\n\n    super().__init__(\n        used=used,\n        online=True,\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        split_by_characters=split_by_characters,\n        separators=separators,\n        n_results=n_results,\n        model=model,\n        vector_db_vendor=vector_db_vendor,\n        connection_args=connection_args,\n        embedding_collection_name=embedding_collection_name,\n        metadata_collection_name=metadata_collection_name,\n        api_key=api_key,\n        base_url=base_url,\n        embeddings=OllamaEmbeddings(\n            base_url=base_url, model=self.model_name\n        ),\n        documentids_workspace=documentids_workspace,\n    )\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.XinferenceDocumentEmbedder","title":"<code>XinferenceDocumentEmbedder</code>","text":"<p>               Bases: <code>DocumentEmbedder</code></p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>class XinferenceDocumentEmbedder(DocumentEmbedder):\n    def __init__(\n        self,\n        used: bool = False,\n        chunk_size: int = 1000,\n        chunk_overlap: int = 0,\n        split_by_characters: bool = True,\n        separators: Optional[list] = None,\n        n_results: int = 3,\n        model: Optional[str] = \"auto\",\n        vector_db_vendor: Optional[str] = None,\n        connection_args: Optional[dict] = None,\n        embedding_collection_name: Optional[str] = None,\n        metadata_collection_name: Optional[str] = None,\n        api_key: Optional[str] = \"none\",\n        base_url: Optional[str] = None,\n        documentids_workspace: Optional[list[str]] = None,\n    ):\n        \"\"\"\n        Extension of the DocumentEmbedder class that uses Xinference for\n        embeddings.\n\n        Args:\n\n            used (bool, optional): whether RAG has been used (frontend setting).\n\n            chunk_size (int, optional): size of chunks to split text into.\n\n            chunk_overlap (int, optional): overlap between chunks.\n\n            split_by_characters (bool, optional): whether to split by characters\n            or tokens.\n\n            separators (Optional[list], optional): list of separators to use when\n            splitting by characters.\n\n            n_results (int, optional): number of results to return from\n            similarity search.\n\n            model (Optional[str], optional): name of model to use for embeddings.\n            Can be \"auto\" to use the first available model.\n\n            vector_db_vendor (Optional[str], optional): name of vector database\n            to use.\n\n            connection_args (Optional[dict], optional): arguments to pass to\n            vector database connection.\n\n            embedding_collection_name (Optional[str], optional): name of\n            collection to store embeddings in.\n\n            metadata_collection_name (Optional[str], optional): name of\n            collection to store metadata in.\n\n            api_key (Optional[str], optional): Xinference API key.\n\n            base_url (Optional[str], optional): base url of Xinference API.\n\n            documentids_workspace (Optional[List[str]], optional): a list of document IDs\n            that defines the scope within which rag operations (remove, similarity search,\n            and get all) occur. Defaults to None, which means the operations will be\n            performed across all documents in the database.\n\n        \"\"\"\n        from xinference.client import Client\n\n        self.model_name = model\n        self.client = Client(base_url=base_url)\n        self.models = {}\n        self.load_models()\n\n        if self.model_name is None or self.model_name == \"auto\":\n            self.model_name = self.list_models_by_type(\"embedding\")[0]\n        self.model_uid = self.models[self.model_name][\"id\"]\n\n        super().__init__(\n            used=used,\n            online=True,\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap,\n            split_by_characters=split_by_characters,\n            separators=separators,\n            n_results=n_results,\n            model=model,\n            vector_db_vendor=vector_db_vendor,\n            connection_args=connection_args,\n            embedding_collection_name=embedding_collection_name,\n            metadata_collection_name=metadata_collection_name,\n            api_key=api_key,\n            base_url=base_url,\n            embeddings=XinferenceEmbeddings(\n                server_url=base_url, model_uid=self.model_uid\n            ),\n            documentids_workspace=documentids_workspace,\n        )\n\n    def load_models(self):\n        \"\"\"\n        Return all models that are currently available on the Xinference server.\n\n        Returns:\n            dict: dict of models\n        \"\"\"\n        for id, model in self.client.list_models().items():\n            model[\"id\"] = id\n            self.models[model[\"model_name\"]] = model\n\n    def list_models_by_type(self, type: str):\n        \"\"\"\n        Return all models of a certain type that are currently available on the\n        Xinference server.\n\n        Args:\n            type (str): type of model to list (e.g. \"embedding\", \"chat\")\n\n        Returns:\n            List[str]: list of model names\n        \"\"\"\n        names = []\n        for name, model in self.models.items():\n            if \"model_ability\" in model:\n                if type in model[\"model_ability\"]:\n                    names.append(name)\n            elif model[\"model_type\"] == type:\n                names.append(name)\n        return names\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.XinferenceDocumentEmbedder.__init__","title":"<code>__init__(used=False, chunk_size=1000, chunk_overlap=0, split_by_characters=True, separators=None, n_results=3, model='auto', vector_db_vendor=None, connection_args=None, embedding_collection_name=None, metadata_collection_name=None, api_key='none', base_url=None, documentids_workspace=None)</code>","text":"<p>Extension of the DocumentEmbedder class that uses Xinference for embeddings.</p> <p>Args:</p> <pre><code>used (bool, optional): whether RAG has been used (frontend setting).\n\nchunk_size (int, optional): size of chunks to split text into.\n\nchunk_overlap (int, optional): overlap between chunks.\n\nsplit_by_characters (bool, optional): whether to split by characters\nor tokens.\n\nseparators (Optional[list], optional): list of separators to use when\nsplitting by characters.\n\nn_results (int, optional): number of results to return from\nsimilarity search.\n\nmodel (Optional[str], optional): name of model to use for embeddings.\nCan be \"auto\" to use the first available model.\n\nvector_db_vendor (Optional[str], optional): name of vector database\nto use.\n\nconnection_args (Optional[dict], optional): arguments to pass to\nvector database connection.\n\nembedding_collection_name (Optional[str], optional): name of\ncollection to store embeddings in.\n\nmetadata_collection_name (Optional[str], optional): name of\ncollection to store metadata in.\n\napi_key (Optional[str], optional): Xinference API key.\n\nbase_url (Optional[str], optional): base url of Xinference API.\n\ndocumentids_workspace (Optional[List[str]], optional): a list of document IDs\nthat defines the scope within which rag operations (remove, similarity search,\nand get all) occur. Defaults to None, which means the operations will be\nperformed across all documents in the database.\n</code></pre> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def __init__(\n    self,\n    used: bool = False,\n    chunk_size: int = 1000,\n    chunk_overlap: int = 0,\n    split_by_characters: bool = True,\n    separators: Optional[list] = None,\n    n_results: int = 3,\n    model: Optional[str] = \"auto\",\n    vector_db_vendor: Optional[str] = None,\n    connection_args: Optional[dict] = None,\n    embedding_collection_name: Optional[str] = None,\n    metadata_collection_name: Optional[str] = None,\n    api_key: Optional[str] = \"none\",\n    base_url: Optional[str] = None,\n    documentids_workspace: Optional[list[str]] = None,\n):\n    \"\"\"\n    Extension of the DocumentEmbedder class that uses Xinference for\n    embeddings.\n\n    Args:\n\n        used (bool, optional): whether RAG has been used (frontend setting).\n\n        chunk_size (int, optional): size of chunks to split text into.\n\n        chunk_overlap (int, optional): overlap between chunks.\n\n        split_by_characters (bool, optional): whether to split by characters\n        or tokens.\n\n        separators (Optional[list], optional): list of separators to use when\n        splitting by characters.\n\n        n_results (int, optional): number of results to return from\n        similarity search.\n\n        model (Optional[str], optional): name of model to use for embeddings.\n        Can be \"auto\" to use the first available model.\n\n        vector_db_vendor (Optional[str], optional): name of vector database\n        to use.\n\n        connection_args (Optional[dict], optional): arguments to pass to\n        vector database connection.\n\n        embedding_collection_name (Optional[str], optional): name of\n        collection to store embeddings in.\n\n        metadata_collection_name (Optional[str], optional): name of\n        collection to store metadata in.\n\n        api_key (Optional[str], optional): Xinference API key.\n\n        base_url (Optional[str], optional): base url of Xinference API.\n\n        documentids_workspace (Optional[List[str]], optional): a list of document IDs\n        that defines the scope within which rag operations (remove, similarity search,\n        and get all) occur. Defaults to None, which means the operations will be\n        performed across all documents in the database.\n\n    \"\"\"\n    from xinference.client import Client\n\n    self.model_name = model\n    self.client = Client(base_url=base_url)\n    self.models = {}\n    self.load_models()\n\n    if self.model_name is None or self.model_name == \"auto\":\n        self.model_name = self.list_models_by_type(\"embedding\")[0]\n    self.model_uid = self.models[self.model_name][\"id\"]\n\n    super().__init__(\n        used=used,\n        online=True,\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        split_by_characters=split_by_characters,\n        separators=separators,\n        n_results=n_results,\n        model=model,\n        vector_db_vendor=vector_db_vendor,\n        connection_args=connection_args,\n        embedding_collection_name=embedding_collection_name,\n        metadata_collection_name=metadata_collection_name,\n        api_key=api_key,\n        base_url=base_url,\n        embeddings=XinferenceEmbeddings(\n            server_url=base_url, model_uid=self.model_uid\n        ),\n        documentids_workspace=documentids_workspace,\n    )\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.XinferenceDocumentEmbedder.list_models_by_type","title":"<code>list_models_by_type(type)</code>","text":"<p>Return all models of a certain type that are currently available on the Xinference server.</p> <p>Parameters:</p> Name Type Description Default <code>type</code> <code>str</code> <p>type of model to list (e.g. \"embedding\", \"chat\")</p> required <p>Returns:</p> Type Description <p>List[str]: list of model names</p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def list_models_by_type(self, type: str):\n    \"\"\"\n    Return all models of a certain type that are currently available on the\n    Xinference server.\n\n    Args:\n        type (str): type of model to list (e.g. \"embedding\", \"chat\")\n\n    Returns:\n        List[str]: list of model names\n    \"\"\"\n    names = []\n    for name, model in self.models.items():\n        if \"model_ability\" in model:\n            if type in model[\"model_ability\"]:\n                names.append(name)\n        elif model[\"model_type\"] == type:\n            names.append(name)\n    return names\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.XinferenceDocumentEmbedder.load_models","title":"<code>load_models()</code>","text":"<p>Return all models that are currently available on the Xinference server.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>dict of models</p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def load_models(self):\n    \"\"\"\n    Return all models that are currently available on the Xinference server.\n\n    Returns:\n        dict: dict of models\n    \"\"\"\n    for id, model in self.client.list_models().items():\n        model[\"id\"] = id\n        self.models[model[\"model_name\"]] = model\n</code></pre>"},{"location":"vectorstore_agent-reference/","title":"Vectorstore agent module","text":"<p>Here we handle connections and management of various vectorstore services and semantic seach.</p>"},{"location":"vectorstore_agent-reference/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus","title":"<code>VectorDatabaseAgentMilvus</code>","text":"<p>The VectorDatabaseAgentMilvus class manages vector databases in a connected host database. It manages an embedding collection <code>_col_embeddings:langchain.vectorstores.Milvus</code>, which is the main information on the embedded text fragments and the basis for similarity search, and a metadata collection <code>_col_metadata:pymilvus.Collection</code>, which stores the metadata of the embedded text fragments. A typical workflow includes the following operations:</p> <ol> <li>connect to a host using <code>connect()</code></li> <li>get all documents in the active database using <code>get_all_documents()</code></li> <li>save a number of fragments, usually from a specific document, using     <code>store_embeddings()</code></li> <li>do similarity search among all fragments of the currently active database     using <code>similarity_search()</code></li> <li>remove a document from the currently active database using     <code>remove_document()</code></li> </ol> Source code in <code>biochatter/vectorstore_agent.py</code> <pre><code>class VectorDatabaseAgentMilvus:\n    \"\"\"\n    The VectorDatabaseAgentMilvus class manages vector databases in a connected\n    host database. It manages an embedding collection\n    `_col_embeddings:langchain.vectorstores.Milvus`, which is the main\n    information on the embedded text fragments and the basis for similarity\n    search, and a metadata collection `_col_metadata:pymilvus.Collection`, which\n    stores the metadata of the embedded text fragments. A typical workflow\n    includes the following operations:\n\n    1. connect to a host using `connect()`\n    2. get all documents in the active database using `get_all_documents()`\n    3. save a number of fragments, usually from a specific document, using\n        `store_embeddings()`\n    4. do similarity search among all fragments of the currently active database\n        using `similarity_search()`\n    5. remove a document from the currently active database using\n        `remove_document()`\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding_func: OpenAIEmbeddings,\n        connection_args: Optional[dict] = None,\n        embedding_collection_name: Optional[str] = None,\n        metadata_collection_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Args:\n            embedding_func OpenAIEmbeddings: Function used to embed the text\n\n            connection_args Optional dict: args to connect Vector Database\n\n            embedding_collection_name Optional str: exposed for test\n\n            metadata_collection_name Optional str: exposed for test\n        \"\"\"\n        self._embedding_func = embedding_func\n        self._col_embeddings: Optional[Milvus] = None\n        self._col_metadata: Optional[Collection] = None\n        self._connection_args = validate_connection_args(connection_args)\n        self._embedding_name = (\n            embedding_collection_name or DOCUMENT_EMBEDDINGS_COLLECTION_NAME\n        )\n        self._metadata_name = (\n            metadata_collection_name or DOCUMENT_METADATA_COLLECTION_NAME\n        )\n\n    def connect(self) -&gt; None:\n        \"\"\"\n        Connect to a host and read two document collections (the default names\n        are `DocumentEmbeddings` and `DocumentMetadata`) in the currently active\n        database (default database name is `default`); if those document\n        collections don't exist, create the two collections.\n        \"\"\"\n        self._connect(**self._connection_args)\n        self._init_host()\n\n    def _connect(self, host: str, port: str, user: str, password: str) -&gt; None:\n        self.alias = self._create_connection_alias(host, port, user, password)\n\n    def _init_host(self) -&gt; None:\n        \"\"\"\n        Initialize host. Will read/create document collection inside currently\n        active database.\n        \"\"\"\n        self._create_collections()\n\n    def _create_connection_alias(\n        self, host: str, port: str, user: str, password: str\n    ) -&gt; str:\n        \"\"\"\n        Connect to host and create a connection alias for metadata collection\n        using a random uuid.\n\n        Args:\n            host (str): host ip address\n            port (str): host port\n\n        Returns:\n            str: connection alias\n        \"\"\"\n        alias = uuid.uuid4().hex\n        try:\n            connections.connect(\n                host=host, port=port, user=user, password=password, alias=alias\n            )\n            logger.debug(f\"Created new connection using: {alias}\")\n            return alias\n        except MilvusException as e:\n            logger.error(f\"Failed to create  new connection using: {alias}\")\n            raise e\n\n    def _create_collections(self) -&gt; None:\n        \"\"\"\n        Create or load the embedding and metadata collections from the currently\n        active database.\n        \"\"\"\n        embedding_exists = utility.has_collection(\n            self._embedding_name, using=self.alias\n        )\n        meta_exists = utility.has_collection(\n            self._metadata_name,\n            using=self.alias,\n        )\n\n        if embedding_exists:\n            self._load_embeddings_collection()\n        else:\n            self._create_embeddings_collection()\n\n        if meta_exists:\n            self._load_metadata_collection()\n        else:\n            self._create_metadata_collection()\n\n        self._create_metadata_collection_index()\n        self._col_metadata.load()\n\n    def _load_embeddings_collection(self) -&gt; None:\n        \"\"\"\n        Load embeddings collection from currently active database.\n        \"\"\"\n        try:\n            self._col_embeddings = Milvus(\n                embedding_function=self._embedding_func,\n                collection_name=self._embedding_name,\n                connection_args=self._connection_args,\n            )\n        except MilvusException as e:\n            logger.error(\n                f\"Failed to load embeddings collection {self._embedding_name}.\"\n            )\n            raise e\n\n    def _create_embeddings_collection(self) -&gt; None:\n        \"\"\"\n        Create embedding collection.\n        All fields: \"meta_id\", \"vector\"\n        \"\"\"\n        try:\n            self._col_embeddings = Milvus(\n                embedding_function=self._embedding_func,\n                collection_name=self._embedding_name,\n                connection_args=self._connection_args,\n            )\n        except MilvusException as e:\n            logger.error(\n                f\"Failed to create embeddings collection {self._embedding_name}\"\n            )\n            raise e\n\n    def _load_metadata_collection(self) -&gt; None:\n        \"\"\"\n        Load metadata collection from currently active database.\n        \"\"\"\n        self._col_metadata = Collection(\n            self._metadata_name,\n            using=self.alias,\n        )\n        self._col_metadata.load()\n\n    def _create_metadata_collection(self) -&gt; None:\n        \"\"\"\n        Create metadata collection.\n\n        All fields: \"id\", \"name\", \"author\", \"title\", \"format\", \"subject\",\n        \"creator\", \"producer\", \"creationDate\", \"modDate\", \"source\", \"embedding\",\n        \"isDeleted\".\n\n        As the vector database requires a vector field, we will create a fake\n        vector \"embedding\". The field \"isDeleted\" is used to specify if the\n        document is deleted.\n        \"\"\"\n        doc_id = FieldSchema(\n            name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True\n        )\n        doc_name = FieldSchema(\n            name=\"name\", dtype=DataType.VARCHAR, max_length=255\n        )\n        doc_author = FieldSchema(\n            name=\"author\", dtype=DataType.VARCHAR, max_length=255\n        )\n        doc_title = FieldSchema(\n            name=\"title\", dtype=DataType.VARCHAR, max_length=1000\n        )\n        doc_format = FieldSchema(\n            name=\"format\", dtype=DataType.VARCHAR, max_length=255\n        )\n        doc_subject = FieldSchema(\n            name=\"subject\", dtype=DataType.VARCHAR, max_length=255\n        )\n        doc_creator = FieldSchema(\n            name=\"creator\", dtype=DataType.VARCHAR, max_length=255\n        )\n        doc_producer = FieldSchema(\n            name=\"producer\", dtype=DataType.VARCHAR, max_length=255\n        )\n        doc_creationDate = FieldSchema(\n            name=\"creationDate\", dtype=DataType.VARCHAR, max_length=64\n        )\n        doc_modDate = FieldSchema(\n            name=\"modDate\", dtype=DataType.VARCHAR, max_length=64\n        )\n        doc_source = FieldSchema(\n            name=\"source\", dtype=DataType.VARCHAR, max_length=1000\n        )\n        embedding = FieldSchema(\n            name=\"embedding\",\n            dtype=DataType.FLOAT_VECTOR,\n            dim=METADATA_VECTOR_DIM,\n        )\n        isDeleted = FieldSchema(\n            name=\"isDeleted\",\n            dtype=DataType.BOOL,\n        )\n        fields = [\n            doc_id,\n            doc_name,\n            doc_author,\n            doc_title,\n            doc_format,\n            doc_subject,\n            doc_creator,\n            doc_producer,\n            doc_creationDate,\n            doc_modDate,\n            doc_source,\n            embedding,\n            isDeleted,\n        ]\n        schema = CollectionSchema(fields=fields)\n        try:\n            self._col_metadata = Collection(\n                name=self._metadata_name, schema=schema, using=self.alias\n            )\n        except MilvusException as e:\n            logger.error(f\"Failed to create collection {self._metadata_name}\")\n            raise e\n\n    def _create_metadata_collection_index(self) -&gt; None:\n        \"\"\"\n        Create index for metadata collection in currently active database.\n        \"\"\"\n        if (\n            not isinstance(self._col_metadata, Collection)\n            or len(self._col_metadata.indexes) &gt; 0\n        ):\n            return\n\n        index_params = {\n            \"metric_type\": \"L2\",\n            \"index_type\": \"HNSW\",\n            \"params\": {\"M\": 8, \"efConstruction\": 64},\n        }\n\n        try:\n            self._col_metadata.create_index(\n                field_name=\"embedding\",\n                index_params=index_params,\n                using=self.alias,\n            )\n        except MilvusException as e:\n            logger.error(\n                \"Failed to create index for meta collection \"\n                f\"{self._metadata_name}.\"\n            )\n            raise e\n\n    def _insert_data(self, documents: list[Document]) -&gt; str:\n        \"\"\"\n        Insert documents into the currently active database.\n\n        Args:\n            documents (List[Documents]): documents array, usually from\n                DocumentReader.load_document, DocumentReader.document_from_pdf,\n                DocumentReader.document_from_txt\n\n        Returns:\n            str: document id\n        \"\"\"\n        if len(documents) == 0:\n            return None\n        metadata = [documents[0].metadata]\n        aligned_metadata = align_metadata(metadata)\n        try:\n            result = self._col_metadata.insert(aligned_metadata)\n            meta_id = str(result.primary_keys[0])\n            self._col_metadata.flush()\n        except MilvusException as e:\n            logger.error(f\"Failed to insert meta data\")\n            raise e\n        aligned_docs = align_embeddings(documents, meta_id)\n        try:\n            # As we passed collection_name, documents will be added to existed collection\n            self._col_embeddings = Milvus.from_documents(\n                embedding=self._embedding_func,\n                collection_name=self._embedding_name,\n                connection_args=self._connection_args,\n                documents=aligned_docs,\n            )\n        except MilvusException as e:\n            logger.error(\n                \"Failed to insert data to embedding collection \"\n                f\"{self._embedding_name}.\"\n            )\n            raise e\n        return meta_id\n\n    def store_embeddings(self, documents: list[Document]) -&gt; str:\n        \"\"\"\n        Store documents in the currently active database.\n\n        Args:\n            documents (List[Documents]): documents array, usually from\n                DocumentReader.load_document, DocumentReader.document_from_pdf,\n                DocumentReader.document_from_txt\n\n        Returns:\n            str: document id\n        \"\"\"\n        if len(documents) == 0:\n            return\n        return self._insert_data(documents)\n\n    def _build_embedding_search_expression(\n        self, meta_ids: list[dict]\n    ) -&gt; Optional[str]:\n        \"\"\"\n        Build search expression for embedding collection. The generated\n        expression follows the pattern: \"meta_id in [{id1}, {id2}, ...]\n\n        Args:\n            meta_ids: the array of metadata id in metadata collection\n\n        Returns:\n            str: search expression or None\n        \"\"\"\n        if len(meta_ids) == 0:\n            return \"meta_id in []\"\n        built_expr = \"\"\"meta_id in [\"\"\"\n        for item in meta_ids:\n            id = f'\"{item[\"id\"]}\",'\n            built_expr += id\n        built_expr = built_expr[:-1]\n        built_expr += \"\"\"]\"\"\"\n        return built_expr\n\n    def _join_embedding_and_metadata_results(\n        self, result_embedding: list[Document], result_meta: list[dict]\n    ) -&gt; list[Document]:\n        \"\"\"\n        Join the search results of embedding collection and results of metadata.\n\n        Args:\n            result_embedding (List[Document]): search result of embedding\n                collection\n\n            result_meta (List[Dict]): search result of metadata collection\n\n        Returns:\n            List[Document]: combined results like\n                [{page_content: str, metadata: {...}}]\n        \"\"\"\n\n        def _find_metadata_by_id(\n            metadata: list[dict], id: str\n        ) -&gt; Optional[dict]:\n            for d in metadata:\n                if str(d[\"id\"]) == id:\n                    return d\n            return None\n\n        joined_docs = []\n        for res in result_embedding:\n            found = _find_metadata_by_id(result_meta, res.metadata[\"meta_id\"])\n            if found is None:  # discard\n                logger.error(\n                    f\"Failed to join meta_id {res.metadata['meta_id']}\"\n                )\n                continue\n            joined_docs.append(\n                Document(page_content=res.page_content, metadata=found)\n            )\n        return joined_docs\n\n    @staticmethod\n    def _build_meta_col_query_expr_for_all_documents(\n        doc_ids: Optional[list[str]] = None,\n    ) -&gt; str:\n        \"\"\"\n        Build metadata collection query expression to obtain all documents.\n\n        Args:\n            doc_ids: the list of document ids (metadata ids), if thie argument is None,\n                     that is, the query is to get all undeleted documents in metadata collection.\n                     Otherwise, the query is to getr all undeleted documents form provided doc_ids\n\n        Returns:\n            query: str\n        \"\"\"\n        expr = (\n            f\"id in {doc_ids} and isDeleted == false\"\n            if doc_ids is not None\n            else \"isDeleted == false\"\n        )\n        return expr.replace('\"', \"\").replace(\"'\", \"\")\n\n    def similarity_search(\n        self, query: str, k: int = 3, doc_ids: list[str] = None\n    ) -&gt; list[Document]:\n        \"\"\"\n        Perform similarity search insider the currently active database\n        according to the input query.\n\n        This method will:\n        1. get all non-deleted meta_id and build to search expression for\n            the currently active embedding collection\n        2. do similarity search in the embedding collection\n        3. combine metadata and embeddings\n\n        Args:\n            query (str): query string\n\n            k (int): the number of results to return\n\n            doc_ids(List[str] optional): the list of document ids, do similarity search across the\n            specified documents\n\n        Returns:\n            List[Document]: search results\n        \"\"\"\n        result_metadata = []\n        expr = VectorDatabaseAgentMilvus._build_meta_col_query_expr_for_all_documents(\n            doc_ids\n        )\n        result_metadata = self._col_metadata.query(\n            expr=expr, output_fields=METADATA_FIELDS\n        )\n        expr = self._build_embedding_search_expression(result_metadata)\n        result_embedding = self._col_embeddings.similarity_search(\n            query=query, k=k, expr=expr\n        )\n        return self._join_embedding_and_metadata_results(\n            result_embedding, result_metadata\n        )\n\n    def remove_document(\n        self, doc_id: str, doc_ids: Optional[list[str]] = None\n    ) -&gt; bool:\n        \"\"\"\n        Remove the document include meta data and its embeddings.\n\n        Args:\n            doc_id (str): the document to be deleted\n\n            doc_ids(List[str] optional): the list of document ids, defines documents scope\n            within which remove operation occurs.\n\n        Returns:\n            bool: True if the document is deleted, False otherwise\n        \"\"\"\n        if not self._col_metadata:\n            return False\n        if doc_ids is not None and (\n            len(doc_ids) == 0 or (len(doc_ids) &gt; 0 and not doc_id in doc_ids)\n        ):\n            return False\n        try:\n            expr = f\"id in [{doc_id}]\"\n            res = self._col_metadata.query(\n                expr=expr, output_fields=METADATA_FIELDS\n            )\n            if len(res) == 0:\n                return False\n            del_res = self._col_metadata.delete(expr)\n            self._col_metadata.flush()\n\n            res = self._col_embeddings.col.query(f'meta_id in [\"{doc_id}\"]')\n            if len(res) == 0:\n                return True\n            ids = [item[\"pk\"] for item in res]\n            embedding_expr = f\"pk in {ids}\"\n            del_res = self._col_embeddings.col.delete(expr=embedding_expr)\n            self._col_embeddings.col.flush()\n            return True\n        except MilvusException as e:\n            logger.error(e)\n            raise e\n\n    def get_all_documents(\n        self, doc_ids: Optional[list[str]] = None\n    ) -&gt; list[dict]:\n        \"\"\"\n        Get all non-deleted documents from the currently active database.\n\n        Args:\n            doc_ids(List[str] optional): the list of document ids, defines documents scope within\n            which the operation of obaining all documents occurs\n\n        Returns:\n            List[Dict]: the metadata of all non-deleted documents in the form\n                [{{id}, {author}, {source}, ...}]\n        \"\"\"\n        try:\n            expr = VectorDatabaseAgentMilvus._build_meta_col_query_expr_for_all_documents(\n                doc_ids\n            )\n            result_metadata = self._col_metadata.query(\n                expr=expr, output_fields=METADATA_FIELDS\n            )\n            return result_metadata\n        except MilvusException as e:\n            logger.error(e)\n            raise e\n</code></pre>"},{"location":"vectorstore_agent-reference/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus.__init__","title":"<code>__init__(embedding_func, connection_args=None, embedding_collection_name=None, metadata_collection_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>embedding_func</code> <code>OpenAIEmbeddings</code> <p>Function used to embed the text</p> required <code>connection_args</code> <code>Optional dict</code> <p>args to connect Vector Database</p> <code>None</code> <code>embedding_collection_name</code> <code>Optional str</code> <p>exposed for test</p> <code>None</code> <code>metadata_collection_name</code> <code>Optional str</code> <p>exposed for test</p> <code>None</code> Source code in <code>biochatter/vectorstore_agent.py</code> <pre><code>def __init__(\n    self,\n    embedding_func: OpenAIEmbeddings,\n    connection_args: Optional[dict] = None,\n    embedding_collection_name: Optional[str] = None,\n    metadata_collection_name: Optional[str] = None,\n):\n    \"\"\"\n    Args:\n        embedding_func OpenAIEmbeddings: Function used to embed the text\n\n        connection_args Optional dict: args to connect Vector Database\n\n        embedding_collection_name Optional str: exposed for test\n\n        metadata_collection_name Optional str: exposed for test\n    \"\"\"\n    self._embedding_func = embedding_func\n    self._col_embeddings: Optional[Milvus] = None\n    self._col_metadata: Optional[Collection] = None\n    self._connection_args = validate_connection_args(connection_args)\n    self._embedding_name = (\n        embedding_collection_name or DOCUMENT_EMBEDDINGS_COLLECTION_NAME\n    )\n    self._metadata_name = (\n        metadata_collection_name or DOCUMENT_METADATA_COLLECTION_NAME\n    )\n</code></pre>"},{"location":"vectorstore_agent-reference/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus.connect","title":"<code>connect()</code>","text":"<p>Connect to a host and read two document collections (the default names are <code>DocumentEmbeddings</code> and <code>DocumentMetadata</code>) in the currently active database (default database name is <code>default</code>); if those document collections don't exist, create the two collections.</p> Source code in <code>biochatter/vectorstore_agent.py</code> <pre><code>def connect(self) -&gt; None:\n    \"\"\"\n    Connect to a host and read two document collections (the default names\n    are `DocumentEmbeddings` and `DocumentMetadata`) in the currently active\n    database (default database name is `default`); if those document\n    collections don't exist, create the two collections.\n    \"\"\"\n    self._connect(**self._connection_args)\n    self._init_host()\n</code></pre>"},{"location":"vectorstore_agent-reference/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus.get_all_documents","title":"<code>get_all_documents(doc_ids=None)</code>","text":"<p>Get all non-deleted documents from the currently active database.</p> <p>Parameters:</p> Name Type Description Default <code>doc_ids(List[str]</code> <code>optional</code> <p>the list of document ids, defines documents scope within</p> required <p>Returns:</p> Type Description <code>list[dict]</code> <p>List[Dict]: the metadata of all non-deleted documents in the form [{{id}, {author}, {source}, ...}]</p> Source code in <code>biochatter/vectorstore_agent.py</code> <pre><code>def get_all_documents(\n    self, doc_ids: Optional[list[str]] = None\n) -&gt; list[dict]:\n    \"\"\"\n    Get all non-deleted documents from the currently active database.\n\n    Args:\n        doc_ids(List[str] optional): the list of document ids, defines documents scope within\n        which the operation of obaining all documents occurs\n\n    Returns:\n        List[Dict]: the metadata of all non-deleted documents in the form\n            [{{id}, {author}, {source}, ...}]\n    \"\"\"\n    try:\n        expr = VectorDatabaseAgentMilvus._build_meta_col_query_expr_for_all_documents(\n            doc_ids\n        )\n        result_metadata = self._col_metadata.query(\n            expr=expr, output_fields=METADATA_FIELDS\n        )\n        return result_metadata\n    except MilvusException as e:\n        logger.error(e)\n        raise e\n</code></pre>"},{"location":"vectorstore_agent-reference/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus.remove_document","title":"<code>remove_document(doc_id, doc_ids=None)</code>","text":"<p>Remove the document include meta data and its embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>doc_id</code> <code>str</code> <p>the document to be deleted</p> required <code>doc_ids(List[str]</code> <code>optional</code> <p>the list of document ids, defines documents scope</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the document is deleted, False otherwise</p> Source code in <code>biochatter/vectorstore_agent.py</code> <pre><code>def remove_document(\n    self, doc_id: str, doc_ids: Optional[list[str]] = None\n) -&gt; bool:\n    \"\"\"\n    Remove the document include meta data and its embeddings.\n\n    Args:\n        doc_id (str): the document to be deleted\n\n        doc_ids(List[str] optional): the list of document ids, defines documents scope\n        within which remove operation occurs.\n\n    Returns:\n        bool: True if the document is deleted, False otherwise\n    \"\"\"\n    if not self._col_metadata:\n        return False\n    if doc_ids is not None and (\n        len(doc_ids) == 0 or (len(doc_ids) &gt; 0 and not doc_id in doc_ids)\n    ):\n        return False\n    try:\n        expr = f\"id in [{doc_id}]\"\n        res = self._col_metadata.query(\n            expr=expr, output_fields=METADATA_FIELDS\n        )\n        if len(res) == 0:\n            return False\n        del_res = self._col_metadata.delete(expr)\n        self._col_metadata.flush()\n\n        res = self._col_embeddings.col.query(f'meta_id in [\"{doc_id}\"]')\n        if len(res) == 0:\n            return True\n        ids = [item[\"pk\"] for item in res]\n        embedding_expr = f\"pk in {ids}\"\n        del_res = self._col_embeddings.col.delete(expr=embedding_expr)\n        self._col_embeddings.col.flush()\n        return True\n    except MilvusException as e:\n        logger.error(e)\n        raise e\n</code></pre>"},{"location":"vectorstore_agent-reference/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus.similarity_search","title":"<code>similarity_search(query, k=3, doc_ids=None)</code>","text":"<p>Perform similarity search insider the currently active database according to the input query.</p> <p>This method will: 1. get all non-deleted meta_id and build to search expression for     the currently active embedding collection 2. do similarity search in the embedding collection 3. combine metadata and embeddings</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>query string</p> required <code>k</code> <code>int</code> <p>the number of results to return</p> <code>3</code> <code>doc_ids(List[str]</code> <code>optional</code> <p>the list of document ids, do similarity search across the</p> required <p>Returns:</p> Type Description <code>list[Document]</code> <p>List[Document]: search results</p> Source code in <code>biochatter/vectorstore_agent.py</code> <pre><code>def similarity_search(\n    self, query: str, k: int = 3, doc_ids: list[str] = None\n) -&gt; list[Document]:\n    \"\"\"\n    Perform similarity search insider the currently active database\n    according to the input query.\n\n    This method will:\n    1. get all non-deleted meta_id and build to search expression for\n        the currently active embedding collection\n    2. do similarity search in the embedding collection\n    3. combine metadata and embeddings\n\n    Args:\n        query (str): query string\n\n        k (int): the number of results to return\n\n        doc_ids(List[str] optional): the list of document ids, do similarity search across the\n        specified documents\n\n    Returns:\n        List[Document]: search results\n    \"\"\"\n    result_metadata = []\n    expr = VectorDatabaseAgentMilvus._build_meta_col_query_expr_for_all_documents(\n        doc_ids\n    )\n    result_metadata = self._col_metadata.query(\n        expr=expr, output_fields=METADATA_FIELDS\n    )\n    expr = self._build_embedding_search_expression(result_metadata)\n    result_embedding = self._col_embeddings.similarity_search(\n        query=query, k=k, expr=expr\n    )\n    return self._join_embedding_and_metadata_results(\n        result_embedding, result_metadata\n    )\n</code></pre>"},{"location":"vectorstore_agent-reference/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus.store_embeddings","title":"<code>store_embeddings(documents)</code>","text":"<p>Store documents in the currently active database.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Documents]</code> <p>documents array, usually from DocumentReader.load_document, DocumentReader.document_from_pdf, DocumentReader.document_from_txt</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>document id</p> Source code in <code>biochatter/vectorstore_agent.py</code> <pre><code>def store_embeddings(self, documents: list[Document]) -&gt; str:\n    \"\"\"\n    Store documents in the currently active database.\n\n    Args:\n        documents (List[Documents]): documents array, usually from\n            DocumentReader.load_document, DocumentReader.document_from_pdf,\n            DocumentReader.document_from_txt\n\n    Returns:\n        str: document id\n    \"\"\"\n    if len(documents) == 0:\n        return\n    return self._insert_data(documents)\n</code></pre>"},{"location":"vectorstore_agent-reference/#biochatter.vectorstore_agent.align_embeddings","title":"<code>align_embeddings(docs, meta_id)</code>","text":"<p>Ensure that the metadata id is present in each document.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>List of documents</p> required <code>meta_id</code> <code>int</code> <p>Metadata id to assign to the documents</p> required <p>Returns:</p> Type Description <code>list[Document]</code> <p>List[Document]: List of documents, with each document having a metadata id.</p> Source code in <code>biochatter/vectorstore_agent.py</code> <pre><code>def align_embeddings(docs: list[Document], meta_id: int) -&gt; list[Document]:\n    \"\"\"\n    Ensure that the metadata id is present in each document.\n\n    Args:\n        docs (List[Document]): List of documents\n\n        meta_id (int): Metadata id to assign to the documents\n\n    Returns:\n        List[Document]: List of documents, with each document having a metadata\n            id.\n    \"\"\"\n    ret = []\n    for doc in docs:\n        ret.append(\n            Document(\n                page_content=doc.page_content,\n                metadata={\"meta_id\": meta_id},\n            )\n        )\n    return ret\n</code></pre>"},{"location":"vectorstore_agent-reference/#biochatter.vectorstore_agent.align_metadata","title":"<code>align_metadata(metadata, isDeleted=False)</code>","text":"<p>Ensure that specific metadata fields are present; if not provided, fill with \"unknown\". Also, add a random vector to each metadata item to simulate an embedding.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>List[Dict]</code> <p>List of metadata items</p> required <code>isDeleted</code> <code>Optional[bool]</code> <p>Whether the document is deleted. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[list]</code> <p>List[List]: List of metadata items, with each item being a list of metadata fields.</p> Source code in <code>biochatter/vectorstore_agent.py</code> <pre><code>def align_metadata(\n    metadata: list[dict], isDeleted: Optional[bool] = False\n) -&gt; list[list]:\n    \"\"\"\n\n    Ensure that specific metadata fields are present; if not provided, fill with\n    \"unknown\". Also, add a random vector to each metadata item to simulate an\n    embedding.\n\n    Args:\n        metadata (List[Dict]): List of metadata items\n\n        isDeleted (Optional[bool], optional): Whether the document is deleted.\n            Defaults to False.\n\n    Returns:\n        List[List]: List of metadata items, with each item being a list of\n            metadata fields.\n    \"\"\"\n    ret = []\n    fields = METADATA_FIELDS.copy()\n    fields.pop(0)\n    for ix, k in enumerate(fields):\n        ret.append([item[k] if k in item else \"unknown\" for item in metadata])\n\n    ret.append(\n        [\n            [random.random() for _ in range(METADATA_VECTOR_DIM)]\n            for _ in range(len(metadata))\n        ]\n    )\n    ret.append([isDeleted for _ in metadata])\n    return ret\n</code></pre>"},{"location":"vignette-custom-bclight-advanced/","title":"Custom BioChatter Light: Advanced Use Case","text":"<p>For more advanced workflows, you may want to customise the BioChatter Light interface to display a new way of visualising your data or interacting with it. Due to the flexible, modular, and easy-to-use Streamlit framework, you can easily create new tabs and customise the existing ones. Here, we show how to create new tabs on the example of a project management tool we developed as a demonstration use case. The final web app is available at https://project.biochatter.org.</p>"},{"location":"vignette-custom-bclight-advanced/#background","title":"Background","text":"<p>Managing a scientific group is challenging for multiple reasons, particularly one which has multiple interdependent projects, each of which is pursued by a small team of junior and senior researchers. To enhance productivity and communication, it could be beneficial to have a tool that takes away some of the burden of project management, to increase the available \"thinking time\" for the scientists (for further reading, see for example this article). In the context of our work, there are two components we see as essential: data management (FAIR and transparent) and simple interfaces (driven by conversational AI). We will be using BioCypher and BioChatter for these two components.</p> <p>We will use a GitHub Project board (here) as the \"ground truth\" for our project management tool. This is close to a real-world scenario and allows connectivity to code repositories, issues, pull requests, and other components of computational collaboration. The linked project is \"synthetic\" data for demonstration purposes. The repository to build the KG and deploy the BioChatter Light web app is available here.</p>"},{"location":"vignette-custom-bclight-advanced/#build-the-kg","title":"Build the KG","text":"<p>We modified an existing adapter for the GitHub GraphQL API to pull data from the GitHub Project board. Thus, the time investment to build the KG was minimal (~3h); this is one central principle of BioCypher. We adapted the code (<code>project_planning/adapters/github_adapter.py</code>) and KG schema (<code>config/schema_config.yaml</code>) to represent the relevant features of the GitHub Project board. The pre-existing KG build and deploy scripts were used via the <code>docker-compose.yml</code> file. For public deployment, we also added a <code>docker-compose-password.yml</code>, which builds a password-protected version of the KG. Deployment and setup of the cloud VM took another ~2h.</p> <p>Be aware that running this script will require a GitHub token with access to the project board. This token should be stored in the environment variable <code>BIOCYPHER_GITHUB_PROJECT_TOKEN</code>.</p>"},{"location":"vignette-custom-bclight-advanced/#add-the-additional-tabs-to-biochatter-light","title":"Add the additional tabs to BioChatter Light","text":"<p>BioChatter Light has a modular architecture to accommodate flexible layout changes. We also added a configuration that allows turning on or off specific tabs via environment variables. For this project, we added three new tabs: \"Summary\", \"Tasks\", and \"Settings\". The \"Summary\" tab shows an overview of the completed tasks in the current iteration of the project, the \"Tasks\" tab shows the upcoming tasks of the group and each team member, and the \"Settings\" tab allows configuration of the queries and LLM instructions used to generate the content for the other tabs.</p> <p>The solution as a web app is only an illustration of the project management tool; for example, we'd envision the deployed version rather as an integration of common messengers (Zulip, Slack, etc.) that acts as a conversational assistant to the group and its members. The web app is a proof of concept and demonstration of the capabilities of BioChatter Light, simulating feedback to the group and individual users via the simplified interface.</p> <p>The tabs were added to the BioChatter Light codebase in the corresponding module (<code>components/panels/project.py</code>), which contains all three tabs. The Streamlit framework makes this relatively easy; each tab only requires about 100 lines of code in this module and only contains simple components such as columns, buttons, and text fields. We also added environment variables to the configuration (<code>components/config.py</code>) to allow turning on or off the new tabs.</p>"},{"location":"vignette-custom-bclight-advanced/#configure-the-biochatter-light-docker-container","title":"Configure the BioChatter Light Docker container","text":"<p>As in the previous vignette, we can now configure the BioChatter Light Docker container to show only the new tabs. We provide these settings via the environment variables we introduced above, while turning off the default tabs. We have also added configurable environment variables for setting a page title, header, and subheader for the web app without having to change the source code.</p> <pre><code>services:\n  ## ... build, import, and deploy the KG ...\n  app:\n    image: biocypher/biochatter-light:0.6.10\n    container_name: app\n    ports:\n      - \"8501:8501\"\n    networks:\n      - biochatter\n    depends_on:\n      import:\n        condition: service_completed_successfully\n    environment:\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n      - BIOCHATTER_LIGHT_TITLE=Project Planner\n      - BIOCHATTER_LIGHT_HEADER=GitHub Project Planning Assistant\n      - BIOCHATTER_LIGHT_SUBHEADER=A BioChatter Demonstration App for integrated project planning using LLMs\n      - DOCKER_COMPOSE=true\n      - CHAT_TAB=false\n      - PROMPT_ENGINEERING_TAB=false\n      - RAG_TAB=false\n      - CORRECTING_AGENT_TAB=false\n      - KNOWLEDGE_GRAPH_TAB=false\n      - LAST_WEEKS_SUMMARY_TAB=true\n      - THIS_WEEKS_TASKS_TAB=true\n      - TASK_SETTINGS_PANEL_TAB=true\n</code></pre> <p>You can see the full configuration in the <code>docker-compose.yml</code> file of the project-planning repository. For public deployment, we also added a password-protected version of the KG, which only requires a few additional lines in the <code>docker-compose-password.yml</code> file. To deploy the tool on a cloud VM, we now only need to run the following commands:</p> <pre><code>git clone https://github.com/biocypher/project-planning.git\ndocker-compose -f project-planning/docker-compose-password.yml up -d\n</code></pre> <p>We just need to make sure to provide an <code>OPENAI_API_KEY</code> and a <code>BIOCYPHER_GITHUB_PROJECT_TOKEN</code> in the VM's environment to be accessed by the Docker workflow.</p>"},{"location":"vignette-custom-bclight-advanced/#useful-tips-for-deployment","title":"Useful tips for deployment","text":"<p>Many vendors offer cloud VMs with pre-installed Docker and Docker Compose, as well as Nginx for reverse proxying. We recommend using a reverse proxy to provide HTTPS and a domain name for the web app. This can be done with a few lines in the Nginx configuration file. For example, to deploy the project management tool on a cloud VM with a domain name <code>project.biochatter.org</code>, you can use the following Nginx configuration:</p> <pre><code>server {\n    listen 80;\n    server_name project.biochatter.org;\n\n    location / {\n        proxy_pass http://localhost:8501;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n</code></pre> <p>You can find more info here. We also recommend to set up certification with Let's Encrypt for HTTPS. This can be done with the Certbot tool, which is available for most Linux distributions.</p> <p>In addition, you need to make sure that your Neo4j deployment is accessible from your web app, and that the connection is secure. You can either make the DB accessible only on the VM's network, which would allow running it without encryption, or you can set up a secure connection with a password. Both options are implemented in the <code>docker-compose.yml</code> and <code>docker-compose-password.yml</code> files of the <code>project-planning</code> repository. You can find more info here.</p>"},{"location":"vignette-custom-bclight-advanced/#conclusion","title":"Conclusion","text":"<p>This vignette showcases the creation of a custom BioChatter Light web app for a dedicated purpose, in this case, project management. The app is a demonstration of the flexibility and ease of use of the BioChatter Light framework, which allows for the rapid development of conversational AI interfaces for various applications. The project management tool is a proof of concept and will be further developed into a conversational assistant that can not only summarise, but interact with the group members, and provide administrative support for larger groups and even organisations.</p> <p>The capabilities of GitHub Projects and their API allow the transfer of issues between boards, which allows for a multi-level approach to project management. Higher-level master boards can collect the tasks and issues of a larger group, and the project management assistant can help in collating those into manageable chunks for smaller teams (such as the board of our synthetic project). The same abstraction can be used at the organisation level, where the aims and challenges of the organisation are broken down into projects and tasks for larger groups.</p>"},{"location":"vignette-custom-bclight-simple/","title":"Custom BioChatter Light: Simple Use Case","text":"<p>For prototyping your own text2cypher workflow, it can be useful to have a simple interface for troubleshooting your queries. This vignette demonstrates how to customise the pre-built BioChatter Light Docker compose to turn off all tabs except the Knowledge Graph (KG) tab. This allows the deployment of an integrated KG build, deployment, and web app for LLM-based question answering.</p>"},{"location":"vignette-custom-bclight-simple/#build-your-kg","title":"Build your KG","text":"<p>First, build your KG. For this example, we use the BioCypher Pole KG as a demo KG. The KG is based on an open-source dataset of crime statistics in Manchester. The schema of the demo KG is described in the Knowledge Graph RAG vignette. For building your own KG, refer to the BioCypher documentation.</p> <p>This KG is built, imported, and deployed in the first three stages of the <code>docker-compose.yml</code> file.</p>"},{"location":"vignette-custom-bclight-simple/#configure-the-biochatter-light-docker-container","title":"Configure the BioChatter Light Docker container","text":"<p>We provide a simple way to customise the BioChatter Light Docker container to show only select components. We can provide these settings via environment variables, so in the case of running from <code>docker-compose.yml</code>, we can set these in the <code>environment</code> section of the <code>app</code> service.</p> <pre><code>services:\n  ## ... build, import, and deploy the KG ...\n  app:\n    image: biocypher/biochatter-light:0.6.10\n    container_name: app\n    ports:\n      - \"8501:8501\"\n    networks:\n      - biochatter\n    depends_on:\n      import:\n        condition: service_completed_successfully\n    environment:\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n      - DOCKER_COMPOSE=true\n      - CHAT_TAB=false\n      - PROMPT_ENGINEERING_TAB=false\n      - RAG_TAB=false\n      - CORRECTING_AGENT_TAB=false\n      - KNOWLEDGE_GRAPH_TAB=true\n</code></pre> <p>In this example, we provide our OpenAI API key and set the <code>DOCKER_COMPOSE</code> flag to <code>true</code>, which tells BioChatter Light to connect to the KG on the Docker network, which uses the service name as the hostname, so in this case, <code>deploy</code> instead of the default <code>localhost</code>.</p> <p>We then turn off all default tabs (chatting, prompt engineering, RAG, and the correcting agent) and turn on the KG tab. Running the docker compose with these settings will build and deploy the KG and the BioChatter Light web app with only the KG tab enabled.</p> <pre><code>git clone https://github.com/biocypher/pole\ncd pole\ndocker-compose up -d\n</code></pre> <p>We are constantly expanding our repertoire of BioChatter Light tabs, so check back for more options in the future. Creating your own tabs is also accessible via our modular architecture and the simple Streamlit framework for UI design. Check the advanced vignette and the codebase of BioChatter Light for more information.</p>"},{"location":"vignette-kg/","title":"Vignette: Knowledge Graph RAG","text":"<p>This vignette demonstrates the KG module of BioChatter as used by the BioChatter Next application. We connect to a BioCypher knowledge graph (KG) to retrieve relevant information for a given question. We then use the retrieved information to generate a response to the question. The application can connect to any real-world BioCypher KG by providing the connection details in the <code>KG Settings</code> dialog.</p>"},{"location":"vignette-kg/#background","title":"Background","text":"<p>For the demonstration purposes of this vignette, we include a demo KG based on an open-source dataset of crime statistics in Manchester, because it allows us to redistribute the KG due to its small size and public domain licence, and because it is easily understood. This is non-trivial for most biomedical datasets; however, we are currently working on a synthetic biomedical example to extend this vignette. This is the schema of the demo KG:</p> <pre><code>graph LR;\n    Person(:Person) -- KNOWS --&gt; Person\n    Person -- FAMILY_REL --&gt; Person\n    Person -- LIVES_AT --&gt; Location(:Location)\n    Person -- PARTY_TO --&gt; Crime(:Crime)\n    Person -- MADE_CALL --&gt; PhoneCall(:PhoneCall)\n    Person -- RECEIVED_CALL --&gt; PhoneCall\n    Crime -- INVESTIGATED_BY --&gt; Officer(:Officer)\n    Crime -- OCCURRED_AT --&gt; Location\n    Object(:Object) -- INVOLVED_IN --&gt; Crime</code></pre> <p>The KG is adapted from a Neo4j tutorial, and is available as a BioCypher adapter including a BioChatter Light integration here. We also include it in an optional BioChatter Next Docker Compose configuration to allow trying it out locally.</p>"},{"location":"vignette-kg/#usage","title":"Usage","text":"<p>In BioChatter Next, we first activate the KG functionality by clicking on the <code>KG Settings</code> button in the sidebar. In the settings dialog, we can activate the KG functionality and select how many results we want to retrieve. Returning to the conversation and enabling the KG functionality for the current chat (directly above the send button), we can then ask the model about the KG. The language model we use in this vignette is, as in the RAG vignette, <code>gpt-3.5-turbo-0613</code> (deprecated as of July 16 2024). The conversation is pasted below for convenience, including the queries generated by BioChatter.</p> <p></p> <p>In the background, the RagAgent module of BioChatter receives the question and generates a query to retrieve the desired information. This is then passed back to the primary model, which includes it in its answer generation.</p>"},{"location":"vignette-kg/#conclusion","title":"Conclusion","text":"<p>The native integration of BioCypher KGs into the BioChatter framework allows for a seamless integration of KGs into the conversational AI. This in turn facilitates knowledge accessibility in a wide range of application domains.</p>"},{"location":"vignette-kg/#note-the-apparent-inability-of-gpt-to-understand-certain-directionalities-and-how-biochatter-compensates-for-this","title":"Note: the apparent inability of GPT to understand certain directionalities, and how BioChatter compensates for this","text":"<p>Interestingly, while <code>gpt-3.5-turbo-0613</code> mostly does a formidable job at translating natural language questions into Cypher queries, it is remarkably obtuse in certain instances. For instance, for the relationship <code>INVESTIGATED_BY</code>, which connects a <code>Crime</code> to an <code>Officer</code>, GPT consistently fails to understand that the relationship implies that the <code>Officer</code> is the one who investigates the <code>Crime</code>. Instead, it consistently interprets the relationship as if the <code>Crime</code> investigates the <code>Officer</code>: it consistently proposes the query <code>MATCH (o:Officer)-[:INVESTIGATED_BY]-&gt;(c:Crime) RETURN c, o</code> instead of the correct <code>MATCH (c:Crime)-[:INVESTIGATED_BY]-&gt;(o:Officer) RETURN c, o</code>. We were not able to change this behaviour with any contextual prompt instructions.</p> <p>For this reason, the BioChatter <code>prompts.py</code> module uses the knowledge we have about the directionality of edges in the BioCypher KG to only propose options for patterns that actually exist in the KG. In the instance of <code>INVESTIGATED_BY</code>, this is the corresponding YAML definition in BioCypher:</p> <pre><code>investigated by:\n    is_a: [fact, core]\n    represented_as: edge\n    label_as_edge: INVESTIGATED_BY\n    input_label: INVESTIGATED_BY\n    source: crime\n    target: officer\n</code></pre> <p>The presence of the <code>source</code> and <code>target</code> annotations allows us to provide only the correct options to the LLM, which in turn allows the LLM to generate the correct query.</p> <p>Strikingly, this even applies to <code>GPT-4</code> and <code>ChatGPT</code>, which we used to generate the Mermaid diagram above. The model immediately proposed a correct Mermaid diagram for most of the entire schema definition YAML file, except for the relationship between <code>Crime</code> and <code>Officer</code>, which it reversed. Via the YAML we provide in the prompt, GPT has access to the correct directionality, but it still fails to understand it.</p> <pre><code>graph LR;\n    Person(:Person) -- KNOWS --&gt; Person\n    Person -- FAMILY_REL --&gt; Person\n    Person -- LIVES_AT --&gt; Location(:Location)\n    Person -- PARTY_TO --&gt; Crime(:Crime)\n    Person -- MADE_CALL --&gt; PhoneCall(:PhoneCall)\n    Person -- RECEIVED_CALL --&gt; PhoneCall\n    Officer(:Officer) -- INVESTIGATED_BY --&gt; Crime\n    Crime -- OCCURRED_AT --&gt; Location\n    Object(:Object) -- INVOLVED_IN --&gt; Crime\n\n    classDef node fill:#f9f,stroke:#333,stroke-width:2px;\n    class Person,Officer,Location,Crime,PhoneCall,Object node;\n</code></pre> <p>Here is the full chat for reference.</p>"},{"location":"vignette-kg/#conversation","title":"Conversation","text":"<p>The boxes below the BioChatter responses in the conversation show the queries generated by the RagAgent module.</p>"},{"location":"vignette-kg/#message-from-you","title":"Message From You","text":"<p>How many crimes happened in total?</p>"},{"location":"vignette-kg/#message-from-biochatter","title":"Message From BioChatter","text":"<p>According to the provided statement, the total number of crimes is 28,762. This information suggests that there have been 28,762 reported crimes in the given context. However, without additional information or context, it is difficult to determine the specific time period, location, or nature of these crimes. It's important to note that crime statistics can vary depending on the source, methodology, and reporting practices. Therefore, it is always advisable to consult official crime reports or local law enforcement agencies for the most accurate and up-to-date information on crime statistics.</p> <pre><code>MATCH (c:Crime)\nRETURN count(c) AS numberOfCrimes\n</code></pre>"},{"location":"vignette-kg/#message-from-you_1","title":"Message From You","text":"<p>Where happened most crimes?</p>"},{"location":"vignette-kg/#message-from-biochatter_1","title":"Message From BioChatter","text":"<p>Based on the provided statement, it appears that the location with the highest number of crimes is a parking area, with a total of 811 reported crimes. However, without additional context or information, it is difficult to determine the consistency or inconsistency of this statement with other available information. It is important to note that crime rates and locations can vary significantly depending on various factors such as the region, city, neighborhood, and specific time period being considered. To provide a more comprehensive analysis, it would be helpful to have access to a broader dataset or additional information about the specific location and its crime statistics.</p> <pre><code>MATCH (c:Crime)-[:OCCURRED_AT]-&gt;(l:Location)\nRETURN l.address AS location, COUNT(c) AS crimeCount\nORDER BY crimeCount DESC\n</code></pre>"},{"location":"vignette-kg/#message-from-you_2","title":"Message From You","text":"<p>Who were the top 3 offenders?</p>"},{"location":"vignette-kg/#message-from-biochatter_2","title":"Message From BioChatter","text":"<p>Based on the provided information, the top three offenders are:</p> <ol> <li> <p>Phillip Williamson: According to the information given, Phillip Williamson has committed 5 crimes.</p> </li> <li> <p>Jessica Kelly: The information states that Jessica Kelly has also committed 5 crimes.</p> </li> <li> <p>Brian Morales: The data indicates that Brian Morales has committed 4 crimes.</p> </li> </ol> <p>These three individuals, Phillip Williamson, Jessica Kelly, and Brian Morales, are the top offenders based on the number of crimes they have committed.</p> <pre><code>MATCH (p:Person)-[:PARTY_TO]-&gt;(c:Crime)\nRETURN p.name, p.surname, COUNT(c) AS num_crimes\nORDER BY num_crimes DESC\nLIMIT 3\n</code></pre>"},{"location":"vignette-rag/","title":"Vignette: Retrieval-Augmented Generation (RAG)","text":"<p>This vignette demonstrates the RAG module of BioChatter as used by the BioChatter Next application. This basic use case involves an LLM manuscript reading assistant and a vector database with embedded scientific manuscripts. The manuscripts are recent reviews on the topic of ABC transporters (active transport proteins that often serve as efflux pumps in many tissues, with a major function of clearing exogenous substances) in drug resistance phenomena, which we currently investigate in our DECIDER cohort.  Manuscripts such as these can be absent from the knowledge base of LLMs for various reasons, such as their recency or the fact that they are not open access. RAG opens up the possibility to retrieve relevant information from these manuscripts, and to inject it into the LLM's generation process.</p>"},{"location":"vignette-rag/#usage","title":"Usage","text":"<p>In BioChatter Next, we first activate the RAG functionality by clicking on the <code>RAG Settings</code> button in the sidebar. In the settings dialog, we can activate the functionality and upload an arbitrary number of documents, which is only limited by the scale of the vector database system. In this case, and for demonstration purposes, we uploaded the four manuscripts, which leads to the state shown in the screenshot below. You can find the citations below.</p> <p></p> <p>Note that we chose to split the documents into fragments of 1000 characters each. We could have split by tokens instead, or changed the fragment length and overlap. Fragment length, overlap, and splitting by tokens or characters are decisions that should be made in tandem with the choice of LLM model, and with the number of fragments that should be retrieved for each query. Most importantly, the total text length should not exceed the input context length of the model. Here, we choose to inject 10 fragments per query.</p> <p>We can now start a new conversation (using the <code>New Persona</code> button in the sidebar), for which we select the <code>Research manuscript helper (RAG)</code> persona, which includes suitable contextual prompts. Upon activating the RAG in the chat (to signal that we wish to perform RAG for our question), we can enter into a dialogue with the assistant. We use <code>gpt-3.5-turbo-0613</code> in this example (deprecated as of July 16 2024), which is a very affordable model. The procedure is demonstrated in the GIF below.</p> <p></p>"},{"location":"vignette-rag/#comparison-with-chatgpt","title":"Comparison with ChatGPT","text":"<p>We can ask the same question to ChatGPT 4 (only subscription access).  By employing web search, ChatGPT 4 is able to find the same study that was the RAG result we were asked about in our follow-up question (Xu et al. 2015); however, the answer is not as detailed as the answer provided by the RAG process, and includes fewer mentions of explicit studies. While it includes sensible information, there is no way of validating most of the claims made by the model. You can read the full answer here. Overall, the quality of ChatGPT 4 is comparable to the RAG output, but the user has less control over the knowledge used, and the answer cannot include information that is not easily found on the web (for instance due to access restrictions). Of note, we used <code>gpt-3.5-turbo-0613</code> in this example, which is less recent and much less expensive than GPT-4.</p> <p>Asking the naive ChatGPT 3.5 (free access), the same question yields a very different answer. The free version of ChatGPT cannot search the internet, and does not provide any citations for the general answer it provides. You can read the full answer here. For convenience, all three conversations are copied below.</p>"},{"location":"vignette-rag/#conclusion","title":"Conclusion","text":"<p>In conclusion, the RAG procedure we implement is able to dramatically improve the quality of GPT-3 answers, making them comparable to GPT-4 with web search, and even surpassing it in some aspects. We invite you to try it out yourself, as we provide an instance of BioChatter Next, including GPT access and a vector database instance, free of charge at https://next.biochatter.org.</p>"},{"location":"vignette-rag/#conversations","title":"Conversations","text":""},{"location":"vignette-rag/#biochatter-rag","title":"BioChatter RAG","text":""},{"location":"vignette-rag/#contextual-prompt","title":"Contextual Prompt","text":"<p>You are an assistant to a biomedical researcher.</p>"},{"location":"vignette-rag/#contextual-prompt_1","title":"Contextual Prompt","text":"<p>You have access to text fragments from one or multiple scientific manuscripts, which were retrieved by their semantic similarity to the user's question.</p>"},{"location":"vignette-rag/#contextual-prompt_2","title":"Contextual Prompt","text":"<p>Your task is to help the user understand the manuscripts and their relevance to their questions. Be concise in your explanations, provide quotes from the manuscripts where appropriate, and be open about limitations in cases where the text does not contain pertinent information.</p>"},{"location":"vignette-rag/#message-from-you","title":"Message From You","text":"<p>What is known about the CFTR protein in resistance to ovarian cancer?</p>"},{"location":"vignette-rag/#message-from-biochatter","title":"Message From BioChatter","text":"<p>Based on the provided information, there are several studies that suggest a potential role of the CFTR (Cystic Fibrosis Transmembrane Conductance Regulator) protein in resistance to ovarian cancer. Here are some key findings from the referenced manuscripts:</p> <ol> <li> <p>Gene expression profiling of ovarian tumors before and after chemotherapy revealed molecular signatures of chemoresistant tumors (M, et al., 2006). This study did not specifically mention CFTR, but it provides insights into the molecular changes associated with chemoresistance.</p> </li> <li> <p>ABCF2 expression, which is related to CFTR, was investigated in clear cell adenocarcinoma of the ovary. The study found a relationship between ABCF2 expression and response to chemotherapy or prognosis (Tsuda H, et al., 2010).</p> </li> <li> <p>Alterations in gene copy numbers, including CFTR, were identified as a mechanism for acquired drug resistance in cancer (Yasui K, et al., 2004).</p> </li> <li> <p>ABCF2, an Nrf2 target gene related to CFTR, was found to contribute to cisplatin resistance in ovarian cancer cells (Bao L, et al., 2017).</p> </li> <li> <p>CFTR was not specifically mentioned in the context of ovarian cancer, but a study on hepatocellular carcinoma found that silencing circ-TCF4.85 inhibited cancer progression through microRNA-486-5p-targeted inhibition of ABCF2 (Gao J, et al., 2020).</p> </li> <li> <p>Constitutive activation of nuclear factor \u03baB (NF-\u03baB) was associated with CFTR expression and promoted cancer progression and poor prognosis in cervical cancer (Wu Z, et al., 2013).</p> </li> <li> <p>High levels of CFTR expression were associated with tumor aggression in ovarian cancer, and knockdown of CFTR suppressed ovarian cancer cell proliferation (Xu J, et al., 2015).</p> </li> </ol> <p>It is important to note that while these studies suggest a potential association between CFTR and resistance to ovarian cancer, the exact mechanisms and clinical implications are still being investigated. Further research is needed to fully understand the role of CFTR in ovarian cancer resistance.</p>"},{"location":"vignette-rag/#message-from-you_1","title":"Message From You","text":"<p>Tell me more about the studies of aggression and knockdown of CFTR in vivo.</p>"},{"location":"vignette-rag/#message-from-biochatter_1","title":"Message From BioChatter","text":"<p>Based on the provided information, there are two relevant studies regarding the aggression and knockdown of CFTR in ovarian cancer:</p> <ol> <li> <p>The study by Wu et al. (2013) investigated the constitutive activation of nuclear factor \u03baB (NF-\u03baB) and its contribution to CFTR expression, cancer progression, and poor prognosis in cervical cancer. Although this study is not specific to ovarian cancer, it provides insights into the role of CFTR and NF-\u03baB in promoting cancer progression.</p> </li> <li> <p>The study by Xu et al. (2015) focused specifically on ovarian cancer and found that high levels of CFTR expression were associated with tumor aggression. They also demonstrated that knockdown of CFTR suppressed the proliferation of ovarian cancer cells in vitro and in vivo. This study suggests that CFTR may play a role in promoting ovarian cancer growth.</p> </li> </ol> <p>These studies provide evidence for the involvement of CFTR in cancer aggression and suggest that targeting CFTR could be a potential therapeutic strategy. However, it is important to note that these studies focus on different types of cancer (cervical and ovarian), and the specific mechanisms underlying CFTR's role in ovarian cancer aggression and its knockdown effects are not fully elucidated.</p> <p>It is worth mentioning that the other references provided in the text fragments are not directly related to the aggression and knockdown of CFTR in ovarian cancer. They discuss topics such as the structure and function of ABC transporters, drug resistance, and inflammation in different contexts.</p>"},{"location":"vignette-rag/#chatgpt-4","title":"ChatGPT 4","text":""},{"location":"vignette-rag/#anonymous","title":"Anonymous","text":"<p>What is known about the CFTR protein in resistance to ovarian cancer?</p>"},{"location":"vignette-rag/#chatgpt","title":"ChatGPT","text":"<p>The CFTR protein, known as the cystic fibrosis transmembrane conductance regulator, plays a significant role in ovarian cancer. Research has shown that CFTR is expressed more in ovarian cancer tissues compared to benign ovarian tumors and normal ovaries. This overexpression of CFTR is associated with more advanced stages of ovarian cancer, poor histopathological grade, and higher serum Ca-125 levels, which are all markers of more aggressive disease. Interestingly, the serous type of ovarian cancer shows stronger CFTR staining compared to other types.</p> <p>Moreover, the role of CFTR extends beyond its traditional function as an ion channel. It interacts with various intracellular signaling pathways that are crucial in cancer progression. For example, in ovarian cancer, CFTR overexpression is linked to interactions with the c-Src signaling pathway, which is involved in cellular growth.</p> <p>The implications of CFTR expression in ovarian cancer are significant. In studies where CFTR was downregulated, there was a decrease in cell invasion, migration, proliferation, adhesion, and colony formation in vitro. In vivo experiments also showed that the tumorigenic abilities of ovarian cancer cells were significantly repressed when CFTR expression was reduced.</p> <p>These findings highlight the complex role of CFTR in ovarian cancer and suggest that it could be a potential target for therapeutic intervention. However, it is important to note that the role of CFTR in cancer is multifaceted and varies among different types of cancers. Therefore, a detailed understanding of its function in specific cancer types, like ovarian cancer, is crucial for developing targeted treatments\u200b\u200b\u200b\u200b (https://pubmed.ncbi.nlm.nih.gov/25738998/, https://jmhg.springeropen.com/articles/10.1186/s43042-022-00308-7).</p>"},{"location":"vignette-rag/#chatgpt-35","title":"ChatGPT 3.5","text":""},{"location":"vignette-rag/#anonymous_1","title":"Anonymous","text":"<p>What is known about the CFTR protein in resistance to ovarian cancer?</p>"},{"location":"vignette-rag/#chatgpt_1","title":"ChatGPT","text":"<p>As of my last knowledge update in January 2022, there is limited direct information about the CFTR (Cystic Fibrosis Transmembrane Conductance Regulator) protein's role in resistance to ovarian cancer. CFTR is primarily associated with cystic fibrosis, a genetic disorder that affects the lungs and other organs. However, some research suggests potential links between CFTR and cancer, including ovarian cancer, although the mechanisms are not fully understood.</p> <p>Here are some key points to consider:</p> <p>CFTR Expression: Studies have shown that CFTR is expressed in various tissues, including the female reproductive system. Abnormal CFTR expression or function could potentially impact the development or progression of ovarian cancer. Research is ongoing to understand the role of CFTR in cancer biology. CFTR Mutations: Certain mutations in the CFTR gene have been associated with an increased risk of cancer, although the evidence for ovarian cancer specifically is limited. Mutations in CFTR are more commonly known for their role in cystic fibrosis. Individuals with cystic fibrosis have an increased risk of various cancers, including gastrointestinal cancers, but the link to ovarian cancer is not well-established. Ion Transport and Cancer: CFTR is involved in ion transport across cell membranes, and this function could potentially influence cellular processes relevant to cancer. Changes in ion transport can affect cell proliferation, apoptosis, and other cellular functions that are important in cancer development and progression. Potential Therapeutic Target: Some research suggests that modulating CFTR activity or expression could be a potential target for cancer therapy, including ovarian cancer. However, more research is needed to determine the feasibility and effectiveness of such approaches. It's important to note that the understanding of the role of CFTR in cancer, including ovarian cancer, is an evolving field, and new research findings may have emerged since my last update in January 2022. To get the most up-to-date information on this topic, I recommend consulting recent scientific literature and speaking with medical experts or researchers actively working in the field of ovarian cancer and CFTR biology.</p>"},{"location":"vignette-rag/#references","title":"References","text":"<ul> <li> <p>[1] ABC transporters affects tumor immune microenvironment to regulate cancer immunotherapy and multidrug resistance</p> </li> <li> <p>[2] ABC transporters and the hallmarks of cancer: roles in cancer aggressiveness beyond multidrug resistance</p> </li> <li> <p>[3] Advances in the structure, mechanism and targeting of chemoresistance-linked ABC transporters</p> </li> <li> <p>[4] ATP-binding cassette (ABC) transporters in cancer: A review of recent updates</p> </li> </ul>"},{"location":"wasm/","title":"LLM in your Browser - WebAssembly","text":"<p>Coming soon.</p>"}]}